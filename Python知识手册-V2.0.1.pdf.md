Python 知识手册-v2.0.1
出品：Python数据之道
作者：Lemonbit
02Feb,2020Python数据之道
前言
(cid:7) (cid:4)
(cid:6)“种一棵树，最好的时间是十年前，其次是现在。”(cid:5)
各位读者好，我是Lemonbit。博客刚兴起不久的时候，我就开始写一些博客文章，但工作之后基本上就没有坚
持写。现在想来，有些可惜了。
从2016年底开始，再次坚持写作，并于2017年开通了微信公众号「Python数据之道」。期间陆陆续续，写了些
许Python及数据科学相关的内容，以及翻译了部分外网文章，希望借此机会，将这些显得零散的内容整合起来，形成
一本小册子，名曰《Python知识手册》。
虽然我写了一些关于Python零基础入门的文章，但《Python知识手册》并没有比较完整的覆盖Python的基
础知识。因此，针对手册的阅读，各位读者最好有一些Python的基本功底。
当前版本中，手册主要包含以下几方面相关的内容：
• Python基础知识
• JupyterNotebook
• Numpy
• Pandas
• Matplotlib
• Seaborn
• Bokeh
• Plotly
• 若干个项目实战案例
• Python学习资料
如没有特殊的说明，手册中代码的运行环境如下：
• windows7或windows10系统或MacOS系统
• python3.5或python3.5+
• Anaconda/PyCharm/JupyterNotebook/VsCode
若上述这些内容，能给诸位读者带来益处，我觉得付出些辛劳，也是值得的。
若对我写的内容有兴趣，欢迎大家通过以下途径来关注。
1.微信公众号
公众号「Python数据之道」秉承“让数据更有价值”的理念，主要分享数据相关的内容，包括数据分析，挖掘，可
视化，机器学习，深度学习等,欢迎关注。
http://liyangbit.com 2Python数据之道
《Python知识手册》中部分文章提供了源代码，可以在公众号「Python数据之道」后台回复「code」来获取。
2.个人网站
网址：http://liyangbit.com
我的个人网站中，包含更多的文章，并且在不断的进行更新。目前，网站中涉及了Python相关一系列内容，包括
Python基础、Python数据科学、项目实战等内容，欢迎访问。
3.感谢的话
最后聊一下我为什么写这本手册：作为能源环保领域的从业者，原本跟Python没有直接的关联，在长期的工作中
我希望自己能尽快走出舒适圈，做一些有意义的事情，所以我跑去研究当前比较火热的人工智能，发现Python的强大
之处，从而开始深入到Python的学习中去了。过去的时间里，在公众号「Python数据之道」里写了一些内容，希望
整合起来，对大家有些启发，于是有了《Python知识手册》。
坚持写作，少了许多与家人陪伴的时光，感谢家人对我的支持与理解。通过「Python数据之道」，结识了不少业界
前辈以及新朋友，通过交流，让我愈发感到自己的渺小与不足，希望能与诸位共同成长。衷心感谢诸位前辈以及众多读
者对「Python数据之道」的支持。手册封面图片由Free-Photos在Pixabay上发布，在此表示感谢。
当然，由于个人水平有限，文章中内容难免有不准确的地方，《Python知识手册》会不定期进行更新，更新后的内
容可以在公众号「Python数据之道」后台回复数字「600」进行获取。
4.做一个跨领域学习者
近期新开设了一个非技术类的微信公众号，名称是「价值前瞻」。在这里推荐下，也算是一点小小私心，希望大家能
关注下。
「价值前瞻」的初衷是希望自己以及更多的人能“做一个有远见的人”，「价值前瞻」聚焦于书籍阅读、投资创业、
自我成长等领域，希望能前瞻思考，不断适应外部环境变化，成就更好的自己。
5.版本历史信息
http://liyangbit.com 3Python数据之道
《Python知识手册》更新信息情况如下：
版本号 日期 主要内容描述
v2.0.1 2020-02-02 修复2.0pdf版没有书签的bug
v2.0 2020-02-02 添加Python基础知识；添加Plotly
内容
v2018 2019-02-25 初始版本
Tips：
您现在看到的可能已经不是最新版本，请在公众号「Python数据之道」后台回复数字「600」获取最新版。
李阳
02Feb2020
http://liyangbit.com 4Contents
1 Python语言的简单介绍 11
1.1 起源 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.2 为什么名字叫“Python” . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.3 Python的发展历史 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.4 Python语言的特点 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.5 Python语言的应用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.6 Python语言的一些缺点 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
1.7 参考文献： . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2 Python安装 17
2.1 安装方法一： . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.2 安装方法二：（推荐） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.3 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3 第一个Python程序 26
3.1 输出函数print() . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.2 输入函数input() . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
3.3 小练习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
4 创建第一个“.py”和“.ipynb”文件 30
4.1 创建和运行“.py”文件 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
4.2 文本编辑器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.3 创建和运行“.ipynb”文件 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4.4 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
5 Python数据类型-List介绍 44
5.1 初识list . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
5.2 访问列表中的值 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
5.3 列表的操作、函数及方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
5.4 list中元素的类型可以多样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
5.5 list的切片（slices） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
5.6 列表推导式(ListComprehension) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
5.7 小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
5Contents Python数据之道
6 Python中字典合并的实现方法 56
6.1 背景 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
6.2 Methon1：适用于Python3.5及以上版本呢 . . . . . . . . . . . . . . . . . . . . . . . . . . 56
6.3 Method2：自定义函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
6.4 多个dict进行合并 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
7 JupyterNotebook主题设置、字体修改等 58
7.1 新的风格预览 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
7.2 主题更换过程 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
7.3 我的主题设置参数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
8 JupyterNotebook中自动补全代码 62
9 JupyterNotebook输出pdf并支持中文显示 64
9.1 安装顺序 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
10 Anaconda安装虚拟环境到指定路径 70
10.1 曾经的困扰 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
10.2 安装虚拟环境 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
10.3 虚拟环境下安装python库 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
11 Numpy的基本用法 75
11.1 Numpy数组对象 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
11.2 创建ndarray数组 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
11.3 Numpy的数值类型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
11.4 ndarray数组的属性. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
11.5 ndarray数组的切片和索引 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
11.6 处理数组形状 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
11.7 数组的类型转换 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
11.8 numpy常用统计函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
11.9 数组的广播 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
11.10 写在最后 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
12 Numpy的random函数简介 96
12.1 numpy.random.rand() . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
12.2 numpy.random.randn() . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
12.3 numpy.random.randint() . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
12.4 生成[0,1)之间的浮点数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
12.5 numpy.random.choice() . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
12.6 numpy.random.seed() . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
13 Numpy中Meshgrid函数介绍及2种应用场景 101
13.1 Meshgrid函数的基本用法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
http://liyangbit.com 6Contents Python数据之道
13.2 Meshgrid函数的一些应用场景 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
13.3 Method-1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
13.4 Method-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
14 Pandas日期数据处理 110
14.1 读取并整理数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
14.2 按日期筛选数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
14.3 按日期显示数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
14.4 按日期统计数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
15 Pandas的DataFrame如何按指定list排序 120
15.1 分析过程 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
15.2 指定list元素多的情况： . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
15.3 指定list元素少的情况： . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
15.4 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
16 Pandas小册子：根据条件创建新的列 127
16.1 第一种方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
16.2 第二种方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
17 Matplotlib饼图简介 130
17.1 官方Demo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
17.2 将实际数据应用于官方Demo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
17.3 一些改善措施 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
18 Matplotlib中等高线图（contour）的绘制 142
18.1 仅绘制等高线 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
18.2 仅填充等高区域颜色： . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
18.3 绘制完整的等高线图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
19 Matplotlib可视化最有价值的50个图表 146
19.1 介绍 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
19.2 准备工作 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
19.3 关联（Correlation） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
19.4 偏差（Deviation） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
19.5 排序（Ranking） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
19.6 分布（Distribution） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
19.7 组成（Composition） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
19.8 变化（Change） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
19.9 分组（Groups） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
20 轻松用Seaborn进行数据可视化 219
20.1 直方图（Distplot） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
http://liyangbit.com 7Contents Python数据之道
20.2 联合分布图（Jointplot） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
20.3 矩阵图（Pairplot） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
20.4 条形图（Barplot） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
20.5 箱形图（Boxplot） . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
20.6 LMPlot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
21 Seaborn可视化：图形个性化设置的几个小技巧 231
21.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
21.2 未个性化设置的情形 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
21.3 进行个性化设置 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
22 Seaborn热力图使用进阶 235
22.1 构造数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
22.2 Seaborn的heatmap各个参数介绍. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
22.3 案例应用：突出显示某些数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
23 Bokeh入门 249
23.1 常规步骤 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
23.2 绘制circle() . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
24 Bokeh：figure详细解读 255
24.1 Bokeh中绘图的一般步骤 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
24.2 综合小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
25 Bokeh:29种基础可视化图形 268
25.1 Bokeh中绘图的一般步骤 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
25.2 综合小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
26 Bokeh中独特的数据类型简介：ColumnDataSource 280
26.1 直接提供数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
26.2 通过ColumnDataSource来提供数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
26.3 小结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
27 Bokeh中数据的添加、修改和筛选 287
27.1 添加新的数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
27.2 数据更新 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
27.3 筛选数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
28 Bokeh中图形与组件的布局简介 296
28.1 图形的布局 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
28.2 组件的布局 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
28.3 图形和组件混合布局 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
http://liyangbit.com 8Contents Python数据之道
29 Bokeh可视化-项目实战经验分享 305
29.1 Bokeh基础. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
29.2 在Bokeh中添加主动交互 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
29.3 在Bokeh中创建交互式可视化应用程序. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
30 PlotlyExpress入门 331
30.1 使用PlotlyExpress轻松地进行数据可视化 . . . . . . . . . . . . . . . . . . . . . . . . . . 331
30.2 可视化分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
30.3 颜色面板和序列 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
30.4 用一行Python代码进行交互式多维可视化 . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
30.5 Plotly生态系统的一部分 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
30.6 能够与Dash完美匹配 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
30.7 设计理念：为什么我们创建PlotlyExpress？ . . . . . . . . . . . . . . . . . . . . . . . . . 345
31 在Pycharm等编辑器下使用PlotlyExpress 347
31.1 第一个示例 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
31.2 第二个示例 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
32 PlotlyExpress合并到Plotly，安装过程有点小尴尬 350
32.1 如何安装？ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
32.2 小小插曲 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
32.3 最后 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
33 推荐一个牛逼的生物信息Python库-DashBio 354
33.1 探索3d状态下的小分子 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
33.2 分析显微镜图像中的细胞 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
33.3 进行药代动力学分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
33.4 可视化FASTA数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
33.5 突出基因组相似性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
33.6 可视化微阵列结果 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361
33.7 搜索和选择序列 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
33.8 3d分子的环境遮挡 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
33.9 使用DashIdeogram可视化染色体 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
33.10可视化基因突变 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
33.11 测量和注释医学图像 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
33.12构建系统发育树和网络图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
33.13Final . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
34 项目实战：智联求职系列 369
34.1 第一篇：数据采集并保存到MongoDB数据库 . . . . . . . . . . . . . . . . . . . . . . . . . . 369
34.2 第二篇：python求职Top10城市分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
http://liyangbit.com 9Contents Python数据之道
35 项目实战：Python数据分析，UFO长啥样？ 388
35.1 数据整理与清洗 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388
35.2 UFO长啥样？. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
35.3 UFO在美国那些州（state）出现的次数比较多？ . . . . . . . . . . . . . . . . . . . . . . . . 394
35.4 UFO在哪些年份出现的次数较多？ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
35.5 1997年以后的UFO事件分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
36 项目实战：世界杯系列 401
36.1 第一篇：2018世界杯：用Python分析热门夺冠球队 . . . . . . . . . . . . . . . . . . . . . . 401
36.2 第二篇：德国是2018世界杯夺冠最大热门？ . . . . . . . . . . . . . . . . . . . . . . . . . . 414
37 项目实战：福布斯系列 434
37.1 数据分析思路 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 434
37.2 数据采集 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 434
37.3 数据完整性检查 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
37.4 补充数据收集 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 442
37.5 数据清洗-2007年数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
37.6 数据清洗-2008-2010年 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
37.7 数据清洗-2011-2015年 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
37.8 数据清洗-2016年 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460
37.9 数据清洗-2017年 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
38 Python学习资料 472
38.1 《笨办法学Python3》 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472
http://liyangbit.com 101 Python 语言的简单介绍
1.1 起源
Python的创立者，GuidovanRossum，荷兰人。1982年，Guido从阿姆斯特丹大学获得了数学和计算机硕
士学位。然而，尽管他算得上是一位数学家，但他更加享受计算机带来的乐趣。用他的话说，尽管拥有数学和计算机双
料资质，他总趋向于做计算机相关的工作，并热衷于做任何和编程相关的活儿。
Python的创始人吉多·范罗苏姆（GuidovanRossum），在1989年12月的圣诞节期间，为了打发时间，决
定开发一种新的脚本解释程序，作为ABC语言的继承。1991年2月，第一个Python编译器诞生。现在，python以
其优美，清晰，简单的特性在全世界广泛流行，成为最主流的编程语言之一。
GuidovanRossum有一句著名的话就是“Lifeisshort,youneedPython”，译为：“人生苦短，我用
Python”。
截止至2017年11月，python在TIOBE流行编程语言排行榜，已经上升到第四名。并且从趋势图中可以看出其
还在呈增长趋势。
Figure1.1:排名情况-2017年11月
11Chapter1. Python语言的简单介绍 Python数据之道
Figure1.2:排名情况-2020年1月
Figure1.3:历年排名情况-截至2017年11月
http://liyangbit.com 12Chapter1. Python语言的简单介绍 Python数据之道
Figure1.4:历年排名情况-截至2020年1月
1.2 为什么名字叫“Python”
我们知道英文单词“Python”的中文翻译是“蟒蛇”或者“巨蛇”的意思。但作为一名编程语言的名称，Python的
初始意思并不是这样的。
Python这个名字，来自Guido所挚爱的电视剧“MontyPython’sFlyingCircus”。他希望这个新的叫做
Python的语言，能符合他的理想：创造一种C和shell之间，功能全面，易学易用，可拓展的语言。
1.3 Python的发展历史
• Python3.6-2016/12/23
• Python3.5-2015/09/13
• Python3.4-2014/03/16
• Python3.3-2012/09/29
• Python3.2-2011/02/20
• Python3.1-2009/06/27
• Python3.0-2008/12/03
• Python2.7-2010/07/03
• Python2.6-2008/10/1
• Python2.5-2006/09/19
• Python2.4–2004/11/30,同年目前最流行的WEB框架Django诞生
• Python2.0-2000/10/16，加入了内存回收机制，构成了现在Python语言框架的基础
• 1999年Python的web框架之祖——Zope1发布
• Python1.0-January1994增加了lambda,map,filterandreduce.
http://liyangbit.com 13Chapter1. Python语言的简单介绍 Python数据之道
• 1991年，第一个Python编译器诞生。它是用C语言实现的，并能够调用C语言的库文件。从一出生，Python
已经具有了：类，函数，异常处理，包含表和词典在内的核心数据类型，以及模块为基础的拓展系统。
• 1989年的圣诞节，Guido开始编写Python语言的编译器。
2014年11月，Python2.7将在2020年停止支持的消息被发布，并且不会再发布2.8版本，建议用户尽可能
的迁移到3.4+,Python最初发布时，在设计上有一些缺陷，比如Unicode标准晚于Python出现，所以一直以来对
Unicode的支持并不完全，而ASCII编码支持的字符有限。例如：对中文支持不好Python3相对Python早期的版
本是一个较大的升级，Py3在设计的时候没有考虑向下兼容，所以很多早期版本的Python的程序无法再Py3上运行。
1.4 Python语言的特点
1）简单易学
Python语言简洁、优雅，相比其他程序语言（比如：Java，C++等），Python语言更容易阅读和编写。Python
语言使用起来很有趣，且该语言更多的是专注于解决方案而不是语法本身。
初学者学Python，不但入门容易，而且将来深入下去，可以编写那些非常非常复杂的程序。
2）开源且免费
Python是一门开源的编程语言，可以免费使用，甚至可以用于商业用途。当然，你也可以提交相关贡献代码。
Python有一个庞大的社区来维护并保持快速发展。
3）可移植性
Python是跨平台的编程语言，它可以运行在windows、Mac和各种Linux/Unix系统上。也就是说，假设在
windows系统下编写的Python程序，在Mac或Linux系统下也是可以运行的。
4）开发效率非常高
Python有非常强大的第三方库，基本上你想通过计算机实现任何功能，Python官方库里都有相应的模块进行支
持，直接下载调用后，在基础库的基础上再进行开发，大大降低开发周期，避免重复造轮子。
5）高级语言
当你用Python语言编写程序的时候，你无需考虑诸如如何管理你的程序使用的内存以及垃圾回收一类的底层细
节。
6）可扩展性
如果你需要你的一段关键代码运行得更快或者希望某些算法不公开，你可以把你的部分程序用C或C++编写，然
后在你的Python程序中使用它们。
7）可嵌入性
你可以把Python代码嵌入你的C/C++程序，从而向你的程序用户提供脚本功能。
8）面向对象
Python中的一切都是对象。面向对象编程（OOP）可以帮助您直观地解决复杂的问题。有了OOP，你可以通过
创建对象把这些复杂的问题划分成更小的集合。
http://liyangbit.com 14Chapter1. Python语言的简单介绍 Python数据之道
1.5 Python语言的应用
1.5.1 Python语言的应用领域
Python被广泛的应用于诸多领域，比如：
• 数据科学：数据科学涉及领域很广，涉及到当前火热的人工智能领域。Python的数据科学包括数据分析、数据
可视化、数据挖掘、自然语言处理、机器学习、深度学习等。典型的Python库有：Numpy，Scipy，Pandas，
Matplotlib，Seaborn，Scikit-learn，tensorflow等。
• 云计算：典型应用——Python开发的OpenStack
• WEB开发：众多优秀的WEB框架，比如：Django、flask、tornado
• 网络爬虫：使用Python可以便捷的编写网络爬虫，从网页上爬取相关信息，常用的Python库包括：Requests，
BeautifulSoup，Scrapy等。
• 系统运维：运维人员必备，slatstack（系统自动化配置和管理工具），Ansible（自动化运维工具）
• 图形界面开发：wxPython、PyQT、TKinter
1.5.2 使用Python的部分知名公司
越来越多的公司选在python作为其主要开发语言，例如：
• Google-GoogleGroups、Gmail、GoogleMaps、AlphaGo等，GoogleAppEngine支持python作
为开发语言
• NASA-美国宇航局，从1994年起把python作为主要开发语言
• Dropbox-美国最大的在线云存储网站，全部用Python实现，每天网站处理10亿个文件的上传和下载
• 豆瓣网-图书、唱片、电影等文化产品的资料数据库网站
• BitTorrent-bt下载软件客户端
• gedit-Linux平台的文本编辑器
• GIMP-Linux平台的图像处理软件(Linux下的PS)
• 知乎（国外的Quora）-社交问答网站，国内最大的问答社区，通过Python开发
• AutodeskMaya-3D建模软件，支持python作为脚本语言
• YouTube:世界上最大的视频网站YouTube就是用Python开发的
• Facebook:大量的基础库均通过Python实现的
• Redhat:世界上最流行的Linux发行版本中的yum包管理工具就是用python开发的
除上面之外，还有搜狐、金山、腾讯、盛大、网易、百度、阿里、淘宝、土豆、新浪、果壳等公司都在使用Python
完成各种各样的任务。
更多案例：
https://www.python.org/about/success/
http://liyangbit.com 15Chapter1. Python语言的简单介绍 Python数据之道
1.6 Python语言的一些缺点
任何编程语言都有缺点，Python也不例外。比如：
（1）速度慢，Python的运行速度相比C语言确实慢很多，跟JAVA相比也要慢一些，因此这也是很多所谓的大牛
不屑于使用Python的主要原因，但其实这里所指的运行速度慢在大多数情况下用户是无法直接感知到的，必须借助测
试工具才能体现出来，比如你用C运一个程序花了0.1s,用Python是0.01s,这样C语言直接比Python快了10倍,
算是非常夸张了，但是你是无法直接通过肉眼感知的，因为一个正常人所能感知的时间最小单位是0.15-0.4s左右。其
实在大多数情况下Python已经完全可以满足你对程序速度的要求，除非你要写对速度要求极高的搜索引擎等，这种情
况下，当然还是建议你用C去实现的。
随着硬件设备及计算力的增强，运行速度慢的这个缺点相对来说有所改善。在编写代码所花时间与程序运行所用时
间进行平衡后，很多企业都开始选择使用Python。
（2）代码不能加密，因为PYTHON是解释性语言，它的源码都是以名文形式存放的，不过我不认为这算是一个缺
点，如果你的项目要求源代码必须是加密的，那你一开始就不应该用Python来去实现。
1.7 参考文献：
1. TIOBEIndexforNovember2017
2. Python的历史
3. Python简介
4. 一门编程语言的发展简史
5. LearnPythonProgramming-TheDefinitiveGuide
http://liyangbit.com 162 Python 安装
Python是跨平台的编程语言，它可以运行在windows、Mac和各种Linux/Unix系统上。也就是说，假设在
windows系统下编写的Python程序，在Mac或Linux系统下也是可以运行的。
目前，Python的版本主要分为两大类，即Python3.X版和Python2.x版，这两个版本是不兼容的。由于
Python3.x版本越来越普及，我们接下来的教程使用Python3.x版（Python3.5版或Python3.6版及以上版本均
可）。
由于个人windows系统用的多，所以这里的Python安装以windows系统为例来演示。
其他系统下的安装，建议大家可以在网上查找相关安装方法。
下面介绍在Windows系统下两种常用的安装方法，第一种最基础，第二种会在以后带来一些便利，个人推荐第二
种安装方法。
2.1 安装方法一：
2.1.1 安装官方版Python3.6
下面以在windows系统下安装Python3.6版为例：
首先，根据你的Windows版本（64位还是32位）从Python的官方网站下载Python3.6对应的64位安装
程序或32位安装程序，然后，运行下载的EXE安装包：
17Chapter2. Python安装 Python数据之道
Figure2.1:安装过程
特别要注意勾上Add Python 3.6 to PATH，然后点“InstallNow”即可完成安装，如下：
Figure2.2:安装过程
http://liyangbit.com 18Chapter2. Python安装 Python数据之道
Figure2.3:安装过程
Figure2.4:安装过程
2.1.2 运行Python3.6
安装完成后，我们可以运行下python的解释器里，来查看是否安装成功。
首先，在windows开始菜单里找到Python3.6，如下：
http://liyangbit.com 19Chapter2. Python安装 Python数据之道
Figure2.5:开始菜单
Figure2.6:开始菜单
http://liyangbit.com 20Chapter2. Python安装 Python数据之道
在这里，Python3.6程序下，有两个可以运行python的解释器，我们可以分别打开来看看。
• IDLE（Python3.664-bit）：
Figure2.7:IDLE
• Python3.6（64-bit）
http://liyangbit.com 21Chapter2. Python安装 Python数据之道
Figure2.8:CMD
这两个界面下，都是可以运行python程序的。这里以IDLE界面位例，来测试下python3.6是否安装成功。
Figure2.9:IDLE
从上述界面来看，我们的安装程序能够运行python语句，已成功的安装了Python3.6。
2.2 安装方法二：（推荐）
第二种安装方法是推荐直接安装Anaconda，它已经包含了Python的对应版本。
为什么要推荐安装Anaconda呢，针对想用Python来进行数据挖掘、机器学习等的童鞋，安装Anaconda会
带来一些优势，比如：
1. 不需要配置PYTHON环境变量；
http://liyangbit.com 22Chapter2. Python安装 Python数据之道
2. 集成很多packages，尤其是科学计算相关的库，省去安装这些库带来的烦扰；
3. packages的安装很简单，conda一键解决；
2.2.1 安装Anaconda
安装最新版的Anaconda可以去官网下载，地址为：
Anaconda官网下载地址
windows系统下，如下：
Figure2.10:Anaconda
根据系统，下载对应的版本，然后根据提示进行安装，选默认设置就可以，一路“next”，即可安装完毕。
安装完毕后，打开“cmd”命令行，查看Anaconda是否安装好。
http://liyangbit.com 23Chapter2. Python安装 Python数据之道
Figure2.11:cmd
在命令行中输入conda info，查看Anaconda安装的版本信息，以及对应的Python版本信息。
Figure2.12:condainfo
如果你得到了和上图类似的界面，恭喜你已经成功安装了Anaconda。
从上图来看，可知我安装的是Anaconda4.3.30版，对应的Python版本是3.5.2版，并不是目前的最新版。
大家可以安装最新的Anaconda版本，也可以选择安装跟我一样的版本。
如果想安装的版本不是最新版，可以在下述网址查找相应的版本。
http://liyangbit.com 24Chapter2. Python安装 Python数据之道
https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/
Figure2.13:版本
2.2.2 运行Python程序
Anaconda安装完成后，可以在几个地方运行Python程序，最常用的是“IPython”。
在windows开始菜单的所有程序中找到anaconda文件夹，点开文件夹，下面可以找到“IPython”。
点击“IPython”，得到如下界面:
Figure2.14:ipython
从上述界面来看，我们的安装程序能够运行python语句，已成功的安装了Anaconda及Python。
2.3 总结
对于上述两种安装Python的方法，个人推荐第二种方法。尤其是想用Python来进行数序分析、数据挖掘、机
器学习等，第二种方法会带来很多便捷性。
http://liyangbit.com 253 第一个 Python 程序
在安装好Python程序后，我们在Anaconda中找到iPython来编写代码并输出。
在编写代码前，先介绍下运行环境：
• windows7
• Python3.5
后续的教程，如果没有特别说明，基本上也是在Python3.x（当前用的是3.5版）下运行。
3.1 输出函数print()
跟其他很多程序一样，我们第一个程序用Python来输出“Hello,world”。
Python代码如下：
1 print(”hello, world”)
输入结果如下：
Figure3.1:hello-world
语法解释：
python语言中，输出用print()函数来描述，括号内可以是字符串（比如“hello,world”），也可以是数据的
运算。
26Chapter3. 第一个Python程序 Python数据之道
当内容是字符串时，可以用双引号，也可以用单引号。代码演示如下：
Figure3.2:demo
注意到，上图中输出“1+2”时，可以用print(1+2)或者1+2，并且数据运算时是不需要加引号的。
当然，print(1+2)显示的结果为“3”，有时候，你可能希望显示这样的结果[1+2=3]，演示如下：
Figure3.3:demo
从上面可以看出，print()可以接收多个参数，参数之间以逗号隔开，遇到逗号会输出一个空格。
还是以“hello,world”为例，可以再看下输出多个字符串的情形。
1 print(”hello,”, ”world”, ”welcome!”)
演示如下：
http://liyangbit.com 27Chapter3. 第一个Python程序 Python数据之道
Figure3.4:demo
3.2 输入函数input()
上面提到的是输出函数，经常会遇到的一种情形是，需要用户输入相关信息，然后程序才继续运行，这种情况怎么
办呢？
Python提供了input()函数来让用户输入信息。
Figure3.5:demo
在上面程序中，运行后需要用户输入用户名信息，程序才能继续运行，我们输入“Lemon”（不含引号）来试试。
输入“Lemon”后，我们再运行代码print(name)，可以看到其输出结果，演示如下：
http://liyangbit.com 28Chapter3. 第一个Python程序 Python数据之道
Figure3.6:demo
上图中，有2个“Lemon”（不含引号），其中第一个是用户输入的信息，第二个是输出的结果。
3.3 小练习
在“iPython”中输入如下代码，看看运行情况。
1 import this
Python程序是不是很简洁易懂啊，快来动手试试吧。
http://liyangbit.com 294 创建第一个“.py” 和 “.ipynb” 文件
上一节中，我们运行了第一个Python程序。在上节中，我们是在Anaconda程序文件夹下打开“iPython”来
运行的Python程序。可能你已经注意到，在上节中，写好的Python程序运行后，输出了相关结果。但是，Python
代码文件并没有保存下来，如果下次还要运行相同的代码，或者已有代码需要进行修改时，在“iPython”中不好实现。
这里，我们来介绍下，在通常情况下，我们应该怎样保存和修改Python程序文件。
通常，我们看到的用Python语言编写的源代码文件，其文件后缀是“.py”或“.ipynb”。这里，我列了两种文件，
其中“.py”文件是标准的Python源代码文件，通常情况下，我们会使用“.py”的python源代码文件。
另一种是“.ipynb”文件，会在本文后续部分进行描述。
4.1 创建和运行“.py”文件
4.1.1 创建“.py”文件
在E盘下创建名称为“tutorials”的文件夹，打开“tutorials”文件夹，鼠标右击创建文本文档，修改文件名称为
“hello.py”，演示如下：
30Chapter4. 创建第一个“.py”和“.ipynb”文件 Python数据之道
Figure4.1:demo
注意要修改文件的后缀为“.py”（默认是“.txt”）
http://liyangbit.com 31Chapter4. 创建第一个“.py”和“.ipynb”文件 Python数据之道
Figure4.2:demo
4.1.2 打开“.py”文件
依次点击“windows开始菜单”，“所有程序”，在Anaconda程序文件夹下打开“spyder”程序（对应下图红色
数字1、2、3）
http://liyangbit.com 32Chapter4. 创建第一个“.py”和“.ipynb”文件 Python数据之道
Figure4.3:demo
然后从“spyder”中打开“hello.py”程序文件（下图红色箭头所指），得到如下界面：
http://liyangbit.com 33Chapter4. 创建第一个“.py”和“.ipynb”文件 Python数据之道
Figure4.4:demo
打开“hello.py”文件后，我们可以从左边看到，这是一个空白文件，里面没有任何代码。
在上图界面的右下角，我们可以找到“iPythonconsole”（iPython控制台），我们可以选择在此输出Python
程序的运行结果。
Figure4.5:demo
http://liyangbit.com 34Chapter4. 创建第一个“.py”和“.ipynb”文件 Python数据之道
4.1.3 运行“.py”文件
在“hello.py”文件中输入如下代码：
1 print(”hello, world”)
保存文件，然后点击绿色的“三角符号”来运行程序（spyder中运行python程序的快捷键是“F5”），如下：
Figure4.6:demo
如果是第一次运行，点击绿色的“三角符号”后，会弹出如下对话界面：
Figure4.7:demo
选择上图所示默认设置，然后点击“run”，运行python程序，此时，我们会在右下角的IPython控制台输出结
果，如下：
http://liyangbit.com 35Chapter4. 创建第一个“.py”和“.ipynb”文件 Python数据之道
Figure4.8:demo
可以看到，我们输出的结果是“hello,world”（不含引号），跟上节在iPython界面下的输出结果是一致的。
我们再来试试在“hello.py”文件中加入更多代码：
1 print(”hello, world”)
2
3 print(”hello,”, ”world”, ”welcome!”)
4
5 print(1+2)
6
7 print('1+2=', 1+2)
保存文件，然后运行程序，运行结果如下：
1 hello, world
2 hello, world welcome!
3 3
4 1+2= 3
请注意，不同于iPython界面，在“hello.py”程序文件中，每一次需要打印输出结果的地方，都需要用print()
函数来输出结果。
http://liyangbit.com 36Chapter4. 创建第一个“.py”和“.ipynb”文件 Python数据之道
4.2 文本编辑器
上面介绍的是在spyder中编辑并运行python程序，有时候，我们可能想在其他文本编辑器中编辑python程
序。有很多的文本编辑器可以用来编写python程序，作为刚开始接触的同学，可以使用notepad++或者sublime
text来编写python程序。这两个文本编辑器都是免费可以使用的，并且程序不大。
上述“hello.py”文件，在notepad++中打开的界面如下：
Figure4.9:demo
关于这两个编辑器的使用，可以自行查找下。
4.3 创建和运行“.ipynb”文件
“.ipynb”文件是使用JupyterNotebook来编写Python程序时的文件。
JupyterNotebook（此前被称为IPythonnotebook）是一个交互式笔记本，支持运行40多种编程语言。它
会在浏览器中打开并运行相关程序，在这里，我们主要介绍其在编写和运行Python程序方面的应用。
在安装好Anaconda后，已经自动安装好了JupyterNotebook，如下所示（红色数字4所指）：
http://liyangbit.com 37Chapter4. 创建第一个“.py”和“.ipynb”文件 Python数据之道
Figure4.10:demo
点击“JupyterNotebook”，可以得到如下启动的命令行界面，以及在浏览器中打开的JupyterNotebook界
面。
命令行界面：
http://liyangbit.com 38Chapter4. 创建第一个“.py”和“.ipynb”文件 Python数据之道
Figure4.11:demo
JupyterNotebook界面：
Figure4.12:demo
从上面可看出，默认的启动界面是以Jupyternotebook的安装路径为初始路径的（即从C盘启动）。
如果我们希望在需要运行的程序所在文件夹启动，该如何操作呢？
以上述E盘“tutorials”文件夹为例，如果我们希望在该文件夹下创建新的“.ipynb”文件或者运行该文件夹下的
“.ipynb”文件，我们可以先打开E盘“tutorials”文件夹，然后按下“Shift”键同时点击右键，得到如下界面：
http://liyangbit.com 39Chapter4. 创建第一个“.py”和“.ipynb”文件 Python数据之道
Figure4.13:demo
点击“在此处打开命令窗口(W)”，在弹出的命令行窗口中输入“jupyternotebook”，在浏览器中得到当前路径
下的jupyternotebook文件列表，如下：
命令行界面：
http://liyangbit.com 40Chapter4. 创建第一个“.py”和“.ipynb”文件 Python数据之道
Figure4.14:demo
JupyterNotebook界面：
Figure4.15:demo
请注意，命令行界面不能关闭，它是Jupyternotebook运行必须要有的环境。
点击上面的“New”，可以开始创建新的“.ipynb”文件。
http://liyangbit.com 41Chapter4. 创建第一个“.py”和“.ipynb”文件 Python数据之道
Figure4.16:demo
在上述新打开的界面中：
• 红色数字1处，可以修改文件名称，比如我们修改为“hello”(不含引号)
• 红色数字2处，点击保存文件
• 红色数字3处，点击添加新的代码行
• 红色数字4处，点击运行代码
• 红色数字5处，选择当前是编写代码还是以其他格式进行编写（比如Markdown），默认是编写代码模式，即
“code”。
现在，我们输入之前在“hello.py”文件中的代码，每次输入一行代码，然后依次运行（点击红色数字4处的符号，
或者按快捷键“Shift”+“Enter”），运行结果如下：
Figure4.17:demo
此时，我们可以在 E 盘 “tutorials”文件夹下看到有一个名称为 “hello.ipynb” 的文件，或者在 Jupyter
notebook的首页下，也可以看到该文件，如下：
http://liyangbit.com 42Chapter4. 创建第一个“.py”和“.ipynb”文件 Python数据之道
Figure4.18:demo
4.4 总结
前面我们分别介绍了“.py”文件和“.ipynb”文件的创建及运行方法，在以后的章节中，我们会大量涉及到这两类
文件的使用。
关于Jupyternotebook，这里也作了简单的介绍。需要说明的是，Jupyternotebook是一个强大的工具，在
Python的数据科学领域会频繁的使用，在后续也会进一步介绍。
http://liyangbit.com 435 Python 数据类型-List 介绍
本文的运行环境：
• windows7系统
• python3.5
• jupyternotebook
5.1 初识list
list是Python内置的一种高级数据类型。list是一种有序的集合，在python中应用很广泛。
1 names = ['James', 'Michael', 'Emma', 'Emily']
2
3 print(”names的 数 据 类 型：”,type(names))
4 print(names)
输出结果:
1 names的 数 据 类 型： <class 'list'>
2 ['James', 'Michael', 'Emma', 'Emily']
用len()可获得list的长度，即list集合所包含的元素个数，如下：
1 n = len(names)
2 print(n)
输出结果:
1 4
空的list
如果list中一个元素也没有，我们可以定义一个空的list，则其长度为0，如下：
1 empty_list = []
2 empty_list
输出结果:
1 []
44Chapter5. Python数据类型-List介绍 Python数据之道
1 len(empty_list)
输出结果:
1 0
5.2 访问列表中的值
用索引来访问list中的每个元素，请注意索引是从0开始，最后一个的索引编号为n-1，即所有元素的编号依次为
(0,1,2,…,n-1)。
list中单个元素的访问如下：
1 names[0]
输出结果:
1 'James'
1 names[1]
输出结果:
1 'Michael'
如果要获取最后一个元素，也可以用-1来做索引，倒数第二个元素可以用-2来索引，其他以此类推。
1 names[-1]
输出结果:
1 'Emily'
1 names[-2]
输出结果:
1 'Emma'
可以通过for循环来列出所有元素。有两种方法可以实现，如下：
方法1
1 for name in names:
2 print(name)
输出结果:
1 James
http://liyangbit.com 45Chapter5. Python数据类型-List介绍 Python数据之道
2 Michael
3 Emma
4 Emily
方法2
1 for i in range(len(names)):
2 print(names[i])
输出结果:
1 James
2 Michael
3 Emma
4 Emily
5.3 列表的操作、函数及方法
5.3.1 list中元素的增、改、删等操作
list是一个可变的有序列表，可以通过添加、修改、删除等操作来操作list中的元素。
往list中添加元素
可以通过append()和insert()方法来往list中添加元素。
其中，append()方法是在list的末尾添加元素；insert()是在指定位置添加元素。
如下：
1 names.append('Jacob')
2 names
输出结果:
1 ['James', 'Michael', 'Emma', 'Emily', 'Jacob']
1 names.insert(1, 'Ava')
2 names
输出结果:
1 ['James', 'Ava', 'Michael', 'Emma', 'Emily', 'Jacob']
请注意，上述insert()方法中，“1”表示在第2个位置添加新的元素（索引是从0开始的）
删除list中的元素
• 用pop()方法删除list末尾的元素
1 names.pop()
http://liyangbit.com 46Chapter5. Python数据类型-List介绍 Python数据之道
输出结果:
1 'Jacob'
1 names
输出结果:
1 ['James', 'Ava', 'Michael', 'Emma', 'Emily']
• 删除指定位置的元素，用pop(i)方法
1 names.pop(0)
输出结果:
1 'James'
1 names
输出结果:
1 ['Ava', 'Michael', 'Emma', 'Emily']
修改list中的元素
如果需要修改list中的元素，可以直接通过list的索引进行赋值来实现，如下：
1 names[2] = 'Lemon'
2 names
输出结果:
1 ['Ava', 'Michael', 'Lemon', 'Emily']
5.3.2 列表操作符
列表可以进行相加“+”和相乘“*”运算，“+”相当于拼接列表，“*”相当于重复列表。
此外，还可以判断元素是否存在于列表中。
1 # 列 表 相 加
2 print(”列 表 相 加：”,[1,2,3]+['a','b'])
3
4 # 列 表 相 乘
5 print(”列 表 相 乘：”,['a','b']*3)
6
7 # 判 断 元 素 是 否 存 在 于 列 表 中
8 print(”判 断 元 素 是 否 存 在 于 列 表 中：”, 'a' in ['a', 'b'])
9 print(”判 断 元 素 是 否 存 在 于 列 表 中：”, 'a' not in ['a', 'b'])
http://liyangbit.com 47Chapter5. Python数据类型-List介绍 Python数据之道
输出结果:
1 列 表 相 加： [1, 2, 3, 'a', 'b']
2 列 表 相 乘： ['a', 'b', 'a', 'b', 'a', 'b']
3 判 断 元 素 是 否 存 在 于 列 表 中： True
4 判 断 元 素 是 否 存 在 于 列 表 中： False
5.3.3 列表函数&方法
列表的函数包括：
• len(list)，列表元素个数，即列表的长度
• max(list)，返回列表元素最大值
• min(list)，返回列表元素最小值
• list(sep)，将元组转为列表
列表的方法
列表的方法除了前面提到的增、改、删等方法外，还有其他一些方法，如下：
• list.count(obj)，统计某个元素在列表中出现的次数
• list.extend(seq)，在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）
• list.index(obj)，从列表中找出某个值第一个匹配项的索引位置
• list.remove(obj)，移除列表中某个值的第一个匹配项
• list.sort()，对原列表进行排序
• list.reverse()，对原列表进行反向排序
1 list1 = [1,2,3,9,6,3]
2 list2 = [3, 8, 5, 4, 7,4]
3 tuple1 = (3,9,6)
4
5 # 列 表 的 函 数
6 print(”列 表 最 大 值：”,max(list1))
7 print(”列 表 最 小 值：”,min(list1))
8 print(”将 元 组 转 为 列 表：”,list(tuple1))
9
10 # 列 表 的 方 法
11 print(”count：”,list1.count(3))
12
13 list1.extend(list2)
14 print(”extend：”,list1)
15
16 print(”index：”,list2.index(4))
17
18 list2.remove(4)
19 print(”remove：”,list2)
20
21 list2.sort()
22 print(”sort：”,list2)
http://liyangbit.com 48Chapter5. Python数据类型-List介绍 Python数据之道
23
24 list2.reverse()
25 print(”reverse：”,list2)
输出结果:
1 列 表 最 大 值： 9
2 列 表 最 小 值： 1
3 将 元 组 转 为 列 表： [3, 9, 6]
4 count： 2
5 extend： [1, 2, 3, 9, 6, 3, 3, 8, 5, 4, 7, 4]
6 index： 3
7 remove： [3, 8, 5, 7, 4]
8 sort： [3, 4, 5, 7, 8]
9 reverse： [8, 7, 5, 4, 3]
5.4 list中元素的类型可以多样
同一个list中的元素的类型可以是字符串（str）、整型（int）、布尔型（Boolean）、以及嵌套的list等，举例如下：
1 a_list = ['Lemon', 100, ['a', 'b', 'c', 'd'], True]
2 a_list
输出结果:
1 ['Lemon', 100, ['a', 'b', 'c', 'd'], True]
1 a_list[0]
输出结果:
1 'Lemon'
1 a_list[2]
输出结果:
1 ['a', 'b', 'c', 'd']
在上述的a_list中，第3个元素(即alist[2])其实本身也是一个list。
list中的元素也可以是list，这样的话，可以进行多重list的嵌套。
上述list，也可以按下述方式来理解。
1 b_list = ['a', 'b', 'c', 'd']
2 a_list = ['Lemon', 100, b_list, True]
3 a_list
输出结果:
http://liyangbit.com 49Chapter5. Python数据类型-List介绍 Python数据之道
1 ['Lemon', 100, ['a', 'b', 'c', 'd'], True]
针对a_list，如果想获取其中元素b_list里面的单个元素“b”，该如何实现呢？
其实，这个类似二维数组，用二维的索引可以获取，如下：
1 item_b = a_list[2][1]
2 item_b
输出结果:
1 'b'
上述[2]表示获取a_list的第3个元素，即b_list，[1]表示获取b_list的第2个元素，即“b”
5.5 list的切片（slices）
前面描述了list中单个元素如何获取，如果想获取其中连续的部分元素，该如何实现呢。
这里可以通过切片(slices)的形式来获取部分连续的元素。
1 c_list = ['James', 'Ava', 'Michael', 'Emma', 'Emily', 'Jacob']
2 c_list
输出结果:
1 ['James', 'Ava', 'Michael', 'Emma', 'Emily', 'Jacob']
list中以切片形式使用时，其结构可参考new_list[start: end : step]
其中“start”和“end”表示索引位置的开始和结束，选取的元素包含“start”，但不包含“end”。
“step”表示步长，默认情况下，“step”为1，演示如下：
Example-1:
1 c_list[1:3]
输出结果:
1 ['Ava', 'Michael']
example-1中，1:3表示切片选取的是第2个元素和第3个元素，即包含索引为1和索引为2的元素。相当于获
取c_list[1]和c_list[2]
Example-2:
1 c_list[::2]
输出结果:
http://liyangbit.com 50Chapter5. Python数据类型-List介绍 Python数据之道
1 ['James', 'Michael', 'Emily']
example-2中“start”和“end”为空的时候，默认是全选，即“start”为0，“end”为len(c_list)-1。
所以c_list[::2]表示的是从索引为0开始，以步长为2来选择元素。
接下来看以下结合上述两个例子的演示：
1 c_list[1:3:2]
输出结果:
1 ['Ava']
Example-3:
前面提到，new_list[-1]表示获取最后一个元素，在切片的步长“step”中，也可以是负数，比如“-1”：
1 c_list[::-1]
输出结果:
1 ['Jacob', 'Emily', 'Emma', 'Michael', 'Ava', 'James']
从上述结果可以看出，当“step”为-1时，我们发现是将列表进行了逆序排序。
再看看步长为“-2”时的结果：
1 c_list[::-2]
输出结果:
1 ['Jacob', 'Emma', 'Ava']
可以这么理解，当步长为正数时，是从左到右以该步长来获取列表中的元素；
而当步长为负数时，是从右到左以该步长的绝对值来获取列表中的元素。
Example-4:
如果想获取离散的元素，比如想获得第1、2、4个元素，能不能通过离散的索引值来获取呢？
我们先来试验一下：
1 c_list[0, 1, 3]
out:
TypeErrorTraceback(mostrecentcalllast)in()—->1c_list[0,1,3]
TypeError:listindicesmustbeintegersorslices,nottuple
从运行结果可以之道，直接取离散的索引号是不行的，它提示list的索引必须是整数或者slices。
http://liyangbit.com 51Chapter5. Python数据类型-List介绍 Python数据之道
那么，我们有没有方法来获取这些离散的元素呢？
方法肯定是有的，其中一种方法就是使用列表推导式。
5.6 列表推导式(ListComprehension)
5.6.1 列表推导式的一般情况
列表推导式的一般语法结构：
new_list = [x for x in iterable]
其中的iterable表示可迭代的对象，包括字符串（str）、列表（list），元组（tuple）、字典（dict）、集合（set），
以及生成器（generator）等。
先来看几个简单的例子：
1 str_list = [x.lower() for x in ”Lemon”]
2 str_list
输出结果:
1 ['l', 'e', 'm', 'o', 'n']
1 list_list = [x**2 for x in [1,2,3,4]]
2 list_list
输出结果:
1 [1, 4, 9, 16]
1 tuple_list = [x+2 for x in (1,2,3,4)]
2 tuple_list
输出结果:
1 [3, 4, 5, 6]
1 ge_list = [x for x in range(8)]
2 ge_list
输出结果:
1 [0, 1, 2, 3, 4, 5, 6, 7]
5.6.2 两层for循环的列表推导式
列表推导式中，可以同时包含多个for循环，比如同时两个for循环，如下：
http://liyangbit.com 52Chapter5. Python数据类型-List介绍 Python数据之道
1 [x**2+y for x in range(5) for y in range(4,7)]
输出结果:
1 [4, 5, 6, 5, 6, 7, 8, 9, 10, 13, 14, 15, 20, 21, 22]
上述结果跟下面的写法结果是一致的：
1 two_for_list = []
2
3 for x in range(5):
4 for y in range(4,7):
5 two_for_list.append(x**2+y)
6 print(two_for_list)
输出结果:
1 [4, 5, 6, 5, 6, 7, 8, 9, 10, 13, 14, 15, 20, 21, 22]
列表推导式中，三层或者三层以上的for循环一般很少用到。
5.6.3 使用两个变量来生成list
列表推导式也可以使用两个或多个变量来生成list，结合字典的使用，举例如下：
1 d = {'x': '1', 'y':'2', 'z':'4'}
2 d_list = [k+'='+v for k,v in d.items()]
3 d_list
输出结果:
1 ['z=4', 'y=2', 'x=1']
5.6.4 含if语句的列表推导式
列表推导式中还可以引入if条件语句，如下：
1 if_list = [x**2 for x in range(10) if x%2==0]
2 if_list
输出结果:
1 [0, 4, 16, 36, 64]
上述列表推导式，如果用普通的for循环来编写的话，内容如下：
1 if_list_1 = []
2 for x in range(10):
3 if x%2==0:
http://liyangbit.com 53Chapter5. Python数据类型-List介绍 Python数据之道
4 if_list_1.append(x**2)
5
6 print(if_list_1)
输出结果:
1 [0, 4, 16, 36, 64]
通过对比，可以看出列表推导式的方式更显得Pythonic。
当然，如果仅仅是编写上更简洁，可能不一定能显现出列表推导式的优势，下面我们来对比下上述两种方式的运行
效率。
1 %%timeit
2
3 if_list = [x**2 for x in range(10) if x%2==0]
4 if_list
输出结果:
1 100000 loops, best of 3: 6.06 µs per loop
1 %%timeit
2
3 def for_loop():
4 if_list_1 = []
5 for x in range(10):
6 if x%2==0:
7 if_list_1.append(x**2)
8 return if_list_1
9
10 for_loop()
输出结果:
1 100000 loops, best of 3: 6.76 µs per loop
从上面的运行结果进行对比，可以看出列表推导式的运行效率要高于普通的for循环编写方式。
因为，建议大家使用列表推导式，不仅写法更Pythonic，且运行效率更高。
包含if-else语句的列表推导式
1 [x**2 if x%2==0 else x+2 for x in range(10)]
输出结果:
1 [0, 3, 4, 5, 16, 7, 36, 9, 64, 11]
包含两个if语句的列表推导式
1 [x**2 for x in range(10) if x%2==0 if x%3==0]
http://liyangbit.com 54Chapter5. Python数据类型-List介绍 Python数据之道
输出结果:
1 [0, 36]
5.7 小结
如果想获取离散的元素，比如想获得第1、2、4个元素，能不能通过离散的索引值来获取呢？
前面已经实践过，直接通过离散的索引值来获取列表中的元素是不行的。
通过列表推导式的学习，我们可以换一种思路，来实现列表中离散的元素的获取，如下：
1 [c_list[i] for i in [0,1,3]]
输出结果:
1 ['James', 'Ava', 'Emma']
列表（list）作为python最基础也是最重要的数据类型之一，在python数据分析以及其他用途中有着重要的作
用，希望上述内容能对于熟悉list有所帮助。
http://liyangbit.com 556 Python 中字典合并的实现方法
6.1 背景
Python中有时候需要将字典进行合并，今天我们来探讨下这个主题。
先来看看下面的运行结果
1 x = {'a':1, 'b':2}
2 y = {'b':4, 'c':5}
3 z = x.update(y)
来看看z的返回结果：
1 print(z)
1 None
发现z返回的是None值，并不是我们想要的结果。
再来看一下此时x的返回结果，发现x已经是合并的结果。
1 x
1 {'a': 1, 'b': 4, 'c': 5}
那么有没有比较简单的实现方法呢。下面我们来探索一下：
6.2 Methon1：适用于Python3.5及以上版本呢
在Python3.5以上版本（含3.5），可以通过下列方法实现
1 z1 = {**x, **y}
2 z1
1 {'a': 1, 'b': 4, 'c': 5}
6.3 Method2：自定义函数
如果在Python2或Python3.4以及低于3.4的版本，要实现两个dict的合并，该怎样做呢？
56Chapter6. Python中字典合并的实现方法 Python数据之道
先定义一个函数，然后用自定义函数来运行
1 def merge(x,y):
2 z = x.copy()
3 z.update(y)
4 return z
1 z2 = merge(x,y)
2 z2
1 {'a': 1, 'b': 4, 'c': 5}
Method2在Python3.5以上版本也是可以运行的。
此外，请注意x中“b”=2，y中“b”=4,而运算结果中“b”=4，是用y中’b’的值来对字典进行更新。
6.4 多个dict进行合并
如果是多个dictionary需要进行合并呢？
同样可以通过自定义的形式来实现。
1 def merge_dicts(*dict_args):
2 result = {}
3 for item in dict_args:
4 result.update(item)
5 return result
1 x1 = {'a':1, 'b':2}
2 y1 = {'b':4, 'c':5}
3 x2 = {'d':8, 'e':10}
1 z3 = merge_dicts(x1,y1,x2)
2 z3
1 {'a': 1, 'b': 4, 'c': 5, 'd': 8, 'e': 10}
此方法在Python2和Python3下都是可以运行的
http://liyangbit.com 577 Jupyter Notebook 主题设置、字体修改等
作为用 Python 进行数据分析的人员，jupyter notebook 在平时使用的频率很高。但经常觉得 jupyter
notebook默认的风格不是很爽，总想换一换。今天，我们来分享下给jupyternotebook更换主题等内容的方法。
7.1 新的风格预览
首先来看看我现在使用的风格，如下：
notebook页面：
Figure7.1:notebook页面
代码文件页面：
Figure7.2:代码文件页面
58Chapter7. JupyterNotebook主题设置、字体修改等 Python数据之道
7.2 主题更换过程
如果你也想更换下jupyternotebook的默认风格，不妨接着往下看。
JupyterNotebook的默认主题是白色背景的，虽然也简洁大方，便于使用。但是在长时间使用默认界面后，我
只有一种感觉，就是亮瞎……无比怀念类似Pycharm或vscode中设置的黑色界面……
于是，想给jupyternotebook也换个黑色主题。我搜索了JupyterNotebook的themes，也就是自定义主
题。发现Github已经有大神早已解决了这个问题。本次，我们用到的库为jupyterthemes，这个第三方库有多个主
题可以更换，并且还可以更换字体类型以及大小等，基本满足我们的需求。jupyterthemes的github地址如下：
https://github.com/dunovank/jupyter-themes
默认情况下库的安装如下：
1 pip install jupyterthemes
但是，上述方法我没有安装成功，主要是获取该库的请求失败。于是，改用豆瓣的python库来源，安装的代码如
下：
1 pip install -i https://pypi.doubanio.com/simple/ jupyterthemes
Figure7.3:豆瓣源
改用豆瓣的来源后，成功安装了jupyterthemes，接下来，我们可以来设置自己喜欢的主题。在命令提示符下输
入下面的代码来查看有哪些主题可以选择。
1 jt -l
有如下一些主题可以选择
Figure7.4:可选主题
http://liyangbit.com 59Chapter7. JupyterNotebook主题设置、字体修改等 Python数据之道
有兴趣的同学可以尝试下这几个主题。设置主题的代码如下：
1 jt -t oceans16
上面设置的是使用oceans16这个主题，更换后的效果如下：
Figure7.5:oceans16主题
Figure7.6:oceans16主题
如果不喜欢上面的主题，可以使用下面的代码来恢复到默认主题，如下：
1 jt -r
对比其他几个主题后，相对来说，我更喜欢monokai以及onedork主题，最后，我选择使用monokai主题，
也就是本文开始的主题风格。
7.3 我的主题设置参数
我的monikai主题设置的详细参数如下：
1 jt -t monokai -f roboto -nf robotosans -tf robotosans -N -T -cellw 70% -dfs 10
-ofs 10
http://liyangbit.com 60Chapter7. JupyterNotebook主题设置、字体修改等 Python数据之道
乍一看，看到这串长长的代码，可能有点晕，但其实很好理解。-t是设置主题，-f设置代码的字体，-nf设置
notebook的字体，等等。更多的参数，请查看jupyterthemes的参数设置说明，如下：
Figure7.7:jupyterthemes的参数设置说明
怎么样，新的界面不错吧，心动不如行动，赶紧动手尝试下吧。
http://liyangbit.com 618 Jupyter Notebook 中自动补全代码
本文来继续介绍jupyternotebook中一些实际的技巧。本次要介绍的两个功能是：
（1）针对jupyternotebook中的Markdown文件自动生成目录
（2）自动补全代码
上述两个功能，都是由python的一个jupyter扩展插件Nbextensions库来实现。安装该库的命令如下：
1 python -m pip install jupyter_contrib_nbextensions
然后执行：
1 jupyter contrib nbextension install --user --skip-running-check
安装完成后，勾选Table of Contents以及Hinterland。其中Hinterland是用来自动补全代码的，这个
拓展的代码补全功能虽然没有PyCharm中的那么全面，但比没有是要好多了。设置如下：
Figure8.1:Hinterland设置
自动补全代码的效果如下：
62Chapter8. JupyterNotebook中自动补全代码 Python数据之道
Figure8.2:自动补全代码的效果
上图在文章中是gif的动态图，可以在下面文章中查看：
http://liyangbit.com/jupyter/jupyter-code-auto-complete/
http://liyangbit.com 639 Jupyter Notebook 输出 pdf 并支持中文显示
JupyterNotebook作为用Python进行数据分析的重要工具之一，其最大的特色是可以将代码和结果同步显示
在源文件里。JupyterNotebook可以用来演示，以及输出各种形式的文件，比如ipynb，html，md，py，pdf等，
本文主要来阐述如何将写好的内容输出为pdf格式的文件。在JupyterNotebook输出pdf过程中，相信不少同学因
为遇到难以解决的坑而退却，这里给大家分享下我的历程，希望对大家有所帮助。
系统环境：
• windows7,windows10
• Anancoda（基于Python3.6版）
9.1 安装顺序
我是按照下面的顺序安装的，缺少支持的话，pip install xxxxxx
1安装pandoc
https://github.com/jgm/pandoc/releases
2安装MiKTex
https://miktex.org/download
3中文支持
（1）直接修改tex模版文件。
首先找到article.tplx文件，我的Anaconda中的路径是D:\ProgramData\Anaconda3\pkgs\nbconvert
-5.3.1-py36h8dc0fde_0\Lib\site-packages\nbconvert\templates\latex\article.tplx
你可以根据自己的安装情况，找到该文件，然后用文本编辑器打开article.tplx，将“\documentclass[11pt
]{article}”修改为“\documentclass{ctexart}”，对比图示如下：
修改前：
64Chapter9. JupyterNotebook输出pdf并支持中文显示 Python数据之道
Figure9.1:修改前
修改后：
Figure9.2:修改后
（2）打开一个含有中文内容的jupyternotebook文件（.ipynb文件），在浏览器中打开，选择输出为pdf文件
（我这里是新建的一个空白的ipynb文档），如下：
http://liyangbit.com 65Chapter9. JupyterNotebook输出pdf并支持中文显示 Python数据之道
Figure9.3:ipynb文档
这里有可能可以正常的输出含有中文的pdf文档，如果已输出，那么恭喜你，已经成功啦。但我在输出含有中文内
容的pdf文档过程中，还遇到了一些问题，主要是显示“XXX.sty”文件缺失，这时候，需要安装缺失的文件，由于默
认安装情况下，经常会失败。此时，我们需要选择安装源，步骤图示如下：
首先，要通过点击“Change”来选择
http://liyangbit.com 66Chapter9. JupyterNotebook输出pdf并支持中文显示 Python数据之道
Figure9.4:安装缺失文件
按下面图示选项，点击“next”
Figure9.5:安装缺失文件
在选择安装源的时候，如果有中国的安装来源，请优先选择国内的，这样安装速度较快，成功率相对高些。如果没
有，可以多尝试几次其他国家和地区的，比如日本等地。
http://liyangbit.com 67Chapter9. JupyterNotebook输出pdf并支持中文显示 Python数据之道
Figure9.6:安装缺失文件：选择安装源
选择好后，点击安装即可。
Figure9.7:安装缺失文件
一般情况下，将上述缺失的多个.sty文件安装好后，是可以输出含中文内容的pdf文档的。至此，我们本次的目的
已基本完成。
http://liyangbit.com 68Chapter9. JupyterNotebook输出pdf并支持中文显示 Python数据之道
但在后面的使用过程中，我还发现了一个小bug，就是：如果.ipynb文件的命名中含有中文，则文件名输出是
不含中文的，不知道这个问题有没有同学可以提供下解决方案，欢迎献计献策~
本文参考以下内容：
Jupyter-Notebook-PDF输出的中文支持
http://liyangbit.com 6910 Anaconda 安装虚拟环境到指定路径
10.1 曾经的困扰
有段时间，想使用基于不同python版本的anaconda，就直接从官网下载了两个不同的anaconda版本进行
安装。刚开始的时候，还觉得也没啥问题。用了一小段时间，在安装其他的第三方库时，经常发现安装失败，并且经常
出现下面的问题：
Figure10.1:出现的问题
这个问题，我google、百度等查了好久，也没有解决好。后来，我把两个版本的anaconda都卸载了，重新安
装了其中一个版本，发现再安装其他第三方库时，上述问题就不存在了。很有可能上述问题就是同时安装两个版本的
anaconda引起的（不过我也不能完全肯定）。
10.2 安装虚拟环境
虽然只安装一个版本的anaconda，能顺利的运行，但有时候，还是需要运行基于不同python版本的anaconda
的。后来了解到，anaconda是可以在虚拟环境下运行不同python版本的。下面的步骤演示了我的安装过程，也希望
大家能避免一些坑。
70Chapter10. Anaconda安装虚拟环境到指定路径 Python数据之道
先说下我的安装环境：
• windows7,64位系统
• 目前安装了anaconda4.2.0版本（基于python3.5）
由于经常看到有说python3.6版本运行效率比3.5版本高，就有一种想安装3.6的冲动，但是由于部分python
库还不支持3.6，所以主要版本还是以3.5为主。anaconda安装python3.6版本的虚拟环境的步骤如下：
输入安装命令：
1 conda create -n py36 python=3.6
结果如下：
Figure10.2:安装演示
“-n”命令，会将虚拟环境安装在anaconda已安装路径默认的envs目录下。
可以看出，命令行中提示有一个警告，由于我的anaconda默认安装路径中有空格，提示可能会引起一些问题。
由于有这个警告，我就没有继续往下安装。另外，我的路径在C盘，考虑到可能空间也不多，还是不要继续往C盘装东
西了。于是想，要是能安装在其他指定路径且路径中没有空格，不就解决这个问题了么。
通过查阅anaconda的文档，发现是可以进行指定路径安装的。可以输入如下命令进行查看：
1 conda create --help
安装虚拟环境到指定路径的命令如下：
1 conda create --prefix=D:\python36\py36 python=3.6
http://liyangbit.com 71Chapter10. Anaconda安装虚拟环境到指定路径 Python数据之道
Figure10.3:安装到指定路径
上面的命令中，路径D:\python36是先建好的文件夹，py36是需要安装的虚拟环境名称。请注意，安装完成后，
虚拟环境的全称包含整个路径，为D:\python36\py36。激活指定路径下的虚拟环境的命令如下：
1 activate D:\python36\py36
退出虚拟环境的命令如下：
1 deactivate
Figure10.4:退出虚拟环境
http://liyangbit.com 72Chapter10. Anaconda安装虚拟环境到指定路径 Python数据之道
想要删除指定路径下的虚拟环境，使用如下的命令：
1 conda remove --prefix=D:\python36\py36 --all
Figure10.5:删除指定路径下的虚拟环境
10.3 虚拟环境下安装python库
如果还想继续安装python2.7的虚拟环境，方法跟安装python3.6是一致的。上述虚拟环境py27安装完成后，
激活后虚拟环境后，可以安装其他python库。比如安装requests库
1 pip install requests
是可以安装成功的。
如果遇到pip安是失败，可尝试用下述方法：（指定路径下安装）：
1 conda install -prefix=D:\pyenv\py27 package
请注意，默认路径下的命令是“conda install -n py27 package”。当然了，有一些库不论conda和pip
都无法直接安装，只能下载.whl进行安装。
最后，来查看已安装好的虚拟环境下都安装了那些python库，输入命令：
1 conda list
发现已安装好的虚拟环境py36下只安装了少量的基本库，如果也想用3.6版来进行科学计算，需要安装许多其他
的库，如果一个一个安装，则会费时费力，而且容易出错，这时anaconda提供了一个命令，可以把基于python3.6
版本的anaconda中的其他库一次安装好，命令如下：
1 conda install -prefix=D:\pyenv\py36 anaconda
http://liyangbit.com 73Chapter10. Anaconda安装虚拟环境到指定路径 Python数据之道
Figure10.6:安装Anaconda中所有的库
请特别注意，全部安装时，安装包会很多，安装时间比较长，同时占用安装空间也会比较大，请根据自己的需求选
择是否安装。我安装py36全部的库花了大概1个多小时。
http://liyangbit.com 7411 Numpy 的基本用法
Numpy是一个开源的Python科学计算库，它是python科学计算库的基础库，许多其他著名的科学计算库如
Pandas，Scikit-learn等都要用到Numpy库的一些功能。本文主要内容如下：
1. Numpy数组对象
2. 创建ndarray数组
3. Numpy的数值类型
4. ndarray数组的属性
5. ndarray数组的切片和索引
6. 处理数组形状
7. 数组的类型转换
8. numpy常用统计函数
9. 数组的广播
11.1 Numpy数组对象
Numpy中的多维数组称为ndarray，这是Numpy中最常见的数组对象。ndarray对象通常包含两个部分：
• ndarray数据本身
• 描述数据的元数据
Numpy数组的优势
• Numpy数组通常是由相同种类的元素组成的，即数组中的数据项的类型一致。这样有一个好处，由于知道数组
元素的类型相同，所以能快速确定存储数据所需空间的大小。
• Numpy数组能够运用向量化运算来处理整个数组，速度较快；而Python的列表则通常需要借助循环语句遍历
列表，运行效率相对来说要差。
• Numpy使用了优化过的CAPI，运算速度较快
关于向量化和标量化运算，对比下面的参考例子就可以看出差异
• 使用python的list进行循环遍历运算
1 def pySum():
2 a = list(range(10000))
3 b = list(range(10000))
4 c = []
5 for i in range(len(a)):
6 c.append(a[i]**2 + b[i]**2)
75Chapter11. Numpy的基本用法 Python数据之道
7
8 return c
1 %timeit pySum()
2
3 Out[]:
4 10 loops, best of 3: 49.4 ms per loop
• 使用numpy进行向量化运算
1 import numpy as np
2 def npSum():
3 a = np.arange(10000)
4 b = np.arange(10000)
5 c = a**2 + b**2
6 return c
1 %timeit npSum()
2
3 Out[]:
4 The slowest run took 262.56 times longer than the fastest. This could mean that
an intermediate result is being cached.
5 1000 loops, best of 3: 128 µs per loop
从上面的运行结果可以看出，numpy的向量化运算的效率要远远高于python的循环遍历运算（效率相差好几
百倍）。（1ms=1000µs）
11.2 创建ndarray数组
首先需要导入numpy库，在导入numpy库时通常使用“np”作为简写，这也是Numpy官方倡导的写法。当然，
你也可以选择其他简写的方式或者直接写numpy，但还是建议用“np”，这样你的程序能和大都数人的程序保持一致。
1 import numpy as np
创建ndarray数组的方式有很多种，这里介绍我使用的较多的几种：
Method1:基于list或tuple
1 # 一 维 数 组
2
3 # 基 于list
4 arr1 = np.array([1,2,3,4])
5 print(arr1)
6
7 # 基 于tuple
8 arr_tuple = np.array((1,2,3,4))
9 print(arr_tuple)
10
11 # 二 维 数 组 (2*3)
http://liyangbit.com 76Chapter11. Numpy的基本用法 Python数据之道
12 arr2 = np.array([[1,2,4], [3,4,5]])
13 arr2
14
15 Out[]:
16 [1 2 3 4]
17 [1 2 3 4]
18 array([[1, 2, 4],
19 [3, 4, 5]])
请注意：
• 一维数组用print输出的时候为[1234]，跟python的列表是有些差异的，没有“,”
• 在创建二维数组时，在每个子list外面还有一个“[]”，形式为“[[list1], [list2]]”
Method2:基于np.arange
1 # 一 维 数 组
2 arr1 = np.arange(5)
3 print(arr1)
4
5 # 二 维 数 组
6 arr2 = np.array([np.arange(3), np.arange(3)])
7 arr2
8
9 Out[]:
10 [0 1 2 3 4]
11 array([[0, 1, 2],
12 [0, 1, 2]])
Method3:基于arange以及reshape创建多维数组
1 # 创 建 三 维 数 组
2 arr = np.arange(24).reshape(2,3,4)
3 arr
4
5 Out[]:
6 array([[[ 0, 1, 2, 3],
7 [ 4, 5, 6, 7],
8 [ 8, 9, 10, 11]],
9
10 [[12, 13, 14, 15],
11 [16, 17, 18, 19],
12 [20, 21, 22, 23]]])
• 请注意：arange的长度与ndarray的维度的乘积要相等，即24=2X3X4
• 用numpy.random创建数组的方法，可以参考我的文章: 《为什么你用不好Numpy的random函数》
• 其他创建ndarray的方法，各位小伙伴们自己可以研究下。
http://liyangbit.com 77Chapter11. Numpy的基本用法 Python数据之道
11.3 Numpy的数值类型
Numpy的数值类型如下：
Figure11.1:Numpy的数值类型
每一种数据类型都有相应的数据转换函数，参考示例如下：
1 np.int8(12.334)
2
3 Out[]:
4 12
1 np.float64(12)
2
3 Out[]:
http://liyangbit.com 78Chapter11. Numpy的基本用法 Python数据之道
4 12.0
1 np.float(True)
2
3 Out[]:
4 1.0
1 bool(1)
2
3 Out[]:
4 True
在创建ndarray数组时，可以指定数值类型：
1 a = np.arange(5, dtype=float)
2 a
3
4 Out[]:
5 array([ 0., 1., 2., 3., 4.])
• 请注意，复数不能转换成为整数类型或者浮点数，比如下面的代码会运行出错
Figure11.2:Error
1 # float(42 + 1j)
11.4 ndarray数组的属性
• dtype属性，ndarray数组的数据类型，数据类型的种类，前面已描述。
1 np.arange(4, dtype=float)
2
3 Out[]:
4 array([ 0., 1., 2., 3.])
1 # 'D'表 示 复 数 类 型
2 np.arange(4, dtype='D')
3
http://liyangbit.com 79Chapter11. Numpy的基本用法 Python数据之道
4 Out[]:
5 array([ 0.+0.j, 1.+0.j, 2.+0.j, 3.+0.j])
1 np.array([1.22,3.45,6.779], dtype='int8')
2
3 Out[]:
4 array([1, 3, 6], dtype=int8)
• ndim属性，数组维度的数量
1 a = np.array([[1,2,3], [7,8,9]])
2 a.ndim
3
4 Out[]:
5 2
• shape属性，数组对象的尺度，对于矩阵，即n行m列,shape是一个元组（tuple）
1 a.shape
2
3 Out[]:
4 (2, 3)
• size属性用来保存元素的数量，相当于shape中nXm的值
1 a.size
2
3 Out[]:
4 6
• itemsize属性返回数组中各个元素所占用的字节数大小。
1 a.itemsize
2
3 Out[]:
4 4
• nbytes属性，如果想知道整个数组所需的字节数量，可以使用nbytes属性。其值等于数组的size属性值乘以
itemsize属性值。
1 a.nbytes
2
3 Out[]:
4 24
1 a.size*a.itemsize
2
3 Out[]:
4 24
• T属性，数组转置
http://liyangbit.com 80Chapter11. Numpy的基本用法 Python数据之道
1 b = np.arange(24).reshape(4,6)
2 b
3
4 Out[]:
5 array([[ 0, 1, 2, 3, 4, 5],
6 [ 6, 7, 8, 9, 10, 11],
7 [12, 13, 14, 15, 16, 17],
8 [18, 19, 20, 21, 22, 23]])
1 b.T
2
3 Out[]:
4 array([[ 0, 6, 12, 18],
5 [ 1, 7, 13, 19],
6 [ 2, 8, 14, 20],
7 [ 3, 9, 15, 21],
8 [ 4, 10, 16, 22],
9 [ 5, 11, 17, 23]])
• 复数的实部和虚部属性，real和imag属性
1 d = np.array([1.2+2j, 2+3j])
2 d
3
4 Out[]:
5 array([ 1.2+2.j, 2.0+3.j])
real属性返回数组的实部
1 d.real
2
3 Out[]:
4 array([ 1.2, 2. ])
imag属性返回数组的虚部
1 d.imag
2
3 Out[]:
4 array([ 2., 3.])
• flat属性，返回一个numpy.flatiter对象，即可迭代的对象。
1 e = np.arange(6).reshape(2,3)
2 e
3
4 Out[]:
5 array([[0, 1, 2],
6 [3, 4, 5]])
http://liyangbit.com 81Chapter11. Numpy的基本用法 Python数据之道
1 f = e.flat
2 f
3
4 Out[]:
5 <numpy.flatiter at 0x65eaca0>
1 for item in f:
2 print(item)
3
4 Out[]:
5 0
6 1
7 2
8 3
9 4
10 5
可通过位置进行索引，如下：
1 f[2]
2
3 Out[]:
4 2
1 f[[1,4]]
2
3 Out[]:
4 array([1, 4])
也可以进行赋值
1 e.flat=7
2 e
3
4 Out[]:
5 array([[7, 7, 7],
6 [7, 7, 7]])
1 e.flat[[1,4]]=1
2 e
3
4 Out[]:
5 array([[7, 1, 7],
6 [7, 1, 7]])
下图是对ndarray各种属性的一个小结
http://liyangbit.com 82Chapter11. Numpy的基本用法 Python数据之道
Figure11.3:ndarray各种属性小结
11.5 ndarray数组的切片和索引
• 一维数组
一维数组的切片和索引与python的list索引类似。
1 a = np.arange(7)
2 a
3
4 Out[]:
5 array([0, 1, 2, 3, 4, 5, 6])
1 a[1:4]
2
3 Out[]:
4 array([1, 2, 3])
1 # 每 间 隔2个 取 一 个 数
2 a[ : 6: 2]
http://liyangbit.com 83Chapter11. Numpy的基本用法 Python数据之道
3
4 Out[]:
5 array([0, 2, 4])
• 二维数组的切片和索引，如下所示：
Figure11.4:二维数组的切片和索引
11.6 处理数组形状
11.6.1 形状转换
• reshape()和resize()
1 b.reshape(4,3)
2
3 Out[]:
4 array([[ 0, 1, 2],
5 [ 3, 4, 5],
6 [ 6, 7, 8],
7 [ 9, 10, 11]])
1 b
2
3 Out[]:
4 array([[ 0, 1, 2, 3],
5 [ 4, 5, 6, 7],
6 [ 8, 9, 10, 11]])
1 b.resize(4,3)
http://liyangbit.com 84Chapter11. Numpy的基本用法 Python数据之道
2 b
3
4 Out[]:
5 array([[ 0, 1, 2],
6 [ 3, 4, 5],
7 [ 6, 7, 8],
8 [ 9, 10, 11]])
函数resize（）的作用跟reshape（）类似，但是会改变所作用的数组，相当于有inplace=True的效果
• ravel()和flatten()，将多维数组转换成一维数组，如下：
1 b.ravel()
2
3 Out[]:
4 array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])
1 b.flatten()
2
3 Out[]:
4 array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])
1 b
2
3 Out[]:
4 array([[ 0, 1, 2],
5 [ 3, 4, 5],
6 [ 6, 7, 8],
7 [ 9, 10, 11]])
两者的区别在于返回拷贝（copy）还是返回视图（view），flatten()返回一份拷贝，需要分配新的内存空间，对
拷贝所做的修改不会影响原始矩阵，而ravel()返回的是视图（view），会影响原始矩阵。
参考如下代码：
http://liyangbit.com 85Chapter11. Numpy的基本用法 Python数据之道
Figure11.5:拷贝vs视图
• 用tuple指定数组的形状，如下：
1 b.shape=(2,6)
2 b
3
4 Out[]:
5 array([[ 0, 1, 20, 3, 4, 5],
6 [ 6, 7, 8, 9, 10, 11]])
• 转置
前面描述了数组转置的属性（T），也可以通过transpose()函数来实现
1 b.transpose()
2
3 Out[]:
4 array([[ 0, 6],
5 [ 1, 7],
6 [20, 8],
7 [ 3, 9],
8 [ 4, 10],
9 [ 5, 11]])
11.6.2 6.2堆叠数组
http://liyangbit.com 86Chapter11. Numpy的基本用法 Python数据之道
1 b
2
3 Out[]:
4 array([[ 0, 1, 20, 3, 4, 5],
5 [ 6, 7, 8, 9, 10, 11]])
1 c = b*2
2 c
3
4 Out[]:
5 array([[ 0, 2, 40, 6, 8, 10],
6 [12, 14, 16, 18, 20, 22]])
• 水平叠加
hstack()
1 np.hstack((b,c))
2
3 Out[]:
4 array([[ 0, 1, 20, 3, 4, 5, 0, 2, 40, 6, 8, 10],
5 [ 6, 7, 8, 9, 10, 11, 12, 14, 16, 18, 20, 22]])
column_stack()函数以列方式对数组进行叠加，功能类似hstack（）
1 np.column_stack((b,c))
2
3 Out[]:
4 array([[ 0, 1, 20, 3, 4, 5, 0, 2, 40, 6, 8, 10],
5 [ 6, 7, 8, 9, 10, 11, 12, 14, 16, 18, 20, 22]])
• 垂直叠加
vstack()
1 np.vstack((b,c))
2
3 Out[]:
4 array([[ 0, 1, 20, 3, 4, 5],
5 [ 6, 7, 8, 9, 10, 11],
6 [ 0, 2, 40, 6, 8, 10],
7 [12, 14, 16, 18, 20, 22]])
row_stack()函数以行方式对数组进行叠加，功能类似vstack（）
1 np.row_stack((b,c))
2
3 Out[]:
4 array([[ 0, 1, 20, 3, 4, 5],
5 [ 6, 7, 8, 9, 10, 11],
6 [ 0, 2, 40, 6, 8, 10],
http://liyangbit.com 87Chapter11. Numpy的基本用法 Python数据之道
7 [12, 14, 16, 18, 20, 22]])
• concatenate()方法，通过设置axis的值来设置叠加方向
axis=1时，沿水平方向叠加
axis=0时，沿垂直方向叠加
1 np.concatenate((b,c),axis=1)
2
3 Out[]:
4 array([[ 0, 1, 20, 3, 4, 5, 0, 2, 40, 6, 8, 10],
5 [ 6, 7, 8, 9, 10, 11, 12, 14, 16, 18, 20, 22]])
1 np.concatenate((b,c),axis=0)
2
3 Out[]:
4 array([[ 0, 1, 20, 3, 4, 5],
5 [ 6, 7, 8, 9, 10, 11],
6 [ 0, 2, 40, 6, 8, 10],
7 [12, 14, 16, 18, 20, 22]])
由于针对数组的轴为0或1的方向经常会混淆，通过示意图，或许可以更好的理解。关于数组的轴方向示意图，以
及叠加的示意图，如下：
Figure11.6:数组的轴方向示意图
http://liyangbit.com 88Chapter11. Numpy的基本用法 Python数据之道
Figure11.7:叠加的示意图
深度叠加
这个有点烧脑，举个例子如下，自己可以体会下：
1 arr_dstack = np.dstack((b,c))
2 print(arr_dstack.shape)
3 arr_dstack
4
5 Out[]:
6 (2, 6, 2)
7
8 array([[[ 0, 0],
9 [ 1, 2],
10 [20, 40],
11 [ 3, 6],
12 [ 4, 8],
13 [ 5, 10]],
14
15 [[ 6, 12],
16 [ 7, 14],
17 [ 8, 16],
18 [ 9, 18],
19 [10, 20],
20 [11, 22]]])
叠加前，b和c均是shape为（2,6）的二维数组，叠加后，arr_dstack是shape为（2,6,2）的三维数组。
深度叠加的示意图如下：
http://liyangbit.com 89Chapter11. Numpy的基本用法 Python数据之道
Figure11.8:深度叠加的示意图
11.6.3 数组的拆分
跟数组的叠加类似，数组的拆分可以分为横向拆分、纵向拆分以及深度拆分。涉及的函数为hsplit()、vsplit()、
dsplit()以及split()
1 b
2
3 Out[]:
4 array([[ 0, 1, 20, 3, 4, 5],
5 [ 6, 7, 8, 9, 10, 11]])
• 沿横向轴拆分（axis=1）
1 np.hsplit(b, 2)
2
3 Out[]:
4 [array([[ 0, 1, 20],
5 [ 6, 7, 8]]), array([[ 3, 4, 5],
6 [ 9, 10, 11]])]
1 np.split(b,2, axis=1)
2
3 Out[]:
4 [array([[ 0, 1, 20],
5 [ 6, 7, 8]]), array([[ 3, 4, 5],
6 [ 9, 10, 11]])]
• 沿纵向轴拆分（axis=0）
http://liyangbit.com 90Chapter11. Numpy的基本用法 Python数据之道
1 np.vsplit(b, 2)
2
3 Out[]:
4 [array([[ 0, 1, 20, 3, 4, 5]]), array([[ 6, 7, 8, 9, 10, 11]])]
1 np.split(b,2,axis=0)
2
3 Out[]:
4 [array([[ 0, 1, 20, 3, 4, 5]]), array([[ 6, 7, 8, 9, 10, 11]])]
• 深度拆分
1 arr_dstack
2
3 Out[]:
4 array([[[ 0, 0],
5 [ 1, 2],
6 [20, 40],
7 [ 3, 6],
8 [ 4, 8],
9 [ 5, 10]],
10
11 [[ 6, 12],
12 [ 7, 14],
13 [ 8, 16],
14 [ 9, 18],
15 [10, 20],
16 [11, 22]]])
1 np.dsplit(arr_dstack,2)
2
3 Out[]:
4 [array([[[ 0],
5 [ 1],
6 [20],
7 [ 3],
8 [ 4],
9 [ 5]],
10
11 [[ 6],
12 [ 7],
13 [ 8],
14 [ 9],
15 [10],
16 [11]]]), array([[[ 0],
17 [ 2],
18 [40],
19 [ 6],
20 [ 8],
21 [10]],
22
http://liyangbit.com 91Chapter11. Numpy的基本用法 Python数据之道
23 [[12],
24 [14],
25 [16],
26 [18],
27 [20],
28 [22]]])]
拆分的结果是原来的三维数组拆分成为两个二维数组。这个烧脑的拆分过程可以自行分析下~~
11.7 数组的类型转换
• 数组转换成list，使用tolist()
1 b
2
3 Out[]:
4 array([[ 0, 1, 20, 3, 4, 5],
5 [ 6, 7, 8, 9, 10, 11]])
1 b.tolist()
2
3 Out[]:
4 [[0, 1, 20, 3, 4, 5], [6, 7, 8, 9, 10, 11]]
• 转换成指定类型，astype()函数
1 b.astype(float)
2
3 Out[]:
4 array([[ 0., 1., 20., 3., 4., 5.],
5 [ 6., 7., 8., 9., 10., 11.]])
11.8 numpy常用统计函数
常用的函数如下,请注意函数在使用时需要指定axis轴的方向，若不指定，默认统计整个数组。
• np.sum()，返回求和
• np.mean()，返回均值
• np.max()，返回最大值
• np.min()，返回最小值
• np.ptp()，数组沿指定轴返回最大值减去最小值，即（max-min）
• np.std()，返回标准偏差（standarddeviation）
• np.var()，返回方差（variance）
• np.cumsum()，返回累加值
• np.cumprod()，返回累乘积值
http://liyangbit.com 92Chapter11. Numpy的基本用法 Python数据之道
1 b
2
3 Out[]:
4 array([[ 0, 1, 20, 3, 4, 5],
5 [ 6, 7, 8, 9, 10, 11]])
1 np.max(b)
2
3 Out[]:
4 20
1 # 沿axis=1轴 方 向 统 计
2 np.max(b,axis=1)
3
4 Out[]:
5 array([20, 11])
1 # 沿axis=0轴 方 向 统 计
2 np.max(b,axis=0)
3
4 Out[]:
5 array([ 6, 7, 20, 9, 10, 11])
1 np.min(b)
2
3 Out[]:
4 0
• np.ptp()，返回整个数组的最大值减去最小值，如下：
1 np.ptp(b)
2
3 Out[]:
4 20
1 # 沿axis=0轴 方 向
2 np.ptp(b, axis=0)
3
4 Out[]:
5 array([ 6, 6, 12, 6, 6, 6])
1 # 沿axis=1轴 方 向
2 np.ptp(b, axis=1)
3
4 Out[]:
5 array([20, 5])
• np.cumsum()，沿指定轴方向进行累加
http://liyangbit.com 93Chapter11. Numpy的基本用法 Python数据之道
1 b.resize(4,3)
2 b
3
4 Out[]:
5 array([[ 0, 1, 20],
6 [ 3, 4, 5],
7 [ 6, 7, 8],
8 [ 9, 10, 11]])
1 np.cumsum(b, axis=1)
2
3 Out[]:
4 array([[ 0, 1, 21],
5 [ 3, 7, 12],
6 [ 6, 13, 21],
7 [ 9, 19, 30]], dtype=int32)
1 np.cumsum(b, axis=0)
2
3 Out[]:
4 array([[ 0, 1, 20],
5 [ 3, 5, 25],
6 [ 9, 12, 33],
7 [18, 22, 44]], dtype=int32)
• np.cumprod()，沿指定轴方向进行累乘积（Returnthecumulativeproductoftheelementsalong
thegivenaxis）
1 np.cumprod(b,axis=1)
2
3 Out[]:
4 array([[ 0, 0, 0],
5 [ 3, 12, 60],
6 [ 6, 42, 336],
7 [ 9, 90, 990]], dtype=int32)
1 np.cumprod(b,axis=0)
2
3 Out[]:
4 array([[ 0, 1, 20],
5 [ 0, 4, 100],
6 [ 0, 28, 800],
7 [ 0, 280, 8800]], dtype=int32)
11.9 数组的广播
当数组跟一个标量进行数学运算时，标量需要根据数组的形状进行扩展，然后执行运算。这个扩展的过程称为“广
播（broadcasting）”
http://liyangbit.com 94Chapter11. Numpy的基本用法 Python数据之道
1 b
2
3 Out[]:
4 array([[ 0, 1, 20],
5 [ 3, 4, 5],
6 [ 6, 7, 8],
7 [ 9, 10, 11]])
1 d = b + 2
2 d
3
4 Out[]:
5 array([[ 2, 3, 22],
6 [ 5, 6, 7],
7 [ 8, 9, 10],
8 [11, 12, 13]])
11.10 写在最后
numpy涵盖的内容其实是非常丰富的，本文仅仅介绍了numpy一些常用的基本功能，算是对numpy的一个
入门级的简单的较为全面的描述。
numpy官方的《NumpyReference》文档，光页面数量就有1000+页，如想要系统的学习numpy，建议仔细
阅读官方的参考文档，可在其官方网站进行查阅。当然，资料都是英文版的，可能看起来难度稍微大点，看习惯了就好。
本文涉及的代码量比较多，如需要查看源代码，请在微信公众号“Python数据之道”（ID：PyDataLab）后台回
复关键字“code”。
http://liyangbit.com 9512 Numpy 的 random 函数简介
在python数据分析的学习和应用过程中，经常需要用到numpy的随机函数，由于随机函数random的功能比
较多，经常会混淆或记不住，下面我们一起来汇总学习下。
1 import numpy as np
12.1 numpy.random.rand()
numpy.random.rand(d0,d1,…,dn)
• rand函数根据给定维度生成[0,1)之间的数据，包含0，不包含1
• dn表格每个维度
• 返回值为指定维度的array
1 np.random.rand(4,2)
2
3 Out[]:
4 array([[ 0.02173903, 0.44376568],
5 [ 0.25309942, 0.85259262],
6 [ 0.56465709, 0.95135013],
7 [ 0.14145746, 0.55389458]])
1 np.random.rand(4,3,2) # shape: 4*3*2
2
3 Out[]:
4 array([[[ 0.08256277, 0.11408276],
5 [ 0.11182496, 0.51452019],
6 [ 0.09731856, 0.18279204]],
7
8 [[ 0.74637005, 0.76065562],
9 [ 0.32060311, 0.69410458],
10 [ 0.28890543, 0.68532579]],
11
12 [[ 0.72110169, 0.52517524],
13 [ 0.32876607, 0.66632414],
14 [ 0.45762399, 0.49176764]],
15
16 [[ 0.73886671, 0.81877121],
17 [ 0.03984658, 0.99454548],
18 [ 0.18205926, 0.99637823]]])
96Chapter12. Numpy的random函数简介 Python数据之道
12.2 numpy.random.randn()
numpy.random.randn(d0,d1,…,dn)
• randn函数返回一个或一组样本，具有标准正态分布。
• dn表格每个维度
• 返回值为指定维度的array
1 np.random.randn() # 当 没 有 参 数 时， 返 回 单 个 数 据
2
3 Out[]:
4 -1.1241580894939212
1 np.random.randn(2,4)
2
3 Out[]:
4 array([[ 0.27795239, -2.57882503, 0.3817649 , 1.42367345],
5 [-1.16724625, -0.22408299, 0.63006614, -0.41714538]])
1 np.random.randn(4,3,2)
2
3 Out[]:
4 array([[[ 1.27820764, 0.92479163],
5 [-0.15151257, 1.3428253 ],
6 [-1.30948998, 0.15493686]],
7
8 [[-1.49645411, -0.27724089],
9 [ 0.71590275, 0.81377671],
10 [-0.71833341, 1.61637676]],
11
12 [[ 0.52486563, -1.7345101 ],
13 [ 1.24456943, -0.10902915],
14 [ 1.27292735, -0.00926068]],
15
16 [[ 0.88303 , 0.46116413],
17 [ 0.13305507, 2.44968809],
18 [-0.73132153, -0.88586716]]])
标准正态分布介绍
• 标准正态分布—standardnormaldistribution
• 标准正态分布又称为u分布，是以0为均值、以1为标准差的正态分布，记为N（0，1）。
12.3 numpy.random.randint()
12.3.1 numpy.random.randint()
numpy.random.randint(low,high=None,size=None,dtype=“l”)
http://liyangbit.com 97Chapter12. Numpy的random函数简介 Python数据之道
• 返回随机整数，范围区间为[low,high），包含low，不包含high
• 参数：low为最小值，high为最大值，size为数组维度大小，dtype为数据类型，默认的数据类型是np.int
• high没有填写时，默认生成随机数的范围是[0，low)
1 np.random.randint(1,size=5) # 返 回[0,1)之 间 的 整 数， 所 以 只 有0
2
3 Out[]:
4 array([0, 0, 0, 0, 0])
1 np.random.randint(1,5) # 返 回1个[1,5)时 间 的 随 机 整 数
2
3 Out[]:
4 4
1 np.random.randint(-5,5,size=(2,2))
2
3 Out[]:
4 array([[ 2, -1],
5 [ 2, 0]])
12.3.2 numpy.random.random_integers
numpy.random.random_integers(low,high=None,size=None)
• 返回随机整数，范围区间为[low,high]，包含low和high
• 参数：low为最小值，high为最大值，size为数组维度大小
• high没有填写时，默认生成随机数的范围是[1，low]
该函数在最新的numpy版本中已被替代，建议使用randint函数
1 np.random.random_integers(1,size=5)
2
3 Out[]:
4 array([1, 1, 1, 1, 1])
12.4 生成[0,1)之间的浮点数
• numpy.random.random_sample(size=None)
• numpy.random.random(size=None)
• numpy.random.ranf(size=None)
• numpy.random.sample(size=None)
1 print('-----------random_sample--------------')
2 print(np.random.random_sample(size=(2,2)))
3 print('-----------random--------------')
4 print(np.random.random(size=(2,2)))
http://liyangbit.com 98Chapter12. Numpy的random函数简介 Python数据之道
5 print('-----------ranf--------------')
6 print(np.random.ranf(size=(2,2)))
7 print('-----------sample--------------')
8 print(np.random.sample(size=(2,2)))
9
10
11 Out[]:
12 -----------random_sample--------------
13 [[ 0.34966859 0.85655008]
14 [ 0.16045328 0.87908218]]
15 -----------random--------------
16 [[ 0.25303772 0.45417512]
17 [ 0.76053763 0.12454433]]
18 -----------ranf--------------
19 [[ 0.0379055 0.51288667]
20 [ 0.71819639 0.97292903]]
21 -----------sample--------------
22 [[ 0.59942807 0.80211491]
23 [ 0.36233939 0.12607092]]
12.5 numpy.random.choice()
numpy.random.choice(a,size=None,replace=True,p=None)
• 从给定的一维数组中生成随机数
• 参数：a为一维数组类似数据或整数；size为数组维度；p为数组中的数据出现的概率
• a为整数时，对应的一维数组为np.arange(a)
1 np.random.choice(5,3)
2
3 Out[]:
4 array([4, 1, 4])
1 np.random.choice(5, 3, replace=False)
2 # 当replace为False时， 生 成 的 随 机 数 不 能 有 重 复 的 数 值
3
4 Out[]:
5 array([0, 3, 1])
1 np.random.choice(5,size=(3,2))
2
3 Out[]:
4 array([[1, 0],
5 [4, 2],
6 [3, 3]])
1 demo_list = ['lenovo', 'sansumg','moto','xiaomi', 'iphone']
2 np.random.choice(demo_list,size=(3,3))
http://liyangbit.com 99Chapter12. Numpy的random函数简介 Python数据之道
3
4 Out[]:
5 array([['moto', 'iphone', 'xiaomi'],
6 ['lenovo', 'xiaomi', 'xiaomi'],
7 ['xiaomi', 'lenovo', 'iphone']],
8 dtype='<U7')
• 参数p的长度与参数a的长度需要一致；
• 参数p为概率，p里的数据之和应为1
1 demo_list = ['lenovo', 'sansumg','moto','xiaomi', 'iphone']
2 np.random.choice(demo_list,size=(3,3), p=[0.1,0.6,0.1,0.1,0.1])
3
4 Out[]:
5 array([['sansumg', 'sansumg', 'sansumg'],
6 ['sansumg', 'sansumg', 'sansumg'],
7 ['sansumg', 'xiaomi', 'iphone']],
8 dtype='<U7')
12.6 numpy.random.seed()
• np.random.seed()的作用：使得随机数据可预测。
• 当我们设置相同的seed，每次生成的随机数相同。如果不设置seed，则每次会生成不同的随机数
1 np.random.seed(0)
2 np.random.rand(5)
3
4 Out[]:
5 array([ 0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
1 np.random.seed(1676)
2 np.random.rand(5)
3
4 Out[]:
5 array([ 0.39983389, 0.29426895, 0.89541728, 0.71807369, 0.3531823 ])
1 np.random.seed(1676)
2 np.random.rand(5)
3
4 Out[]:
5 array([ 0.39983389, 0.29426895, 0.89541728, 0.71807369, 0.3531823 ])
http://liyangbit.com 10013 Numpy 中 Meshgrid 函数介绍及 2 种应用场景
近期在好几个地方都看到meshgrid的使用，虽然之前也注意到meshgrid的用法。但总觉得印象不深刻，不是
太了解meshgrid的应用场景。所以，本文将进一步介绍Numpy中meshgrid的用法。
13.1 Meshgrid函数的基本用法
在Numpy的官方文章里，meshgrid函数的英文描述也显得文绉绉的，理解起来有些难度。可以这么理解，
meshgrid函数用两个坐标轴上的点在平面上画网格。
用法：
[X,Y]=meshgrid(x,y)
[X,Y]=meshgrid(x)与[X,Y]=meshgrid(x,x)是等同的
[X,Y,Z]=meshgrid(x,y,z)生成三维数组，可用来计算三变量的函数和绘制三维立体图
这里，主要以[X,Y]=meshgrid(x,y)为例，来对该函数进行介绍。[X,Y] = meshgrid(x,y)将向量x和y
定义的区域转换成矩阵X和Y,其中矩阵X的行向量是向量x的简单复制，而矩阵Y的列向量是向量y的简单复制(注：
下面代码中X和Y均是数组，在文中统一称为矩阵了)。假设x是长度为m的向量，y是长度为n的向量，则最终生成
的矩阵X和Y的维度都是n*m（注意不是m*n）。文字描述可能不是太好理解，下面通过代码演示下：
加载数据
1 import numpy as np
2 import matplotlib.pyplot as plt
3
4 %matplotlib inline
5
6 m, n = (5, 3)
7 x = np.linspace(0, 1, m)
8 y = np.linspace(0, 1, n)
9
10 X, Y = np.meshgrid(x,y)
查看向量x和向量y
1 x
2
3 out:
4 array([ 0. , 0.25, 0.5 , 0.75, 1. ])
101Chapter13. Numpy中Meshgrid函数介绍及2种应用场景 Python数据之道
5
6 y
7
8 out:
9 array([ 0. , 0.5, 1. ])
查看矩阵X和矩阵Y
1 X
2
3 out:
4 array([[ 0. , 0.25, 0.5 , 0.75, 1. ],
5 [ 0. , 0.25, 0.5 , 0.75, 1. ],
6 [ 0. , 0.25, 0.5 , 0.75, 1. ]])
7
8 Y
9
10 out:
11 array([[ 0. , 0. , 0. , 0. , 0. ],
12 [ 0.5, 0.5, 0.5, 0.5, 0.5],
13 [ 1. , 1. , 1. , 1. , 1. ]])
查看矩阵对应的维度
1 X.shape
2
3 out:
4 (3, 5)
5
6 Y.shape
7
8 out:
9 (3, 5)
meshgrid函数的运行过程，可以通过下面的示意图来加深理解：
http://liyangbit.com 102Chapter13. Numpy中Meshgrid函数介绍及2种应用场景 Python数据之道
Figure13.1:meshgrid
再者，也可以通过在matplotlib中进行可视化，来查看函数运行后得到的网格化数据的结果
1 plt.plot(X, Y, marker='.', color='blue', linestyle='none')
2 plt.show()
Figure13.2:result
当然，我们也可以获得网格平面上坐标点的数据，如下：
1 z = [i for i in zip(X.flat,Y.flat)]
2 z
http://liyangbit.com 103Chapter13. Numpy中Meshgrid函数介绍及2种应用场景 Python数据之道
3
4 out:
5 [(0.0, 0.0),
6 (0.25, 0.0),
7 (0.5, 0.0),
8 (0.75, 0.0),
9 (1.0, 0.0),
10 (0.0, 0.5),
11 (0.25, 0.5),
12 (0.5, 0.5),
13 (0.75, 0.5),
14 (1.0, 0.5),
15 (0.0, 1.0),
16 (0.25, 1.0),
17 (0.5, 1.0),
18 (0.75, 1.0),
19 (1.0, 1.0)]
13.2 Meshgrid函数的一些应用场景
Meshgrid函数常用的场景有等高线绘制及机器学习中SVC超平面的绘制（二维场景下）。分别图示如下：
（1）等高线
Figure13.3:contour
（2）SVC中超平面的绘制：
http://liyangbit.com 104Chapter13. Numpy中Meshgrid函数介绍及2种应用场景 Python数据之道
Figure13.4:svc
关于场景（1）和场景（2），将在后续的文章里做进一步描述。当然，可能还有些其他场景，这里就不做进一步介绍
了。#Pandas:如何将一列中的文本拆分为多行？
在数据处理过程中，经常会遇到以下类型的数据：
http://liyangbit.com 105Chapter13. Numpy中Meshgrid函数介绍及2种应用场景 Python数据之道
Figure13.5:脏数据
在同一列中，本该分别填入多行中的数据，被填在一行里了，然而在分析的时候，需要拆分成为多行。在上图中，
列名为“Country”，index为4和5的单元格内，值为“UK/Australia”和“UK/Netherland”。今天，我们来介绍
将含有多值的内容分拆成多行的几种方法。加载数据如下：
1 import pandas as pd
2
3 df = pd.DataFrame({'Country':['China','US','Japan','EU','UK/Australia', 'UK/
Netherland'],
4 'Number':[100, 150, 120, 90, 30, 2],
5 'Value': [1, 2, 3, 4, 5, 6],
6 'label': list('abcdef')})
7 df
8
9 Out[2]:
10 Country Number Value label
11 0 China 100 1 a
12 1 US 150 2 b
13 2 Japan 120 3 c
14 3 EU 90 4 d
15 4 UK/Australia 30 5 e
16 5 UK/Netherland 2 6 f
13.3 Method-1
分为如下几步：1. 将含有多值的列进行拆分，然后通过stack()方法进行变换，并通过index的设置来完成1.
用drop()方法从DataFrame中删除含有多值的列1.然后用join()方法来合并
http://liyangbit.com 106Chapter13. Numpy中Meshgrid函数介绍及2种应用场景 Python数据之道
1 df.drop('Country', axis=1).join(df['Country'].str.split('/', expand=True).stack
().reset_index(level=1, drop=True).rename('Country'))
2 Out[3]:
3 Number Value label Country
4 0 100 1 a China
5 1 150 2 b US
6 2 120 3 c Japan
7 3 90 4 d EU
8 4 30 5 e UK
9 4 30 5 e Australia
10 5 2 6 f UK
11 5 2 6 f Netherland
过程分步介绍
1 df['Country'].str.split('/', expand=True).stack()
2 Out[4]:
3 0 0 China
4 1 0 US
5 2 0 Japan
6 3 0 EU
7 4 0 UK
8 1 Australia
9 5 0 UK
10 1 Netherland
11 dtype: object
12
13 df['Country'].str.split('/', expand=True).stack().reset_index(level=1, drop=
True)
14 Out[5]:
15 0 China
16 1 US
17 2 Japan
18 3 EU
19 4 UK
20 4 Australia
21 5 UK
22 5 Netherland
23 dtype: object
24
25 df['Country'].str.split('/', expand=True).stack().reset_index(level=1, drop=
True).rename('Country')
26 Out[6]:
27 0 China
28 1 US
29 2 Japan
30 3 EU
31 4 UK
32 4 Australia
33 5 UK
34 5 Netherland
http://liyangbit.com 107Chapter13. Numpy中Meshgrid函数介绍及2种应用场景 Python数据之道
35 Name: Country, dtype: object
36
37 df.drop('Country', axis=1)
38 Out[7]:
39 Number Value label
40 0 100 1 a
41 1 150 2 b
42 2 120 3 c
43 3 90 4 d
44 4 30 5 e
45 5 2 6 f
13.4 Method-2
该方法的思路跟Method-1基本是一样的，只是在具体的细节方面有些差异。代码如下：
1
2 df['Country'].str.split('/', expand=True).stack().reset_index(level=0).
set_index('level_0').rename(columns={0:'Country'}).join(df.drop('Country',
axis=1))
3 Out[8]:
4 Country Number Value label
5 0 China 100 1 a
6 1 US 150 2 b
7 2 Japan 120 3 c
8 3 EU 90 4 d
9 4 UK 30 5 e
10 4 Australia 30 5 e
11 5 UK 2 6 f
12 5 Netherland 2 6 f
过程分步介绍如下：
1 df['Country'].str.split('/', expand=True).stack().reset_index(level=0)
2 Out[9]:
3 level_0 0
4 0 0 China
5 0 1 US
6 0 2 Japan
7 0 3 EU
8 0 4 UK
9 1 4 Australia
10 0 5 UK
11 1 5 Netherland
12
13 df['Country'].str.split('/', expand=True).stack().reset_index(level=0).
set_index('level_0')
14 Out[10]:
15 0
http://liyangbit.com 108Chapter13. Numpy中Meshgrid函数介绍及2种应用场景 Python数据之道
16 level_0
17 0 China
18 1 US
19 2 Japan
20 3 EU
21 4 UK
22 4 Australia
23 5 UK
24 5 Netherland
25
26 df['Country'].str.split('/', expand=True).stack().reset_index(level=0).
set_index('level_0').rename(columns={0:'Country'})
27 Out[11]:
28 Country
29 level_0
30 0 China
31 1 US
32 2 Japan
33 3 EU
34 4 UK
35 4 Australia
36 5 UK
37 5 Netherland
38
39 df.drop('Country', axis=1)
40 Out[12]:
41 Number Value label
42 0 100 1 a
43 1 150 2 b
44 2 120 3 c
45 3 90 4 d
46 4 30 5 e
47 5 2 6 f
闲谈
当然，将某列中含有多值的单元拆分成多行，还有其他方法，各位小伙伴们可以研究下~~
http://liyangbit.com 10914 Pandas 日期数据处理
原文标题：Pandas日期数据处理：如何按日期筛选、显示及统计数据
Pandas有着强大的日期数据处理功能，本期我们来了解下pandas处理日期数据的一些基本功能，主要包括以
下三个方面：
• 按日期筛选数据
• 按日期显示数据
• 按日期统计数据
运行环境为windows系统，64位，python3.5。
14.1 读取并整理数据
• 首先引入pandas库
1 import pandas as pd
• 从csv文件中读取数据
1 df = pd.read_csv('date.csv', header=None)
2 print(df.head(2))
1 0 1
2 0 2013-10-24 3
3 1 2013-10-25 4
• 整理数据
1 df.columns = ['date','number']
2 df['date'] = pd.to_datetime(df['date']) #将 数 据 类 型 转 换 为 日 期 类 型
3 df = df.set_index('date') # 将date设 置 为index
4 print(df.head(2))
5 print(df.tail(2))
6 print(df.shape)
1 number
2 date
3 2013-10-24 3
4 2013-10-25 4
5 number
110Chapter14. Pandas日期数据处理 Python数据之道
6 date
7 2017-02-14 6
8 2017-02-22 6
9 (425, 1)
• df的行数一共是425行。
查看Dataframe的数据类型
1 print(type(df))
2 print(df.index)
3 print(type(df.index))
1 <class 'pandas.core.frame.DataFrame'>
2 DatetimeIndex(['2013-10-24', '2013-10-25', '2013-10-29', '2013-10-30',
3 '2013-11-04', '2013-11-06', '2013-11-08', '2013-11-12',
4 '2013-11-14', '2013-11-25',
5 ...
6 '2017-01-03', '2017-01-07', '2017-01-14', '2017-01-17',
7 '2017-01-23', '2017-01-25', '2017-01-26', '2017-02-07',
8 '2017-02-14', '2017-02-22'],
9 dtype='datetime64[ns]', name='date', length=425, freq=None)
10 <class 'pandas.tseries.index.DatetimeIndex'>
构造Series类型数据
1 s = pd.Series(df['number'], index=df.index)
2 print(type(s))
3 s.head(2)
1 <class 'pandas.core.series.Series'>
2
3 date
4 2013-10-24 3
5 2013-10-25 4
6 Name: number, dtype: int64
14.2 按日期筛选数据
按年度获取数据
1 print('---------获 取2013年 的 数 据-----------')
2 print(df['2013'].head(2)) # 获 取2013年 的 数 据
3 print(df['2013'].tail(2)) # 获 取2013年 的 数 据
1 ---------获 取2013年 的 数 据-----------
2 number
3 date
4 2013-10-24 3
http://liyangbit.com 111Chapter14. Pandas日期数据处理 Python数据之道
5 2013-10-25 4
6 number
7 date
8 2013-12-27 2
9 2013-12-30 2
获取2016至2017年的数据
1 print('---------获 取2016至2017年 的 数 据-----------')
2 print(df['2016':'2017'].head(2)) #获 取2016至2017年 的 数 据
3 print(df['2016':'2017'].tail(2)) #获 取2016至2017年 的 数 据
1 ---------获 取2016至2017年 的 数 据-----------
2 number
3 date
4 2016-01-04 4
5 2016-01-07 6
6 number
7 date
8 2017-02-14 6
9 2017-02-22 6
获取某月的数据
1 print('---------获 取 某 月 的 数 据-----------')
2 print(df['2013-11']) # 获 取 某 月 的 数 据
1 ---------获 取 某 月 的 数 据-----------
2 number
3 date
4 2013-11-04 1
5 2013-11-06 3
6 2013-11-08 1
7 2013-11-12 5
8 2013-11-14 2
9 2013-11-25 1
10 2013-11-29 1
获取具体某天的数据
• 请注意dataframe类型的数据，获取具体某天的数据时，跟series是有些差异的，详细情况如下述代码所示：
1 # 按 日 期 筛 选 数 据
2 print('---------获 取 具 体 某 天 的 数 据-----------')
3 # 获 取 具 体 某 天 的 数 据
4 print(s['2013-11-06'])
5
6 # 获 取 具 体 某 天 的 数 据， 用datafrme直 接 选 取 某 天 时 会 报 错， 而series的 数 据 就 没 有 问 题
7 # print(df['2013-11-06'])
8
9 #可 以 考 虑 用 区 间 来 获 取 某 天 的 数 据
http://liyangbit.com 112Chapter14. Pandas日期数据处理 Python数据之道
10 print(df['2013-11-06':'2013-11-06'])
1 ---------获 取 具 体 某 天 的 数 据-----------
2 3
3 number
4 date
5 2013-11-06 3
• dataframe的truncate函数可以获取某个时期之前或之后的数据，或者某个时间区间的数据
• 但一般建议直接用切片（slice），这样更为直观，方便
1 # dataframe的truncate函 数 可 以 获 取 某 个 时 期 之 前 或 之 后 的 数 据， 或 者 某 个 时 间 区 间 的 数
据
2 # 但 一 般 建 议 直 接 用 切 片 （slice）， 这 样 更 为 直 观， 方 便
3 print('---------获 取 某 个 时 期 之 前 或 之 后 的 数 据-----------')
4 print('--------after------------')
5 print(df.truncate(after = '2013-11'))
6 print('--------before------------')
7 print(df.truncate(before='2017-02'))
1 ---------获 取 某 个 时 期 之 前 或 之 后 的 数 据-----------
2 --------after------------
3 number
4 date
5 2013-10-24 3
6 2013-10-25 4
7 2013-10-29 2
8 2013-10-30 1
9 --------before------------
10 number
11 date
12 2017-02-07 8
13 2017-02-14 6
14 2017-02-22 6
14.3 按日期显示数据
14.3.1 to_period()方法
• 请注意df.index的数据类型是DatetimeIndex；
• df_peirod的数据类型是PeriodIndex
按月显示，但不统计
1 df_period = df.to_period('M') #按 月 显 示， 但 不 统 计
2 print(type(df_period))
3
4 print(type(df_period.index))
http://liyangbit.com 113Chapter14. Pandas日期数据处理 Python数据之道
5 # 请 注 意df.index的 数 据 类 型 是DatetimeIndex；
6 # df_peirod的 数 据 类 型 是PeriodIndex
7
8 print(df_period.head())
1 <class 'pandas.core.frame.DataFrame'>
2 <class 'pandas.tseries.period.PeriodIndex'>
3 number
4 date
5 2013-10 3
6 2013-10 4
7 2013-10 2
8 2013-10 1
9 2013-11 1
按季度显示，但不统计
1 print(df.to_period('Q').head()) #按 季 度 显 示， 但 不 统 计
1 number
2 date
3 2013Q4 3
4 2013Q4 4
5 2013Q4 2
6 2013Q4 1
7 2013Q4 1
按年度显示，但不统计
1 print(df.to_period('A').head()) #按 年 度 显 示， 但 不 统 计
1 number
2 date
3 2013 3
4 2013 4
5 2013 2
6 2013 1
7 2013 1
14.3.2 asfreq()方法
按年度频率显示
1 df_period.index.asfreq('A') # 'A'默 认 是'A-DEC',其 他 如'A-JAN'
1 PeriodIndex(['2013', '2013', '2013', '2013', '2013', '2013', '2013', '2013',
2 '2013', '2013',
3 ...
4 '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2017',
http://liyangbit.com 114Chapter14. Pandas日期数据处理 Python数据之道
5 '2017', '2017'],
6 dtype='period[A-DEC]', name='date', length=425, freq='A-DEC')
1 df_period.index.asfreq('A-JAN') # 'A'默 认 是'A-DEC',其 他 如'A-JAN'
1 PeriodIndex(['2014', '2014', '2014', '2014', '2014', '2014', '2014', '2014',
2 '2014', '2014',
3 ...
4 '2017', '2017', '2017', '2017', '2017', '2017', '2017', '2018',
5 '2018', '2018'],
6 dtype='period[A-JAN]', name='date', length=425, freq='A-JAN')
• 按年度频率在不同情形下的显示，可参考下图所示：
按季度频率显示
1 df_period.index.asfreq('Q') # 'Q'默 认 是'Q-DEC',其 他 如 “Q-SEP”，“Q-FEB”
1 PeriodIndex(['2013Q4', '2013Q4', '2013Q4', '2013Q4', '2013Q4', '2013Q4',
2 '2013Q4', '2013Q4', '2013Q4', '2013Q4',
3 ...
4 '2017Q1', '2017Q1', '2017Q1', '2017Q1', '2017Q1', '2017Q1',
5 '2017Q1', '2017Q1', '2017Q1', '2017Q1'],
6 dtype='period[Q-DEC]', name='date', length=425, freq='Q-DEC')
1 df_period.index.asfreq('Q-SEP') # 可 以 显 示 不 同 的 季 度 财 年，“Q-SEP”，“Q-FEB”
2 # df_period.index = df_period.index.asfreq('Q-DEC') # 可 以 显 示 不 同 的 季 度 财
年，“Q-SEP”，“Q-FEB”
3 # print(df_period.head())
1 PeriodIndex(['2014Q1', '2014Q1', '2014Q1', '2014Q1', '2014Q1', '2014Q1',
2 '2014Q1', '2014Q1', '2014Q1', '2014Q1',
3 ...
4 '2017Q2', '2017Q2', '2017Q2', '2017Q2', '2017Q2', '2017Q2',
5 '2017Q2', '2017Q2', '2017Q2', '2017Q2'],
6 dtype='period[Q-SEP]', name='date', length=425, freq='Q-SEP')
• 按季度频率在不同情形下的显示，可参考下图所示：
http://liyangbit.com 115Chapter14. Pandas日期数据处理 Python数据之道
按月度频率显示
1 df_period.index.asfreq('M') # 按 月 份 显 示
1 PeriodIndex(['2013-10', '2013-10', '2013-10', '2013-10', '2013-11', '2013-11',
2 '2013-11', '2013-11', '2013-11', '2013-11',
3 ...
4 '2017-01', '2017-01', '2017-01', '2017-01', '2017-01', '2017-01',
5 '2017-01', '2017-02', '2017-02', '2017-02'],
6 dtype='period[M]', name='date', length=425, freq='M')
按工作日显示
• method1
1 df_period.index.asfreq('B', how='start') # 按 工 作 日 期 显 示
1 PeriodIndex(['2013-10-01', '2013-10-01', '2013-10-01', '2013-10-01',
2 '2013-11-01', '2013-11-01', '2013-11-01', '2013-11-01',
3 '2013-11-01', '2013-11-01',
4 ...
5 '2017-01-02', '2017-01-02', '2017-01-02', '2017-01-02',
6 '2017-01-02', '2017-01-02', '2017-01-02', '2017-02-01',
7 '2017-02-01', '2017-02-01'],
8 dtype='period[B]', name='date', length=425, freq='B')
• method2
1 df_period.index.asfreq('B', how='end') # 按 工 作 日 期 显 示
1 PeriodIndex(['2013-10-31', '2013-10-31', '2013-10-31', '2013-10-31',
2 '2013-11-29', '2013-11-29', '2013-11-29', '2013-11-29',
3 '2013-11-29', '2013-11-29',
4 ...
5 '2017-01-31', '2017-01-31', '2017-01-31', '2017-01-31',
6 '2017-01-31', '2017-01-31', '2017-01-31', '2017-02-28',
7 '2017-02-28', '2017-02-28'],
8 dtype='period[B]', name='date', length=425, freq='B')
http://liyangbit.com 116Chapter14. Pandas日期数据处理 Python数据之道
14.4 按日期统计数据
14.4.1 按日期统计数据
按周统计数据
1 print(df.resample('w').sum().head())
2 # “w”，week
1 number
2 date
3 2013-10-27 7.0
4 2013-11-03 3.0
5 2013-11-10 5.0
6 2013-11-17 7.0
7 2013-11-24 NaN
按月统计数据
1 print(df.resample('M').sum().head())
2 # ”MS”是 每 个 月 第 一 天 为 开 始 日 期, ”M”是 每 个 月 最 后 一 天
1 number
2 date
3 2013-10-31 10
4 2013-11-30 14
5 2013-12-31 27
6 2014-01-31 16
7 2014-02-28 4
按季度统计数据
1 print(df.resample('Q').sum().head())
2 # ”QS”是 每 个 季 度 第 一 天 为 开 始 日 期, ”Q”是 每 个 季 度 最 后 一 天
1 number
2 date
3 2013-12-31 51
4 2014-03-31 73
5 2014-06-30 96
6 2014-09-30 136
7 2014-12-31 148
按年统计数据
1
2 print(df.resample('AS').sum())
3 # ”AS”是 每 年 第 一 天 为 开 始 日 期, ”A是 每 年 最 后 一 天
1 number
2 date
http://liyangbit.com 117Chapter14. Pandas日期数据处理 Python数据之道
3 2013-01-01 51
4 2014-01-01 453
5 2015-01-01 743
6 2016-01-01 1552
7 2017-01-01 92
• 关于日期的类型，按参考下图所示来选择合适的分期频率：
14.4.2 按日期统计后，按年或季度或月份显示
按年统计并显示
1 print(df.resample('AS').sum().to_period('A'))
http://liyangbit.com 118Chapter14. Pandas日期数据处理 Python数据之道
2 # 按 年 统 计 并 显 示
1 number
2 date
3 2013 51
4 2014 453
5 2015 743
6 2016 1552
7 2017 92
按季度统计并显示
1 print(df.resample('Q').sum().to_period('Q').head())
2 # 按 季 度 统 计 并 显 示
1 number
2 date
3 2013Q4 51
4 2014Q1 73
5 2014Q2 96
6 2014Q3 136
7 2014Q4 148
按月度统计并显示
1 print(df.resample('M').sum().to_period('M').head())
2 # 按 月 度 统 计 并 显 示
1 number
2 date
3 2013-10 10
4 2013-11 14
5 2013-12 27
6 2014-01 16
7 2014-02 4
http://liyangbit.com 11915 Pandas 的 DataFrame 如何按指定 list 排序
写这篇文章的起由是有一天微信上一位朋友问到一个问题，问题大体意思概述如下：
现在有一个pandas的Series和一个python的list，想让Series按指定的list进行排序，如何实现？
这个问题的需求用流程图描述如下：
120Chapter15. Pandas的DataFrame如何按指定list排序 Python数据之道
Figure15.1:排序需求示意图
我思考了一下，这个问题解决的核心是引入pandas的数据类型“category”，从而进行排序。在具体的分析过程
中，先将pandas的Series转换成为DataFrame，然后设置数据类型，再进行排序。思路用流程图表示如下：
http://liyangbit.com 121Chapter15. Pandas的DataFrame如何按指定list排序 Python数据之道
Figure15.2:排序过程及结果
15.1 分析过程
• 引入pandas库
1 import pandas as pd
http://liyangbit.com 122Chapter15. Pandas的DataFrame如何按指定list排序 Python数据之道
• 构造Series数据
1 s = pd.Series({'a':1,'b':2,'c':3})
2 s
3
4 Out[]:
5 a 1
6 b 2
7 c 3
8 dtype: int64
1 s.index
2
3 Out[]:
4 Index(['a', 'b', 'c'], dtype='object')
• 指定的list，后续按指定list的元素顺序进行排序
1 list_custom = ['b', 'a', 'c']
2 list_custom
3
4 Out[]:
5 ['b', 'a', 'c']
• 将Series转换成DataFrame
1 df = pd.DataFrame(s)
2 df = df.reset_index()
3 df.columns = ['words', 'number']
4 df
设置成“category”数据类型
1 # 设 置 成 “category” 数 据 类 型
2 df['words'] = df['words'].astype('category')
1 # inplace = True， 使 recorder_categories生 效
2 df['words'].cat.reorder_categories(list_custom, inplace=True)
3
4 # inplace = True， 使 df生 效
http://liyangbit.com 123Chapter15. Pandas的DataFrame如何按指定list排序 Python数据之道
5 df.sort_values('words', inplace=True)
6 df
15.2 指定list元素多的情况：
若指定的list所包含元素比Dataframe中需要排序的列的元素多，怎么办？
• reorder_catgories（）方法不能继续使用，因为该方法使用时要求新的categories和dataframe中的
categories的元素个数和内容必须一致，只是顺序不同。
• 这种情况下，可以使用set_categories()方法来实现。新的list可以比dataframe中元素多。
1 list_custom_new = ['d', 'c', 'b','a','e']
2 dict_new = {'e':1, 'b':2, 'c':3}
3 df_new = pd.DataFrame(list(dict_new.items()), columns=['words', 'value'])
4 print(list_custom_new)
5 df_new.sort_values('words', inplace=True)
6 df_new
1 ['d', 'c', 'b', 'a', 'e']
1 df_new['words'] = df_new['words'].astype('category')
2
3 # inplace = True， 使 set_categories生 效
4 df_new['words'].cat.set_categories(list_custom_new, inplace=True)
5
6 df_new.sort_values('words', ascending=True)
http://liyangbit.com 124Chapter15. Pandas的DataFrame如何按指定list排序 Python数据之道
15.3 指定list元素少的情况：
若指定的list所包含元素比Dataframe中需要排序的列的元素少，怎么办？
• 这种情况下，set_categories()方法还是可以使用的，只是没有的元素会以NaN表示
注意下面的list中没有元素“b”
1 list_custom_new = ['d', 'c','a','e']
2 dict_new = {'e':1, 'b':2, 'c':3}
3 df_new = pd.DataFrame(list(dict_new.items()), columns=['words', 'value'])
4 print(list_custom_new)
5 df_new.sort_values('words', inplace=True)
6 df_new
1 ['d', 'c', 'a', 'e']
1 df_new['words'] = df_new['words'].astype('category')
2
3 # inplace = True， 使 set_categories生 效
4 df_new['words'].cat.set_categories(list_custom_new, inplace=True)
5
6 df_new.sort_values('words', ascending=True)
http://liyangbit.com 125Chapter15. Pandas的DataFrame如何按指定list排序 Python数据之道
15.4 总结
根据指定的list所包含元素比Dataframe中需要排序的列的元素的多或少，可以分为三种情况：
• 相等的情况下，可以使用reorder_categories和set_categories方法；
• list的元素比较多的情况下，可以使用set_categories方法；
• list的元素比较少的情况下，也可以使用set_categories方法，但list中没有的元素会在DataFrame中以
NaN表示。
源代码：在微信公众号“Python数据之道”（ID：PyDataLab）后台回复关键字“code”获取。
http://liyangbit.com 12616 Pandas 小册子：根据条件创建新的列
在进行数据分析时，经常会遇到根据已有的数据列，按照一定条件创建新的数据列，然后进行进一步分析。今天，
我们来看一个根据已有数据按照一定条件创建新的数据列的方法。数据如下：
1 import pandas as pd
2
3 df = pd.DataFrame({'team_A': ['Spain', 'Germany', 'Brazil', 'France'],
4 'team_B': ['USA', 'Argentina', 'Mexico', 'Belgium'],
5 'score_A': [5, 3, 2, 0],
6 'score_B': [4, 0, 3, 0]},
7 columns = ['team_A', 'team_B', 'score_A', 'score_B'])
8 df
9
10 Out[2]:
11 team_A team_B score_A score_B
12 0 Spain USA 5 4
13 1 Germany Argentina 3 0
14 2 Brazil Mexico 2 3
15 3 France Belgium 0 0
问题：从上面数据中创建新的一个数据列，用来存储获胜队伍的名称。即，根据“score_A”与“score_B”比较
的结果，来获取相应的结果。
例如，第一行，“Spain”:“USA”为5:4，“Spain”获胜，新创建的列中存储的结果为“Spain”。下面介绍两种方
法来实现上述要求。
16.1 第一种方法
第一种方法是利用Pandas中DataFrame的条件选择功能来实现，过程如下：
1 # 创 建 新 的 列 ”win_team”， 赋 值 为 空 白
2 df['win_team'] = ''
3
4 # 创 建 判 断 条 件 mask
5 mask = df['score_A'] - df['score_B']
6 df.loc[mask > 0, 'win_team'] = df.loc[mask > 0, 'team_A']
7 df.loc[mask < 0, 'win_team'] = df.loc[mask < 0, 'team_B']
8 df.loc[mask == 0, 'win_team'] = 'Draw'
9
10 df
127Chapter16. Pandas小册子：根据条件创建新的列 Python数据之道
11
12 Out[3]:
13 team_A team_B score_A score_B win_team
14 0 Spain USA 5 4 Spain
15 1 Germany Argentina 3 0 Germany
16 2 Brazil Mexico 2 3 Mexico
17 3 France Belgium 0 0 Draw
16.2 第二种方法
第二种方法是结合DataFrame.iterrows()以及Python的list的功能来实现，过程如下：
1 # The second method to get the winners
2
3 def find_win_team(df):
4 winners = []
5 for i, row in df.iterrows():
6 if row['score_A'] > row['score_B']:
7 winners.append(row['team_A'])
8 elif row['score_A'] < row['score_B']:
9 winners.append(row['team_B'])
10 else:
11 winners.append('Draw')
12 return winners
13
14 df['winner'] = find_win_team(df)
15 df
16
17 Out[4]:
18 team_A team_B score_A score_B win_team winner
19 0 Spain USA 5 4 Spain Spain
20 1 Germany Argentina 3 0 Germany Germany
21 2 Brazil Mexico 2 3 Mexico Mexico
22 3 France Belgium 0 0 Draw Draw
关于DataFrame.iterrows(),我们先来看看其运行结果。
1 for row_index, row in df.iterrows():
2 print('%s\n%s' % (row_index, row))
3
4 0
5 team_A Spain
6 team_B USA
7 score_A 5
8 score_B 4
9 win_team Spain
10 winner Spain
11 Name: 0, dtype: object
12 1
http://liyangbit.com 128Chapter16. Pandas小册子：根据条件创建新的列 Python数据之道
13 team_A Germany
14 team_B Argentina
15 score_A 3
16 score_B 0
17 win_team Germany
18 winner Germany
19 Name: 1, dtype: object
20 2
21 team_A Brazil
22 team_B Mexico
23 score_A 2
24 score_B 3
25 win_team Mexico
26 winner Mexico
27 Name: 2, dtype: object
28 3
29 team_A France
30 team_B Belgium
31 score_A 0
32 score_B 0
33 win_team Draw
34 winner Draw
35 Name: 3, dtype: object
DataFrame.iterrows()的作用是将dataframe的每行转换成为一个Series，可以理解为针对于每一行，做了
行列转置。图示如下：
Figure16.1:iterrows()的作用
http://liyangbit.com 12917 Matplotlib 饼图简介
Matplotlib，官方提供的饼图Demo，功能比较比较简单，在实际应用过程中，往往会有许多个性化的绘制需求，
在这里跟大家一起了解下饼图（piechart）的一些特色的功能的实现。首先引入相关Python库。
1 from matplotlib import font_manager as fm
2 import matplotlib as mpl
3 import pandas as pd
4 import numpy as np
5 import matplotlib.pyplot as plt
6 % matplotlib inline
7 plt.style.use('ggplot')
17.1 官方Demo
1 import matplotlib.pyplot as plt
2
3 # Pie chart, where the slices will be ordered and plotted counter-clockwise:
4 labels = 'Frogs', 'Hogs', 'Dogs', 'Logs'
5 sizes = [15, 30, 45, 10]
6 explode = (0, 0.1, 0, 0) # only ”explode” the 2nd slice (i.e. 'Hogs')
7
8 fig1, ax1 = plt.subplots()
9 ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
10 shadow=True, startangle=90)
11 ax1.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.
12
13 plt.savefig('Demo_official.jpg')
14 plt.show()
130Chapter17. Matplotlib饼图简介 Python数据之道
17.2 将实际数据应用于官方Demo
1 # 原 始 数 据
2 shapes = ['Cross', 'Cone', 'Egg', 'Teardrop', 'Chevron', 'Diamond', 'Cylinder',
3 'Rectangle', 'Flash', 'Cigar', 'Changing', 'Formation', 'Oval', 'Disk',
4 'Sphere', 'Fireball', 'Triangle', 'Circle', 'Light']
5 values = [ 287, 383, 842, 866, 1187, 1405, 1495, 1620, 1717,
6 2313, 2378, 3070, 4332, 5841, 6482, 7785, 9358, 9818, 20254]
7
8 s = pd.Series(values, index=shapes)
9 s
1 from matplotlib import font_manager as fm
2 import matplotlib as mpl
3
4 # Pie chart, where the slices will be ordered and plotted counter-clockwise:
5 labels = s.index
6 sizes = s.values
7 explode = (0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) # only ”explode” the 1st
slice
8
9 fig1, ax1 = plt.subplots()
10 patches, texts, autotexts = ax1.pie(sizes, explode=explode, labels=labels,
autopct='%1.0f%%',
11 shadow=False, startangle=170)
12 ax1.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.
13
http://liyangbit.com 131Chapter17. Matplotlib饼图简介 Python数据之道
14 plt.savefig('Demo_project.jpg')
15 plt.show()
上图的一些问题：
1. 颜色比较生硬
2. 部分文字拥挤在一起，绘图显示不齐整
17.3 一些改善措施
• 重新设置字体大小
• 设置自选颜色
• 设置图例
• 将某些类别突出显示
17.3.1 重新设置字体大小
1 from matplotlib import font_manager as fm
2 import matplotlib as mpl
3
4 labels = s.index
5 sizes = s.values
6 explode = (0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) # only ”explode” the 1st
slice
7
8 fig1, ax1 = plt.subplots()
http://liyangbit.com 132Chapter17. Matplotlib饼图简介 Python数据之道
9
10 patches, texts, autotexts = ax1.pie(sizes, explode=explode, labels=labels,
autopct='%1.0f%%',
11 shadow=False, startangle=170)
12 ax1.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.
13
14 # 重 新 设 置 字 体 大 小
15 proptease = fm.FontProperties()
16 proptease.set_size('xx-small')
17 # font size include: ‘xx-small’,x-small’,'small’,'medium’,‘large’,‘x-
large’,‘xx-large’ or number, e.g. '12'
18 plt.setp(autotexts, fontproperties=proptease)
19 plt.setp(texts, fontproperties=proptease)
20
21 plt.savefig('Demo_project_set_font.jpg')
22 plt.show()
17.3.2 设置显示颜色，Method1：
1 from matplotlib import font_manager as fm
2 import matplotlib as mpl
3
4 # Pie chart, where the slices will be ordered and plotted counter-clockwise:
5 labels = s.index
6 sizes = s.values
7 explode = (0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) # only ”explode” the 1st
slice
8
http://liyangbit.com 133Chapter17. Matplotlib饼图简介 Python数据之道
9 fig1, ax1 = plt.subplots(figsize=(6,6)) # 设 置 绘 图 区 域 大 小
10
11 a = np.random.rand(1,19)
12 color_vals = list(a[0])
13 my_norm = mpl.colors.Normalize(-1, 1) # 将 颜 色 数 据 的 范 围 设 置 为 [0, 1]
14 my_cmap = mpl.cm.get_cmap('rainbow', len(color_vals)) # 可 选 择 合 适 的colormap，
如：'rainbow'
15
16 patches, texts, autotexts = ax1.pie(sizes, explode=explode, labels=labels,
autopct='%1.0f%%',
17 shadow=False, startangle=170, colors=my_cmap(my_norm(color_vals)))
18
19 ax1.axis('equal')
20
21 # 重 新 设 置 字 体 大 小
22 proptease = fm.FontProperties()
23 proptease.set_size('xx-small')
24 # font size include: ‘xx-small’,x-small’,'small’,'medium’,‘large’,‘x-
large’,‘xx-large’ or number, e.g. '12'
25 plt.setp(autotexts, fontproperties=proptease)
26 plt.setp(texts, fontproperties=proptease)
27
28 plt.savefig('Demo_project_set_color_1.jpg')
29 plt.show()
http://liyangbit.com 134Chapter17. Matplotlib饼图简介 Python数据之道
上面这种方法设置颜色时，但类别比较多时，部分颜色的填充会重复。有时候，我们可能想设置成连续的颜色，可
以有另外一种方法来实现。
17.3.3 设置显示颜色，Method2：
1 from matplotlib import font_manager as fm
2 from matplotlib import cm
3
4 labels = s.index
5 sizes = s.values
6 # explode = (0.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) # only ”explode” the 1st
slice
7
8 fig, ax = plt.subplots(figsize=(6,6)) # 设 置 绘 图 区 域 大 小
9
10
11 colors = cm.rainbow(np.arange(len(sizes))/len(sizes)) # colormaps: Paired,
autumn, rainbow, gray,spring,Darks
http://liyangbit.com 135Chapter17. Matplotlib饼图简介 Python数据之道
12 patches, texts, autotexts = ax.pie(sizes, labels=labels, autopct='%1.0f%%',
13 shadow=False, startangle=170, colors=colors)
14
15 ax.axis('equal')
16 ax.set_title('Shapes -------------------', loc='left')
17
18 # 重 新 设 置 字 体 大 小
19 proptease = fm.FontProperties()
20 proptease.set_size('xx-small')
21 # font size include: ‘xx-small’,x-small’,'small’,'medium’,‘large’,‘x-
large’,‘xx-large’ or number, e.g. '12'
22 plt.setp(autotexts, fontproperties=proptease)
23 plt.setp(texts, fontproperties=proptease)
24
25 plt.savefig('Demo_project_set_color_2.jpg')
26 plt.show()
从上图可以看出，颜色显示是连续的，实现了我们想要的效果
http://liyangbit.com 136Chapter17. Matplotlib饼图简介 Python数据之道
17.3.4 设置图例（legend）
1 from matplotlib import font_manager as fm
2 from matplotlib import cm
3
4 labels = s.index
5 sizes = s.values
6 # explode = (0.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) # only ”explode” the 1st
slice
7
8 fig, ax = plt.subplots(figsize=(6,6)) # 设 置 绘 图 区 域 大 小
9
10
11 colors = cm.rainbow(np.arange(len(sizes))/len(sizes)) # colormaps: Paired,
autumn, rainbow, gray,spring,Darks
12 patches, texts, autotexts = ax.pie(sizes, labels=labels, autopct='%1.0f%%',
13 shadow=False, startangle=170, colors=colors)
14
15 ax.axis('equal')
16
17 # 重 新 设 置 字 体 大 小
18 proptease = fm.FontProperties()
19 proptease.set_size('xx-small')
20 # font size include: ‘xx-small’,x-small’,'small’,'medium’,‘large’,‘x-
large’,‘xx-large’ or number, e.g. '12'
21 plt.setp(autotexts, fontproperties=proptease)
22 plt.setp(texts, fontproperties=proptease)
23
24
25 ax.legend(labels, loc=2)
26
27 plt.savefig('Demo_project_set_legend_error.jpg')
28 plt.show()
http://liyangbit.com 137Chapter17. Matplotlib饼图简介 Python数据之道
从上面可看出，当类别较多时，图例（legend）的位置摆放显示有重叠，显示有些问题，需要进行调整。
17.3.5 重新设置图例（legend）
1 from matplotlib import font_manager as fm
2 from matplotlib import cm
3
4 labels = s.index
5 sizes = s.values
6 # explode = (0.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) # only ”explode” the 1st
slice
7
8 fig, axes = plt.subplots(figsize=(10,5),ncols=2) # 设 置 绘 图 区 域 大 小
9 ax1, ax2 = axes.ravel()
10
11 colors = cm.rainbow(np.arange(len(sizes))/len(sizes)) # colormaps: Paired,
autumn, rainbow, gray,spring,Darks
12 patches, texts, autotexts = ax1.pie(sizes, labels=labels, autopct='%1.0f%%',
13 shadow=False, startangle=170, colors=colors)
http://liyangbit.com 138Chapter17. Matplotlib饼图简介 Python数据之道
14
15 ax1.axis('equal')
16
17 # 重 新 设 置 字 体 大 小
18 proptease = fm.FontProperties()
19 proptease.set_size('xx-small')
20 # font size include: ‘xx-small’,x-small’,'small’,'medium’,‘large’,‘x-
large’,‘xx-large’ or number, e.g. '12'
21 plt.setp(autotexts, fontproperties=proptease)
22 plt.setp(texts, fontproperties=proptease)
23
24 ax1.set_title('Shapes', loc='center')
25
26 # ax2 只 显 示 图 例 （legend）
27 ax2.axis('off')
28 ax2.legend(patches, labels, loc='center left')
29
30 plt.tight_layout()
31 plt.savefig('Demo_project_set_legend_good.jpg')
32 plt.show()
17.3.6 将某些类别突出显示
• 将某些类别突出显示
• 控制label的显示位置
• 控制百分比的显示位置
• 控制突出位置的大小
1 from matplotlib import font_manager as fm
2 from matplotlib import cm
3
4 labels = s.index
5 sizes = s.values
http://liyangbit.com 139Chapter17. Matplotlib饼图简介 Python数据之道
6 explode = (0.1,0,0,0,0,0,0,0,0,0,0,0,0,0.2,0,0,0,0.1,0) # ”explode” ， show
the selected slice
7
8 fig, axes = plt.subplots(figsize=(8,5),ncols=2) # 设 置 绘 图 区 域 大 小
9 ax1, ax2 = axes.ravel()
10
11 colors = cm.rainbow(np.arange(len(sizes))/len(sizes)) # colormaps: Paired,
autumn, rainbow, gray,spring,Darks
12 patches, texts, autotexts = ax1.pie(sizes, labels=labels, autopct='%1.0f%%',
explode=explode,
13 shadow=False, startangle=170, colors=colors, labeldistance=1.2,
pctdistance=1.03, radius=0.4)
14 # labeldistance: 控 制labels显 示 的 位 置
15 # pctdistance: 控 制 百 分 比 显 示 的 位 置
16 # radius: 控 制 切 片 突 出 的 距 离
17
18 ax1.axis('equal')
19
20 # 重 新 设 置 字 体 大 小
21 proptease = fm.FontProperties()
22 proptease.set_size('xx-small')
23 # font size include: ‘xx-small’,x-small’,'small’,'medium’,‘large’,‘x-
large’,‘xx-large’ or number, e.g. '12'
24 plt.setp(autotexts, fontproperties=proptease)
25 plt.setp(texts, fontproperties=proptease)
26
27 ax1.set_title('Shapes', loc='center')
28
29 # ax2 只 显 示 图 例 （legend）
30 ax2.axis('off')
31 ax2.legend(patches, labels, loc='center left')
32
33 plt.tight_layout()
34 # plt.savefig(”pie_shape_ufo.png”, bbox_inches='tight')
35 plt.savefig('Demo_project_final.jpg')
36 plt.show()
http://liyangbit.com 140Chapter17. Matplotlib饼图简介 Python数据之道
本文涉及的代码量比较多，如需要查看源代码，请在微信公众号“Python数据之道”（ID：PyDataLab）后台回
复关键字“code”。
http://liyangbit.com 14118 Matplotlib 中等高线图（contour）的绘制
在此前的文章里，我们介绍了Numpy 中 meshgrid 函数的应用，并提到了等高线图的使用。下面，我们来介绍一
下matplotlib中等高线图的绘制。先上图来看一下等高线图的绘制效果，是不是很炫啊。
Figure18.1:等高线图的效果图
下面我们来分步骤介绍matplotlib中等高线图（contour）的绘制。
数据初始化
1 import matplotlib
2 import numpy as np
3 import matplotlib.cm as cm
4 import matplotlib.mlab as mlab
5 import matplotlib.pyplot as plt
6
7 %matplotlib inline
8
9 # 定 义 等 高 线 高 度 函 数
10 def f(x, y):
11 return (1 - x / 2 + x ** 5 + y ** 3) * np.exp(- x ** 2 - y ** 2)
12
142Chapter18. Matplotlib中等高线图（contour）的绘制 Python数据之道
13 # 数 据 数 目
14 n = 256
15 # 定 义x, y
16 x = np.linspace(-3, 3, n)
17 y = np.linspace(-3, 3, n)
18
19 # 生 成 网 格 数 据
20 X, Y = np.meshgrid(x, y)
注：数据初始化过程中用到了Numpy的meshgrid函数，有兴趣的同学可以了解下这个函数的使用，链接如下：
• Numpy中Meshgrid函数介绍及2种应用场景
18.1 仅绘制等高线
1 # 设 置 图 像 大 小 尺 寸
2 plt.figure(figsize=(10,6))
3
4 # 填 充 等 高 线 的 颜 色, 8是 等 高 线 分 为 几 部 分
5 plt.contour(X, Y,f(X, Y), 8,alpha = 0.75, cmap = plt.cm.hot)
结果如下：
18.2 仅填充等高区域颜色：
1 # 设 置 图 像 大 小 尺 寸
2 plt.figure(figsize=(10,6))
3
http://liyangbit.com 143Chapter18. Matplotlib中等高线图（contour）的绘制 Python数据之道
4 # 填 充 等 高 区 域 的 颜 色, 8是 等 高 线 分 为 几 部 分
5 plt.contourf(X, Y,f(X, Y), 8, alpha = 0.75, cmap = plt.cm.hot)
结果如下：
18.3 绘制完整的等高线图
1 # 设 置 图 像 大 小 尺 寸
2 plt.figure(figsize=(10,6))
3
4 # 填 充 等 高 线 的 颜 色, 8是 等 高 线 分 为 几 部 分
5 plt.contourf(X, Y, f(X, Y), 8, alpha = 0.75, cmap = plt.cm.hot)
6
7 C = plt.contour(X,Y, f(X,Y), 8, colors='black', linewidth=8)
8
9 # 绘 制 等 高 线 数 据
10 plt.clabel(C, inline = True, fontsize = 10)
11
12 # 去 除 坐 标 轴
13 plt.xticks(())
14 plt.yticks(())
15
16 plt.show()
结果如下：
http://liyangbit.com 144Chapter18. Matplotlib中等高线图（contour）的绘制 Python数据之道
是不是很容易实现啊，赶快动手来试试吧。
http://liyangbit.com 14519 Matplotlib 可视化最有价值的 50 个图表
原文标题：深度好文|Matplotlib可视化最有价值的50个图表（附完整Python源代码）
本文总结了Matplotlib以及Seaborn用的最多的50个图形，数量掌握这些图形的绘制，对于数据分析的可视
化有莫大的作用，强烈推荐大家阅读后续内容。
Tips：
（1）本文原文部分代码有不准确的地方，已进行修改；
（2）所有正确的源代码，我已整合到jupyternotebook文件中，可以在公众号『Python数据之道』后台回复
“code”，可获得本文源代码；
（3）运行本文代码，除了安装matplotlib和seaborn可视化库外，还需要安装其他的一些辅助可视化库，已在
代码部分作标注，具体内容请查看下面文章内容
在数据分析和可视化中最有用的50个Matplotlib图表。这些图表列表允许您使用python的matplotlib和
seaborn库选择要显示的可视化对象。
19.1 介绍
这些图表根据可视化目标的7个不同情景进行分组。例如，如果要想象两个变量之间的关系，请查看“相关”部分下
的图表。或者，如果您想要显示值如何随时间变化，请查看“更改”部分，依此类推。
有效图表的重要特征：
• 在不歪曲事实的情况下传达正确和必要的信息。
• 设计简单，您不必太费力就能理解它。
• 从审美角度支持信息而不是掩盖信息。
• 信息没有超负荷。
19.2 准备工作
在代码运行前先引入下面的设置内容。当然，单独的图表，可以重新设置显示要素。
1
2 # !pip install brewer2mpl
3 import numpy as np
4 import pandas as pd
146Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
5 import matplotlib as mpl
6 import matplotlib.pyplot as plt
7 import seaborn as sns
8 import warnings; warnings.filterwarnings(action='once')
9
10 large = 22; med = 16; small = 12
11 params = {'axes.titlesize': large,
12 'legend.fontsize': med,
13 'figure.figsize': (16, 10),
14 'axes.labelsize': med,
15 'axes.titlesize': med,
16 'xtick.labelsize': med,
17 'ytick.labelsize': med,
18 'figure.titlesize': large}
19 plt.rcParams.update(params)
20 plt.style.use('seaborn-whitegrid')
21 sns.set_style(”white”)
22 %matplotlib inline
23
24 # Version
25 print(mpl.__version__) #> 3.0.0
26 print(sns.__version__) #> 0.9.0
27
28 out:
29 3.0.2
30 0.9.0
19.3 关联（Correlation）
关联图表用于可视化2个或更多变量之间的关系。也就是说，一个变量如何相对于另一个变化。
19.3.1 散点图（Scatterplot）
散点图是用于研究两个变量之间关系的经典的和基本的图表。如果数据中有多个组，则可能需要以不同颜色可视化
每个组。在matplotlib中，您可以使用plt.scatterplot（）方便地执行此操作。
1 # Import dataset
2 midwest = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/
master/midwest_filter.csv”)
3
4 # Prepare Data
5 # Create as many colors as there are unique midwest['category']
6 categories = np.unique(midwest['category'])
7 colors = [plt.cm.tab10(i/float(len(categories)-1)) for i in range(len(
categories))]
8
9 # Draw Plot for Each Category
http://liyangbit.com 147Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
10 plt.figure(figsize=(16, 10), dpi= 80, facecolor='w', edgecolor='k')
11
12 for i, category in enumerate(categories):
13 plt.scatter('area', 'poptotal',
14 data=midwest.loc[midwest.category==category, :],
15 s=20, cmap=colors[i], label=str(category))
16 # ”c=” 修 改 为 ”cmap=”，Python数 据 之 道 备 注
17
18 # Decorations
19 plt.gca().set(xlim=(0.0, 0.1), ylim=(0, 90000),
20 xlabel='Area', ylabel='Population')
21
22 plt.xticks(fontsize=12); plt.yticks(fontsize=12)
23 plt.title(”Scatterplot of Midwest Area vs Population”, fontsize=22)
24 plt.legend(fontsize=12)
25 plt.show()
Figure19.1:散点图
19.3.2 带边界的气泡图（BubbleplotwithEncircling）
有时，您希望在边界内显示一组点以强调其重要性。在这个例子中，你从数据框中获取记录，并用下面代码中描述
的encircle（）来使边界显示出来。
1 from matplotlib import patches
2 from scipy.spatial import ConvexHull
3 import warnings; warnings.simplefilter('ignore')
4 sns.set_style(”white”)
5
http://liyangbit.com 148Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
6 # Step 1: Prepare Data
7 midwest = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/
master/midwest_filter.csv”)
8
9 # As many colors as there are unique midwest['category']
10 categories = np.unique(midwest['category'])
11 colors = [plt.cm.tab10(i/float(len(categories)-1)) for i in range(len(
categories))]
12
13 # Step 2: Draw Scatterplot with unique color for each category
14 fig = plt.figure(figsize=(16, 10), dpi= 80, facecolor='w', edgecolor='k')
15
16 for i, category in enumerate(categories):
17 plt.scatter('area', 'poptotal', data=midwest.loc[midwest.category==category
, :],
18 s='dot_size', cmap=colors[i], label=str(category), edgecolors='
black', linewidths=.5)
19 # ”c=” 修 改 为 ”cmap=”，Python数 据 之 道 备 注
20
21 # Step 3: Encircling
22 # https://stackoverflow.com/questions/44575681/how-do-i-encircle-different-data
-sets-in-scatter-plot
23 def encircle(x,y, ax=None, **kw):
24 if not ax: ax=plt.gca()
25 p = np.c_[x,y]
26 hull = ConvexHull(p)
27 poly = plt.Polygon(p[hull.vertices,:], **kw)
28 ax.add_patch(poly)
29
30 # Select data to be encircled
31 midwest_encircle_data = midwest.loc[midwest.state=='IN', :]
32
33 # Draw polygon surrounding vertices
34 encircle(midwest_encircle_data.area, midwest_encircle_data.poptotal, ec=”k”, fc
=”gold”, alpha=0.1)
35 encircle(midwest_encircle_data.area, midwest_encircle_data.poptotal, ec=”
firebrick”, fc=”none”, linewidth=1.5)
36
37 # Step 4: Decorations
38 plt.gca().set(xlim=(0.0, 0.1), ylim=(0, 90000),
39 xlabel='Area', ylabel='Population')
40
41 plt.xticks(fontsize=12); plt.yticks(fontsize=12)
42 plt.title(”Bubble Plot with Encircling”, fontsize=22)
43 plt.legend(fontsize=12)
44 plt.show()
http://liyangbit.com 149Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.2:气泡图
19.3.3 带线性回归最佳拟合线的散点图（Scatterplotwithlinearregressionlineofbestfit）
如果你想了解两个变量如何相互改变，那么最佳拟合线就是常用的方法。下图显示了数据中各组之间最佳拟合线的
差异。要禁用分组并仅为整个数据集绘制一条最佳拟合线，请从下面的sns.lmplot（）调用中删除hue ='cyl'参数。
1 # Import Data
2 df = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/master/
mpg_ggplot2.csv”)
3 df_select = df.loc[df.cyl.isin([4,8]), :]
4
5 # Plot
6 sns.set_style(”white”)
7 gridobj = sns.lmplot(x=”displ”, y=”hwy”, hue=”cyl”, data=df_select,
8 height=7, aspect=1.6, robust=True, palette='tab10',
9 scatter_kws=dict(s=60, linewidths=.7, edgecolors='black'))
10
11 # Decorations
12 gridobj.set(xlim=(0.5, 7.5), ylim=(0, 50))
13 plt.title(”Scatterplot with line of best fit grouped by number of cylinders”,
fontsize=20)
14 plt.show()
http://liyangbit.com 150Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.3:带线性回归最佳拟合线的散点图
针对每列绘制线性回归线
或者，可以在其每列中显示每个组的最佳拟合线。可以通过在sns.lmplot()中设置col=groupingcolumn
参数来实现，如下：
1 # Import Data
2 df = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/master/
mpg_ggplot2.csv”)
3 df_select = df.loc[df.cyl.isin([4,8]), :]
4
5 # Each line in its own column
6 sns.set_style(”white”)
7 gridobj = sns.lmplot(x=”displ”, y=”hwy”,
8 data=df_select,
9 height=7,
10 robust=True,
11 palette='Set1',
12 col=”cyl”,
13 scatter_kws=dict(s=60, linewidths=.7, edgecolors='black'))
14
15 # Decorations
16 gridobj.set(xlim=(0.5, 7.5), ylim=(0, 50))
17 plt.show()
http://liyangbit.com 151Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.4:带线性回归最佳拟合线的散点图
19.3.4 抖动图（Jitteringwithstripplot）
通常，多个数据点具有完全相同的X和Y值。结果，多个点绘制会重叠并隐藏。为避免这种情况，请将数据点稍微
抖动，以便您可以直观地看到它们。使用seaborn的stripplot（）很方便实现这个功能。
1 # Import Data
2 df = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/master/
mpg_ggplot2.csv”)
3
4 # Draw Stripplot
5 fig, ax = plt.subplots(figsize=(16,10), dpi= 80)
6 sns.stripplot(df.cty, df.hwy, jitter=0.25, size=8, ax=ax, linewidth=.5)
7
8 # Decorations
9 plt.title('Use jittered plots to avoid overlapping of points', fontsize=22)
10 plt.show()
http://liyangbit.com 152Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.5:抖动图
19.3.5 计数图（CountsPlot）
避免点重叠问题的另一个选择是增加点的大小，这取决于该点中有多少点。因此，点的大小越大，其周围的点的集
中度越高。
1 # Import Data
2 df = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/master/
mpg_ggplot2.csv”)
3 df_counts = df.groupby(['hwy', 'cty']).size().reset_index(name='counts')
4
5 # Draw Stripplot
6 fig, ax = plt.subplots(figsize=(16,10), dpi= 80)
7 sns.stripplot(df_counts.cty, df_counts.hwy, size=df_counts.counts*2, ax=ax)
8
9 # Decorations
10 plt.title('Counts Plot - Size of circle is bigger as more points overlap',
fontsize=22)
11 plt.show()
http://liyangbit.com 153Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.6:计数图
19.3.6 边缘直方图（MarginalHistogram）
边缘直方图具有沿X和Y轴变量的直方图。这用于可视化X和Y之间的关系以及单独的X和Y的单变量分布。这
种图经常用于探索性数据分析（EDA）。
1 # Import Data
2 df = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/master/
mpg_ggplot2.csv”)
3
4 # Create Fig and gridspec
5 fig = plt.figure(figsize=(16, 10), dpi= 80)
6 grid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)
7
8 # Define the axes
9 ax_main = fig.add_subplot(grid[:-1, :-1])
10 ax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])
11 ax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])
12
13 # Scatterplot on main ax
14 ax_main.scatter('displ', 'hwy', s=df.cty*4, c=df.manufacturer.astype('category'
).cat.codes, alpha=.9, data=df, cmap=”tab10”, edgecolors='gray', linewidths
=.5)
15
16 # histogram on the right
17 ax_bottom.hist(df.displ, 40, histtype='stepfilled', orientation='vertical',
color='deeppink')
18 ax_bottom.invert_yaxis()
http://liyangbit.com 154Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
19
20 # histogram in the bottom
21 ax_right.hist(df.hwy, 40, histtype='stepfilled', orientation='horizontal',
color='deeppink')
22
23 # Decorations
24 ax_main.set(title='Scatterplot with Histograms \n displ vs hwy', xlabel='displ'
, ylabel='hwy')
25 ax_main.title.set_fontsize(20)
26 for item in ([ax_main.xaxis.label, ax_main.yaxis.label] + ax_main.
get_xticklabels() + ax_main.get_yticklabels()):
27 item.set_fontsize(14)
28
29 xlabels = ax_main.get_xticks().tolist()
30 ax_main.set_xticklabels(xlabels)
31 plt.show()
Figure19.7:边缘直方图
19.3.7 边缘箱形图（MarginalBoxplot）
边缘箱形图与边缘直方图具有相似的用途。然而，箱线图有助于精确定位X和Y的中位数、第25和第75百分位
数。
1 # Import Data
2 df = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/master/
mpg_ggplot2.csv”)
3
4 # Create Fig and gridspec
http://liyangbit.com 155Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
5 fig = plt.figure(figsize=(16, 10), dpi= 80)
6 grid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)
7
8 # Define the axes
9 ax_main = fig.add_subplot(grid[:-1, :-1])
10 ax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])
11 ax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])
12
13 # Scatterplot on main ax
14 ax_main.scatter('displ', 'hwy', s=df.cty*5, c=df.manufacturer.astype('category'
).cat.codes, alpha=.9, data=df, cmap=”Set1”, edgecolors='black', linewidths
=.5)
15
16 # Add a graph in each part
17 sns.boxplot(df.hwy, ax=ax_right, orient=”v”)
18 sns.boxplot(df.displ, ax=ax_bottom, orient=”h”)
19
20 # Decorations ------------------
21 # Remove x axis name for the boxplot
22 ax_bottom.set(xlabel='')
23 ax_right.set(ylabel='')
24
25 # Main Title, Xlabel and YLabel
26 ax_main.set(title='Scatterplot with Histograms \n displ vs hwy', xlabel='displ'
, ylabel='hwy')
27
28 # Set font size of different components
29 ax_main.title.set_fontsize(20)
30 for item in ([ax_main.xaxis.label, ax_main.yaxis.label] + ax_main.
get_xticklabels() + ax_main.get_yticklabels()):
31 item.set_fontsize(14)
32
33 plt.show()
http://liyangbit.com 156Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.8:边缘箱形图
19.3.8 相关图（Correllogram）
相关图用于直观地查看给定数据框（或二维数组）中所有可能的数值变量对之间的相关度量。
1 # Import Dataset
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mtcars.csv”)
3
4 # Plot
5 plt.figure(figsize=(12,10), dpi= 80)
6 sns.heatmap(df.corr(), xticklabels=df.corr().columns, yticklabels=df.corr().
columns, cmap='RdYlGn', center=0, annot=True)
7
8 # Decorations
9 plt.title('Correlogram of mtcars', fontsize=22)
10 plt.xticks(fontsize=12)
11 plt.yticks(fontsize=12)
12 plt.show()
http://liyangbit.com 157Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.9:相关图
19.3.9 矩阵图（PairwisePlot）
矩阵图是探索性分析中的最爱，用于理解所有可能的数值变量对之间的关系。它是双变量分析的必备工具。
1 # Load Dataset
2 df = sns.load_dataset('iris')
3
4 # Plot
5 plt.figure(figsize=(10,8), dpi= 80)
6 sns.pairplot(df, kind=”scatter”, hue=”species”, plot_kws=dict(s=80, edgecolor=”
white”, linewidth=2.5))
7 plt.show()
http://liyangbit.com 158Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.10:矩阵图
1 # Load Dataset
2 df = sns.load_dataset('iris')
3
4 # Plot
5 plt.figure(figsize=(10,8), dpi= 80)
6 sns.pairplot(df, kind=”reg”, hue=”species”)
7 plt.show()
http://liyangbit.com 159Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.11:矩阵图
19.4 偏差（Deviation）
19.4.1 发散型条形图（DivergingBars）
如果您想根据单个指标查看项目的变化情况，并可视化此差异的顺序和数量，那么散型条形图（DivergingBars）
是一个很好的工具。它有助于快速区分数据中组的性能，并且非常直观，并且可以立即传达这一点。
1 # Prepare Data
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mtcars.csv”)
3 x = df.loc[:, ['mpg']]
4 df['mpg_z'] = (x - x.mean())/x.std()
5 df['colors'] = ['red' if x < 0 else 'green' for x in df['mpg_z']]
6 df.sort_values('mpg_z', inplace=True)
7 df.reset_index(inplace=True)
8
9 # Draw plot
10 plt.figure(figsize=(14,10), dpi= 80)
11 plt.hlines(y=df.index, xmin=0, xmax=df.mpg_z, color=df.colors, alpha=0.4,
linewidth=5)
12
http://liyangbit.com 160Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
13 # Decorations
14 plt.gca().set(ylabel='$Model$', xlabel='$Mileage$')
15 plt.yticks(df.index, df.cars, fontsize=12)
16 plt.title('Diverging Bars of Car Mileage', fontdict={'size':20})
17 plt.grid(linestyle='--', alpha=0.5)
18 plt.show()
Figure19.12:发散型条形图
19.4.2 发散型文本（DivergingTexts）
发散型文本（DivergingTexts）与发散型条形图（DivergingBars）相似，如果你想以一种漂亮和可呈现的方式
显示图表中每个项目的价值，就可以使用这种方法。
1 # Prepare Data
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mtcars.csv”)
3 x = df.loc[:, ['mpg']]
4 df['mpg_z'] = (x - x.mean())/x.std()
5 df['colors'] = ['red' if x < 0 else 'green' for x in df['mpg_z']]
6 df.sort_values('mpg_z', inplace=True)
7 df.reset_index(inplace=True)
8
9 # Draw plot
10 plt.figure(figsize=(14,14), dpi= 80)
11 plt.hlines(y=df.index, xmin=0, xmax=df.mpg_z)
12 for x, y, tex in zip(df.mpg_z, df.index, df.mpg_z):
13 t = plt.text(x, y, round(tex, 2), horizontalalignment='right' if x < 0 else
'left',
http://liyangbit.com 161Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
14 verticalalignment='center', fontdict={'color':'red' if x < 0
else 'green', 'size':14})
15
16 # Decorations
17 plt.yticks(df.index, df.cars, fontsize=12)
18 plt.title('Diverging Text Bars of Car Mileage', fontdict={'size':20})
19 plt.grid(linestyle='--', alpha=0.5)
20 plt.xlim(-2.5, 2.5)
21 plt.show()
Figure19.13:发散型文本
19.4.3 发散型包点图（DivergingDotPlot）
发散型包点图（DivergingDotPlot）也类似于发散型条形图（DivergingBars）。然而，与发散型条形图
（DivergingBars）相比，条的缺失减少了组之间的对比度和差异。
1 # Prepare Data
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mtcars.csv”)
3 x = df.loc[:, ['mpg']]
4 df['mpg_z'] = (x - x.mean())/x.std()
http://liyangbit.com 162Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
5 df['colors'] = ['red' if x < 0 else 'darkgreen' for x in df['mpg_z']]
6 df.sort_values('mpg_z', inplace=True)
7 df.reset_index(inplace=True)
8
9 # Draw plot
10 plt.figure(figsize=(14,16), dpi= 80)
11 plt.scatter(df.mpg_z, df.index, s=450, alpha=.6, color=df.colors)
12 for x, y, tex in zip(df.mpg_z, df.index, df.mpg_z):
13 t = plt.text(x, y, round(tex, 1), horizontalalignment='center',
14 verticalalignment='center', fontdict={'color':'white'})
15
16 # Decorations
17 # Lighten borders
18 plt.gca().spines[”top”].set_alpha(.3)
19 plt.gca().spines[”bottom”].set_alpha(.3)
20 plt.gca().spines[”right”].set_alpha(.3)
21 plt.gca().spines[”left”].set_alpha(.3)
22
23 plt.yticks(df.index, df.cars)
24 plt.title('Diverging Dotplot of Car Mileage', fontdict={'size':20})
25 plt.xlabel('$Mileage$')
26 plt.grid(linestyle='--', alpha=0.5)
27 plt.xlim(-2.5, 2.5)
28 plt.show()
http://liyangbit.com 163Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.14:发散型包点图
19.4.4 带标记的发散型棒棒糖图（DivergingLollipopChartwithMarkers）
带标记的棒棒糖图通过强调您想要引起注意的任何重要数据点并在图表中适当地给出推理，提供了一种对差异进行
可视化的灵活方式。
1 # Prepare Data
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mtcars.csv”)
3 x = df.loc[:, ['mpg']]
4 df['mpg_z'] = (x - x.mean())/x.std()
5 df['colors'] = 'black'
6
7 # color fiat differently
8 df.loc[df.cars == 'Fiat X1-9', 'colors'] = 'darkorange'
9 df.sort_values('mpg_z', inplace=True)
10 df.reset_index(inplace=True)
11
http://liyangbit.com 164Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
12
13 # Draw plot
14 import matplotlib.patches as patches
15
16 plt.figure(figsize=(14,16), dpi= 80)
17 plt.hlines(y=df.index, xmin=0, xmax=df.mpg_z, color=df.colors, alpha=0.4,
linewidth=1)
18 plt.scatter(df.mpg_z, df.index, color=df.colors, s=[600 if x == 'Fiat X1-9'
else 300 for x in df.cars], alpha=0.6)
19 plt.yticks(df.index, df.cars)
20 plt.xticks(fontsize=12)
21
22 # Annotate
23 plt.annotate('Mercedes Models', xy=(0.0, 11.0), xytext=(1.0, 11), xycoords='
data',
24 fontsize=15, ha='center', va='center',
25 bbox=dict(boxstyle='square', fc='firebrick'),
26 arrowprops=dict(arrowstyle='-[, widthB=2.0, lengthB=1.5', lw=2.0,
color='steelblue'), color='white')
27
28 # Add Patches
29 p1 = patches.Rectangle((-2.0, -1), width=.3, height=3, alpha=.2, facecolor='red
')
30 p2 = patches.Rectangle((1.5, 27), width=.8, height=5, alpha=.2, facecolor='
green')
31 plt.gca().add_patch(p1)
32 plt.gca().add_patch(p2)
33
34 # Decorate
35 plt.title('Diverging Bars of Car Mileage', fontdict={'size':20})
36 plt.grid(linestyle='--', alpha=0.5)
37 plt.show()
http://liyangbit.com 165Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.15:带标记的发散型棒棒糖图
19.4.5 面积图（AreaChart）
通过对轴和线之间的区域进行着色，面积图不仅强调峰和谷，而且还强调高点和低点的持续时间。高点持续时间越
长，线下面积越大。
1 import numpy as np
2 import pandas as pd
3
4 # Prepare Data
5 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/economics.csv”
, parse_dates=['date']).head(100)
6 x = np.arange(df.shape[0])
7 y_returns = (df.psavert.diff().fillna(0)/df.psavert.shift(1)).fillna(0) * 100
8
9 # Plot
10 plt.figure(figsize=(16,10), dpi= 80)
http://liyangbit.com 166Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
11 plt.fill_between(x[1:], y_returns[1:], 0, where=y_returns[1:] >= 0, facecolor='
green', interpolate=True, alpha=0.7)
12 plt.fill_between(x[1:], y_returns[1:], 0, where=y_returns[1:] <= 0, facecolor='
red', interpolate=True, alpha=0.7)
13
14 # Annotate
15 plt.annotate('Peak \n1975', xy=(94.0, 21.0), xytext=(88.0, 28),
16 bbox=dict(boxstyle='square', fc='firebrick'),
17 arrowprops=dict(facecolor='steelblue', shrink=0.05), fontsize=15,
color='white')
18
19
20 # Decorations
21 xtickvals = [str(m)[:3].upper()+”-”+str(y) for y,m in zip(df.date.dt.year, df.
date.dt.month_name())]
22 plt.gca().set_xticks(x[::6])
23 plt.gca().set_xticklabels(xtickvals[::6], rotation=90, fontdict={'
horizontalalignment': 'center', 'verticalalignment': 'center_baseline'})
24 plt.ylim(-35,35)
25 plt.xlim(1,100)
26 plt.title(”Month Economics Return %”, fontsize=22)
27 plt.ylabel('Monthly returns %')
28 plt.grid(alpha=0.5)
29 plt.show()
Figure19.16:面积图
http://liyangbit.com 167Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
19.5 排序（Ranking）
19.5.1 有序条形图（OrderedBarChart）
有序条形图有效地传达了项目的排名顺序。但是，在图表上方添加度量标准的值，用户可以从图表本身获取精确信
息。
1 # Prepare Data
2 df_raw = pd.read_csv(”https://github.com/selva86/datasets/raw/master/
mpg_ggplot2.csv”)
3 df = df_raw[['cty', 'manufacturer']].groupby('manufacturer').apply(lambda x: x.
mean())
4 df.sort_values('cty', inplace=True)
5 df.reset_index(inplace=True)
6
7 # Draw plot
8 import matplotlib.patches as patches
9
10 fig, ax = plt.subplots(figsize=(16,10), facecolor='white', dpi= 80)
11 ax.vlines(x=df.index, ymin=0, ymax=df.cty, color='firebrick', alpha=0.7,
linewidth=20)
12
13 # Annotate Text
14 for i, cty in enumerate(df.cty):
15 ax.text(i, cty+0.5, round(cty, 1), horizontalalignment='center')
16
17
18 # Title, Label, Ticks and Ylim
19 ax.set_title('Bar Chart for Highway Mileage', fontdict={'size':22})
20 ax.set(ylabel='Miles Per Gallon', ylim=(0, 30))
21 plt.xticks(df.index, df.manufacturer.str.upper(), rotation=60,
horizontalalignment='right', fontsize=12)
22
23 # Add patches to color the X axis labels
24 p1 = patches.Rectangle((.57, -0.005), width=.33, height=.13, alpha=.1,
facecolor='green', transform=fig.transFigure)
25 p2 = patches.Rectangle((.124, -0.005), width=.446, height=.13, alpha=.1,
facecolor='red', transform=fig.transFigure)
26 fig.add_artist(p1)
27 fig.add_artist(p2)
28 plt.show()
http://liyangbit.com 168Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.17:有序条形图
19.5.2 棒棒糖图（LollipopChart）
棒棒糖图表以一种视觉上令人愉悦的方式提供与有序条形图类似的目的。
1 # Prepare Data
2 df_raw = pd.read_csv(”https://github.com/selva86/datasets/raw/master/
mpg_ggplot2.csv”)
3 df = df_raw[['cty', 'manufacturer']].groupby('manufacturer').apply(lambda x: x.
mean())
4 df.sort_values('cty', inplace=True)
5 df.reset_index(inplace=True)
6
7 # Draw plot
8 fig, ax = plt.subplots(figsize=(16,10), dpi= 80)
9 ax.vlines(x=df.index, ymin=0, ymax=df.cty, color='firebrick', alpha=0.7,
linewidth=2)
10 ax.scatter(x=df.index, y=df.cty, s=75, color='firebrick', alpha=0.7)
11
12 # Title, Label, Ticks and Ylim
13 ax.set_title('Lollipop Chart for Highway Mileage', fontdict={'size':22})
14 ax.set_ylabel('Miles Per Gallon')
15 ax.set_xticks(df.index)
16 ax.set_xticklabels(df.manufacturer.str.upper(), rotation=60, fontdict={'
horizontalalignment': 'right', 'size':12})
17 ax.set_ylim(0, 30)
http://liyangbit.com 169Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
18
19 # Annotate
20 for row in df.itertuples():
21 ax.text(row.Index, row.cty+.5, s=round(row.cty, 2), horizontalalignment= '
center', verticalalignment='bottom', fontsize=14)
22
23 plt.show()
Figure19.18:棒棒糖图
19.5.3 包点图（DotPlot）
包点图表传达了项目的排名顺序，并且由于它沿水平轴对齐，因此您可以更容易地看到点彼此之间的距离。
1 # Prepare Data
2 df_raw = pd.read_csv(”https://github.com/selva86/datasets/raw/master/
mpg_ggplot2.csv”)
3 df = df_raw[['cty', 'manufacturer']].groupby('manufacturer').apply(lambda x: x.
mean())
4 df.sort_values('cty', inplace=True)
5 df.reset_index(inplace=True)
6
7 # Draw plot
8 fig, ax = plt.subplots(figsize=(16,10), dpi= 80)
9 ax.hlines(y=df.index, xmin=11, xmax=26, color='gray', alpha=0.7, linewidth=1,
linestyles='dashdot')
10 ax.scatter(y=df.index, x=df.cty, s=75, color='firebrick', alpha=0.7)
http://liyangbit.com 170Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
11
12 # Title, Label, Ticks and Ylim
13 ax.set_title('Dot Plot for Highway Mileage', fontdict={'size':22})
14 ax.set_xlabel('Miles Per Gallon')
15 ax.set_yticks(df.index)
16 ax.set_yticklabels(df.manufacturer.str.title(), fontdict={'horizontalalignment'
: 'right'})
17 ax.set_xlim(10, 27)
18 plt.show()
Figure19.19:包点图
19.5.4 坡度图（SlopeChart）
坡度图最适合比较给定人/项目的“前”和“后”位置。
1 import matplotlib.lines as mlines
2 # Import Data
3 df = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/master/
gdppercap.csv”)
4
5 left_label = [str(c) + ', '+ str(round(y)) for c, y in zip(df.continent, df['
1952'])]
6 right_label = [str(c) + ', '+ str(round(y)) for c, y in zip(df.continent, df['
1957'])]
7 klass = ['red' if (y1-y2) < 0 else 'green' for y1, y2 in zip(df['1952'], df['
1957'])]
8
9 # draw line
http://liyangbit.com 171Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
10 # https://stackoverflow.com/questions/36470343/how-to-draw-a-line-with-
matplotlib/36479941
11 def newline(p1, p2, color='black'):
12 ax = plt.gca()
13 l = mlines.Line2D([p1[0],p2[0]], [p1[1],p2[1]], color='red' if p1[1]-p2[1]
> 0 else 'green', marker='o', markersize=6)
14 ax.add_line(l)
15 return l
16
17 fig, ax = plt.subplots(1,1,figsize=(14,14), dpi= 80)
18
19 # Vertical Lines
20 ax.vlines(x=1, ymin=500, ymax=13000, color='black', alpha=0.7, linewidth=1,
linestyles='dotted')
21 ax.vlines(x=3, ymin=500, ymax=13000, color='black', alpha=0.7, linewidth=1,
linestyles='dotted')
22
23 # Points
24 ax.scatter(y=df['1952'], x=np.repeat(1, df.shape[0]), s=10, color='black',
alpha=0.7)
25 ax.scatter(y=df['1957'], x=np.repeat(3, df.shape[0]), s=10, color='black',
alpha=0.7)
26
27 # Line Segmentsand Annotation
28 for p1, p2, c in zip(df['1952'], df['1957'], df['continent']):
29 newline([1,p1], [3,p2])
30 ax.text(1-0.05, p1, c + ', ' + str(round(p1)), horizontalalignment='right',
verticalalignment='center', fontdict={'size':14})
31 ax.text(3+0.05, p2, c + ', ' + str(round(p2)), horizontalalignment='left',
verticalalignment='center', fontdict={'size':14})
32
33 # 'Before' and 'After' Annotations
34 ax.text(1-0.05, 13000, 'BEFORE', horizontalalignment='right', verticalalignment
='center', fontdict={'size':18, 'weight':700})
35 ax.text(3+0.05, 13000, 'AFTER', horizontalalignment='left', verticalalignment='
center', fontdict={'size':18, 'weight':700})
36
37 # Decoration
38 ax.set_title(”Slopechart: Comparing GDP Per Capita between 1952 vs 1957”,
fontdict={'size':22})
39 ax.set(xlim=(0,4), ylim=(0,14000), ylabel='Mean GDP Per Capita')
40 ax.set_xticks([1,3])
41 ax.set_xticklabels([”1952”, ”1957”])
42 plt.yticks(np.arange(500, 13000, 2000), fontsize=12)
43
44 # Lighten borders
45 plt.gca().spines[”top”].set_alpha(.0)
46 plt.gca().spines[”bottom”].set_alpha(.0)
47 plt.gca().spines[”right”].set_alpha(.0)
48 plt.gca().spines[”left”].set_alpha(.0)
49 plt.show()
http://liyangbit.com 172Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.20:坡度图
19.5.5 哑铃图（DumbbellPlot）
哑铃图表传达了各种项目的“前”和“后”位置以及项目的等级排序。如果您想要将特定项目/计划对不同对象的影响
可视化，那么它非常有用。
1 import matplotlib.lines as mlines
2
3 # Import Data
4 df = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/master/
health.csv”)
5 df.sort_values('pct_2014', inplace=True)
6 df.reset_index(inplace=True)
7
8 # Func to draw line segment
9 def newline(p1, p2, color='black'):
http://liyangbit.com 173Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
10 ax = plt.gca()
11 l = mlines.Line2D([p1[0],p2[0]], [p1[1],p2[1]], color='skyblue')
12 ax.add_line(l)
13 return l
14
15 # Figure and Axes
16 fig, ax = plt.subplots(1,1,figsize=(14,14), facecolor='#f7f7f7', dpi= 80)
17
18 # Vertical Lines
19 ax.vlines(x=.05, ymin=0, ymax=26, color='black', alpha=1, linewidth=1,
linestyles='dotted')
20 ax.vlines(x=.10, ymin=0, ymax=26, color='black', alpha=1, linewidth=1,
linestyles='dotted')
21 ax.vlines(x=.15, ymin=0, ymax=26, color='black', alpha=1, linewidth=1,
linestyles='dotted')
22 ax.vlines(x=.20, ymin=0, ymax=26, color='black', alpha=1, linewidth=1,
linestyles='dotted')
23
24 # Points
25 ax.scatter(y=df['index'], x=df['pct_2013'], s=50, color='#0e668b', alpha=0.7)
26 ax.scatter(y=df['index'], x=df['pct_2014'], s=50, color='#a3c4dc', alpha=0.7)
27
28 # Line Segments
29 for i, p1, p2 in zip(df['index'], df['pct_2013'], df['pct_2014']):
30 newline([p1, i], [p2, i])
31
32 # Decoration
33 ax.set_facecolor('#f7f7f7')
34 ax.set_title(”Dumbell Chart: Pct Change - 2013 vs 2014”, fontdict={'size':22})
35 ax.set(xlim=(0,.25), ylim=(-1, 27), ylabel='Mean GDP Per Capita')
36 ax.set_xticks([.05, .1, .15, .20])
37 ax.set_xticklabels(['5%', '15%', '20%', '25%'])
38 ax.set_xticklabels(['5%', '15%', '20%', '25%'])
39 plt.show()
http://liyangbit.com 174Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.21:哑铃图
19.6 分布（Distribution）
19.6.1 连续变量的直方图（HistogramforContinuousVariable）
直方图显示给定变量的频率分布。下面的图表示基于类型变量对频率条进行分组，从而更好地了解连续变量和类型
变量。
1 # Import Data
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mpg_ggplot2.
csv”)
3
4 # Prepare data
5 x_var = 'displ'
6 groupby_var = 'class'
7 df_agg = df.loc[:, [x_var, groupby_var]].groupby(groupby_var)
8 vals = [df[x_var].values.tolist() for i, df in df_agg]
http://liyangbit.com 175Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
9
10 # Draw
11 plt.figure(figsize=(16,9), dpi= 80)
12 colors = [plt.cm.Spectral(i/float(len(vals)-1)) for i in range(len(vals))]
13 n, bins, patches = plt.hist(vals, 30, stacked=True, density=False, color=colors
[:len(vals)])
14
15 # Decoration
16 plt.legend({group:col for group, col in zip(np.unique(df[groupby_var]).tolist()
, colors[:len(vals)])})
17 plt.title(f”Stacked Histogram of ${x_var}$ colored by ${groupby_var}$”,
fontsize=22)
18 plt.xlabel(x_var)
19 plt.ylabel(”Frequency”)
20 plt.ylim(0, 25)
21 plt.xticks(ticks=bins[::3], labels=[round(b,1) for b in bins[::3]])
22 plt.show()
Figure19.22:连续变量的直方图
19.6.2 类型变量的直方图（HistogramforCategoricalVariable）
类型变量的直方图显示该变量的频率分布。通过对条形图进行着色，可以将分布与表示颜色的另一个类型变量相关
联。
1 # Import Data
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mpg_ggplot2.
csv”)
3
4 # Prepare data
http://liyangbit.com 176Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
5 x_var = 'manufacturer'
6 groupby_var = 'class'
7 df_agg = df.loc[:, [x_var, groupby_var]].groupby(groupby_var)
8 vals = [df[x_var].values.tolist() for i, df in df_agg]
9
10 # Draw
11 plt.figure(figsize=(16,9), dpi= 80)
12 colors = [plt.cm.Spectral(i/float(len(vals)-1)) for i in range(len(vals))]
13 n, bins, patches = plt.hist(vals, df[x_var].unique().__len__(), stacked=True,
density=False, color=colors[:len(vals)])
14
15 # Decoration
16 plt.legend({group:col for group, col in zip(np.unique(df[groupby_var]).tolist()
, colors[:len(vals)])})
17 plt.title(f”Stacked Histogram of ${x_var}$ colored by ${groupby_var}$”,
fontsize=22)
18 plt.xlabel(x_var)
19 plt.ylabel(”Frequency”)
20 plt.ylim(0, 40)
21 plt.xticks(ticks=bins, labels=np.unique(df[x_var]).tolist(), rotation=90,
horizontalalignment='left')
22 plt.show()
Figure19.23:类型变量的直方图
http://liyangbit.com 177Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
19.6.3 密度图（DensityPlot）
密度图是一种常用工具，用于可视化连续变量的分布。通过“响应”变量对它们进行分组，您可以检查X和Y之间的
关系。以下情况用于表示目的，以描述城市里程的分布如何随着汽缸数的变化而变化。
1 # Import Data
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mpg_ggplot2.
csv”)
3
4 # Draw Plot
5 plt.figure(figsize=(16,10), dpi= 80)
6 sns.kdeplot(df.loc[df['cyl'] == 4, ”cty”], shade=True, color=”g”, label=”Cyl=4”
, alpha=.7)
7 sns.kdeplot(df.loc[df['cyl'] == 5, ”cty”], shade=True, color=”deeppink”, label=
”Cyl=5”, alpha=.7)
8 sns.kdeplot(df.loc[df['cyl'] == 6, ”cty”], shade=True, color=”dodgerblue”,
label=”Cyl=6”, alpha=.7)
9 sns.kdeplot(df.loc[df['cyl'] == 8, ”cty”], shade=True, color=”orange”, label=”
Cyl=8”, alpha=.7)
10
11 # Decoration
12 plt.title('Density Plot of City Mileage by n_Cylinders', fontsize=22)
13 plt.legend()
14 plt.show()
Figure19.24:密度图
http://liyangbit.com 178Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
19.6.4 直方密度线图（DensityCurveswithHistogram）
带有直方图的密度曲线汇集了两个图所传达的集体信息，因此您可以将它们放在一个图中而不是两个图中。
1 # Import Data
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mpg_ggplot2.
csv”)
3
4 # Draw Plot
5 plt.figure(figsize=(13,10), dpi= 80)
6 sns.distplot(df.loc[df['class'] == 'compact', ”cty”], color=”dodgerblue”, label
=”Compact”, hist_kws={'alpha':.7}, kde_kws={'linewidth':3})
7 sns.distplot(df.loc[df['class'] == 'suv', ”cty”], color=”orange”, label=”SUV”,
hist_kws={'alpha':.7}, kde_kws={'linewidth':3})
8 sns.distplot(df.loc[df['class'] == 'minivan', ”cty”], color=”g”, label=”minivan
”, hist_kws={'alpha':.7}, kde_kws={'linewidth':3})
9 plt.ylim(0, 0.35)
10
11 # Decoration
12 plt.title('Density Plot of City Mileage by Vehicle Type', fontsize=22)
13 plt.legend()
14 plt.show()
Figure19.25:直方密度线图
http://liyangbit.com 179Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
19.6.5 JoyPlot
JoyPlot允许不同组的密度曲线重叠，这是一种可视化大量分组数据的彼此关系分布的好方法。它看起来很悦目，
并清楚地传达了正确的信息。它可以使用基于matplotlib的joypy包轻松构建。（『Python数据之道』注：需要安装
joypy库）
1 # !pip install joypy
2 # Python数 据 之 道 备 注
3 import joypy
4
5 # Import Data
6 mpg = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mpg_ggplot2.
csv”)
7
8 # Draw Plot
9 plt.figure(figsize=(16,10), dpi= 80)
10 fig, axes = joypy.joyplot(mpg, column=['hwy', 'cty'], by=”class”, ylim='own',
figsize=(14,10))
11
12 # Decoration
13 plt.title('Joy Plot of City and Highway Mileage by Class', fontsize=22)
14 plt.show()
Figure19.26:JoyPlot
http://liyangbit.com 180Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
19.6.6 分布式包点图（DistributedDotPlot）
分布式包点图显示按组分割的点的单变量分布。点数越暗，该区域的数据点集中度越高。通过对中位数进行不同着
色，组的真实定位立即变得明显。
1 import matplotlib.patches as mpatches
2
3 # Prepare Data
4 df_raw = pd.read_csv(”https://github.com/selva86/datasets/raw/master/
mpg_ggplot2.csv”)
5 cyl_colors = {4:'tab:red', 5:'tab:green', 6:'tab:blue', 8:'tab:orange'}
6 df_raw['cyl_color'] = df_raw.cyl.map(cyl_colors)
7
8 # Mean and Median city mileage by make
9 df = df_raw[['cty', 'manufacturer']].groupby('manufacturer').apply(lambda x: x.
mean())
10 df.sort_values('cty', ascending=False, inplace=True)
11 df.reset_index(inplace=True)
12 df_median = df_raw[['cty', 'manufacturer']].groupby('manufacturer').apply(
lambda x: x.median())
13
14 # Draw horizontal lines
15 fig, ax = plt.subplots(figsize=(16,10), dpi= 80)
16 ax.hlines(y=df.index, xmin=0, xmax=40, color='gray', alpha=0.5, linewidth=.5,
linestyles='dashdot')
17
18 # Draw the Dots
19 for i, make in enumerate(df.manufacturer):
20 df_make = df_raw.loc[df_raw.manufacturer==make, :]
21 ax.scatter(y=np.repeat(i, df_make.shape[0]), x='cty', data=df_make, s=75,
edgecolors='gray', c='w', alpha=0.5)
22 ax.scatter(y=i, x='cty', data=df_median.loc[df_median.index==make, :], s
=75, c='firebrick')
23
24 # Annotate
25 ax.text(33, 13, ”$red \; dots \; are \; the \: median$”, fontdict={'size':12},
color='firebrick')
26
27 # Decorations
28 red_patch = plt.plot([],[], marker=”o”, ms=10, ls=””, mec=None, color='
firebrick', label=”Median”)
29 plt.legend(handles=red_patch)
30 ax.set_title('Distribution of City Mileage by Make', fontdict={'size':22})
31 ax.set_xlabel('Miles Per Gallon (City)', alpha=0.7)
32 ax.set_yticks(df.index)
33 ax.set_yticklabels(df.manufacturer.str.title(), fontdict={'horizontalalignment'
: 'right'}, alpha=0.7)
34 ax.set_xlim(1, 40)
35 plt.xticks(alpha=0.7)
36 plt.gca().spines[”top”].set_visible(False)
37 plt.gca().spines[”bottom”].set_visible(False)
http://liyangbit.com 181Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
38 plt.gca().spines[”right”].set_visible(False)
39 plt.gca().spines[”left”].set_visible(False)
40 plt.grid(axis='both', alpha=.4, linewidth=.1)
41 plt.show()
Figure19.27:分布式包点图
19.6.7 箱形图（BoxPlot）
箱形图是一种可视化分布的好方法，记住中位数、第25个第45个四分位数和异常值。但是，您需要注意解释可能
会扭曲该组中包含的点数的框的大小。因此，手动提供每个框中的观察数量可以帮助克服这个缺点。
例如，左边的前两个框具有相同大小的框，即使它们的值分别是5和47。因此，写入该组中的观察数量是必要的。
1 # Import Data
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mpg_ggplot2.
csv”)
3
4 # Draw Plot
5 plt.figure(figsize=(13,10), dpi= 80)
6 sns.boxplot(x='class', y='hwy', data=df, notch=False)
7
8 # Add N Obs inside boxplot (optional)
9 def add_n_obs(df,group_col,y):
10 medians_dict = {grp[0]:grp[1][y].median() for grp in df.groupby(group_col)}
11 xticklabels = [x.get_text() for x in plt.gca().get_xticklabels()]
12 n_obs = df.groupby(group_col)[y].size().values
13 for (x, xticklabel), n_ob in zip(enumerate(xticklabels), n_obs):
14 plt.text(x, medians_dict[xticklabel]*1.01, ”#obs : ”+str(n_ob),
horizontalalignment='center', fontdict={'size':14}, color='white')
http://liyangbit.com 182Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
15
16 add_n_obs(df,group_col='class',y='hwy')
17
18 # Decoration
19 plt.title('Box Plot of Highway Mileage by Vehicle Class', fontsize=22)
20 plt.ylim(10, 40)
21 plt.show()
Figure19.28:箱形图
19.6.8 包点+箱形图（Dot+BoxPlot）
包点+箱形图（Dot+BoxPlot）传达类似于分组的箱形图信息。此外，这些点可以了解每组中有多少数据点。
1 # Import Data
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mpg_ggplot2.
csv”)
3
4 # Draw Plot
5 plt.figure(figsize=(13,10), dpi= 80)
6 sns.boxplot(x='class', y='hwy', data=df, hue='cyl')
7 sns.stripplot(x='class', y='hwy', data=df, color='black', size=3, jitter=1)
8
9 for i in range(len(df['class'].unique())-1):
http://liyangbit.com 183Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
10 plt.vlines(i+.5, 10, 45, linestyles='solid', colors='gray', alpha=0.2)
11
12 # Decoration
13 plt.title('Box Plot of Highway Mileage by Vehicle Class', fontsize=22)
14 plt.legend(title='Cylinders')
15 plt.show()
Figure19.29:包点+箱形图
19.6.9 小提琴图（ViolinPlot）
小提琴图是箱形图在视觉上令人愉悦的替代品。小提琴的形状或面积取决于它所持有的观察次数。但是，小提琴图
可能更难以阅读，并且在专业设置中不常用。
1 # Import Data
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mpg_ggplot2.
csv”)
3
4 # Draw Plot
5 plt.figure(figsize=(13,10), dpi= 80)
6 sns.violinplot(x='class', y='hwy', data=df, scale='width', inner='quartile')
7
8 # Decoration
9 plt.title('Violin Plot of Highway Mileage by Vehicle Class', fontsize=22)
http://liyangbit.com 184Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
10 plt.show()
Figure19.30:小提琴图
19.6.10 人口金字塔（PopulationPyramid）
人口金字塔可用于显示由数量排序的组的分布。或者它也可以用于显示人口的逐级过滤，因为它在下面用于显示有
多少人通过营销渠道的每个阶段。
1 # Read data
2 df = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/master/
email_campaign_funnel.csv”)
3
4 # Draw Plot
5 plt.figure(figsize=(13,10), dpi= 80)
6 group_col = 'Gender'
7 order_of_bars = df.Stage.unique()[::-1]
8 colors = [plt.cm.Spectral(i/float(len(df[group_col].unique())-1)) for i in
range(len(df[group_col].unique()))]
9
10 for c, group in zip(colors, df[group_col].unique()):
11 sns.barplot(x='Users', y='Stage', data=df.loc[df[group_col]==group, :],
order=order_of_bars, color=c, label=group)
12
http://liyangbit.com 185Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
13 # Decorations
14 plt.xlabel(”$Users$”)
15 plt.ylabel(”Stage of Purchase”)
16 plt.yticks(fontsize=12)
17 plt.title(”Population Pyramid of the Marketing Funnel”, fontsize=22)
18 plt.legend()
19 plt.show()
Figure19.31:人口金字塔
19.6.11 分类图（CategoricalPlots）
由seaborn库提供的分类图可用于可视化彼此相关的2个或更多分类变量的计数分布。
1 # Load Dataset
2 titanic = sns.load_dataset(”titanic”)
3
4 # Plot
5 g = sns.catplot(”alive”, col=”deck”, col_wrap=4,
6 data=titanic[titanic.deck.notnull()],
7 kind=”count”, height=3.5, aspect=.8,
8 palette='tab20')
9
10 fig.suptitle('sf')
11 plt.show()
http://liyangbit.com 186Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.32:分类图
1 # Load Dataset
2 titanic = sns.load_dataset(”titanic”)
3
4 # Plot
5 sns.catplot(x=”age”, y=”embark_town”,
6 hue=”sex”, col=”class”,
7 data=titanic[titanic.embark_town.notnull()],
8 orient=”h”, height=5, aspect=1, palette=”tab10”,
9 kind=”violin”, dodge=True, cut=0, bw=.2)
Figure19.33:分类图
http://liyangbit.com 187Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
19.7 组成（Composition）
19.7.1 华夫饼图（WaffleChart）
可以使用pywaffle包创建华夫饼图，并用于显示更大群体中的组的组成。（『Python数据之道』注：需要安装
pywaffle库）
1 #! pip install pywaffle
2 # Reference: https://stackoverflow.com/questions/41400136/how-to-do-waffle-
charts-in-python-square-piechart
3 from pywaffle import Waffle
4
5 # Import
6 df_raw = pd.read_csv(”https://github.com/selva86/datasets/raw/master/
mpg_ggplot2.csv”)
7
8 # Prepare Data
9 df = df_raw.groupby('class').size().reset_index(name='counts')
10 n_categories = df.shape[0]
11 colors = [plt.cm.inferno_r(i/float(n_categories)) for i in range(n_categories)]
12
13 # Draw Plot and Decorate
14 fig = plt.figure(
15 FigureClass=Waffle,
16 plots={
17 '111': {
18 'values': df['counts'],
19 'labels': [”{0} ({1})”.format(n[0], n[1]) for n in df[['class', '
counts']].itertuples()],
20 'legend': {'loc': 'upper left', 'bbox_to_anchor': (1.05, 1), '
fontsize': 12},
21 'title': {'label': '# Vehicles by Class', 'loc': 'center', '
fontsize':18}
22 },
23 },
24 rows=7,
25 colors=colors,
26 figsize=(16, 9)
27 )
Figure19.34:华夫饼图
http://liyangbit.com 188Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
1 #! pip install pywaffle
2 from pywaffle import Waffle
3
4 # Import
5 # df_raw = pd.read_csv(”https://github.com/selva86/datasets/raw/master/
mpg_ggplot2.csv”)
6
7 # Prepare Data
8 # By Class Data
9 df_class = df_raw.groupby('class').size().reset_index(name='counts_class')
10 n_categories = df_class.shape[0]
11 colors_class = [plt.cm.Set3(i/float(n_categories)) for i in range(n_categories)
]
12
13 # By Cylinders Data
14 df_cyl = df_raw.groupby('cyl').size().reset_index(name='counts_cyl')
15 n_categories = df_cyl.shape[0]
16 colors_cyl = [plt.cm.Spectral(i/float(n_categories)) for i in range(
n_categories)]
17
18 # By Make Data
19 df_make = df_raw.groupby('manufacturer').size().reset_index(name='counts_make')
20 n_categories = df_make.shape[0]
21 colors_make = [plt.cm.tab20b(i/float(n_categories)) for i in range(n_categories
)]
22
23
24 # Draw Plot and Decorate
25 fig = plt.figure(
26 FigureClass=Waffle,
27 plots={
28 '311': {
29 'values': df_class['counts_class'],
30 'labels': [”{1}”.format(n[0], n[1]) for n in df_class[['class', '
counts_class']].itertuples()],
31 'legend': {'loc': 'upper left', 'bbox_to_anchor': (1.05, 1), '
fontsize': 12, 'title':'Class'},
32 'title': {'label': '# Vehicles by Class', 'loc': 'center', '
fontsize':18},
33 'colors': colors_class
34 },
35 '312': {
36 'values': df_cyl['counts_cyl'],
37 'labels': [”{1}”.format(n[0], n[1]) for n in df_cyl[['cyl', '
counts_cyl']].itertuples()],
38 'legend': {'loc': 'upper left', 'bbox_to_anchor': (1.05, 1), '
fontsize': 12, 'title':'Cyl'},
39 'title': {'label': '# Vehicles by Cyl', 'loc': 'center', 'fontsize'
:18},
40 'colors': colors_cyl
http://liyangbit.com 189Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
41 },
42 '313': {
43 'values': df_make['counts_make'],
44 'labels': [”{1}”.format(n[0], n[1]) for n in df_make[['manufacturer
', 'counts_make']].itertuples()],
45 'legend': {'loc': 'upper left', 'bbox_to_anchor': (1.05, 1), '
fontsize': 12, 'title':'Manufacturer'},
46 'title': {'label': '# Vehicles by Make', 'loc': 'center', 'fontsize
':18},
47 'colors': colors_make
48 }
49 },
50 rows=9,
51 figsize=(16, 14)
52 )
Figure19.35:华夫饼图
http://liyangbit.com 190Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
19.7.2 饼图（PieChart）
饼图是显示组成的经典方式。然而，现在通常不建议使用它，因为馅饼部分的面积有时会变得误导。因此，如果您
要使用饼图，强烈建议明确记下饼图每个部分的百分比或数字。
1 # Import
2 df_raw = pd.read_csv(”https://github.com/selva86/datasets/raw/master/
mpg_ggplot2.csv”)
3
4 # Prepare Data
5 df = df_raw.groupby('class').size()
6
7 # Make the plot with pandas
8 df.plot(kind='pie', subplots=True, figsize=(8, 8))
9 plt.title(”Pie Chart of Vehicle Class - Bad”)
10 plt.ylabel(””)
11 plt.show()
Figure19.36:饼图
http://liyangbit.com 191Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
1 # Import
2 df_raw = pd.read_csv(”https://github.com/selva86/datasets/raw/master/
mpg_ggplot2.csv”)
3
4 # Prepare Data
5 df = df_raw.groupby('class').size().reset_index(name='counts')
6
7 # Draw Plot
8 fig, ax = plt.subplots(figsize=(12, 7), subplot_kw=dict(aspect=”equal”), dpi=
80)
9
10 data = df['counts']
11 categories = df['class']
12 explode = [0,0,0,0,0,0.1,0]
13
14 def func(pct, allvals):
15 absolute = int(pct/100.*np.sum(allvals))
16 return ”{:.1f}% ({:d} )”.format(pct, absolute)
17
18 wedges, texts, autotexts = ax.pie(data,
19 autopct=lambda pct: func(pct, data),
20 textprops=dict(color=”w”),
21 colors=plt.cm.Dark2.colors,
22 startangle=140,
23 explode=explode)
24
25 # Decoration
26 ax.legend(wedges, categories, title=”Vehicle Class”, loc=”center left”,
bbox_to_anchor=(1, 0, 0.5, 1))
27 plt.setp(autotexts, size=10, weight=700)
28 ax.set_title(”Class of Vehicles: Pie Chart”)
29 plt.show()
http://liyangbit.com 192Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.37:饼图
19.7.3 树形图（Treemap）
树形图类似于饼图，它可以更好地完成工作而不会误导每个组的贡献。（『Python数据之道』注：需要安装
squarify库）
1 # pip install squarify
2 import squarify
3
4 # Import Data
5 df_raw = pd.read_csv(”https://github.com/selva86/datasets/raw/master/
mpg_ggplot2.csv”)
6
7 # Prepare Data
8 df = df_raw.groupby('class').size().reset_index(name='counts')
9 labels = df.apply(lambda x: str(x[0]) + ”\n (” + str(x[1]) + ”)”, axis=1)
10 sizes = df['counts'].values.tolist()
11 colors = [plt.cm.Spectral(i/float(len(labels))) for i in range(len(labels))]
12
13 # Draw Plot
14 plt.figure(figsize=(12,8), dpi= 80)
15 squarify.plot(sizes=sizes, label=labels, color=colors, alpha=.8)
16
17 # Decorate
http://liyangbit.com 193Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
18 plt.title('Treemap of Vechile Class')
19 plt.axis('off')
20 plt.show()
Figure19.38:树形图
19.7.4 条形图（BarChart）
条形图是基于计数或任何给定指标可视化项目的经典方式。在下面的图表中，我为每个项目使用了不同的颜色，但
您通常可能希望为所有项目选择一种颜色，除非您按组对其进行着色。颜色名称存储在下面代码中的all_colors中。您
可以通过在plt.plot（）中设置颜色参数来更改条的颜色。
1 import random
2
3 # Import Data
4 df_raw = pd.read_csv(”https://github.com/selva86/datasets/raw/master/
mpg_ggplot2.csv”)
5
6 # Prepare Data
7 df = df_raw.groupby('manufacturer').size().reset_index(name='counts')
8 n = df['manufacturer'].unique().__len__()+1
9 all_colors = list(plt.cm.colors.cnames.keys())
10 random.seed(100)
11 c = random.choices(all_colors, k=n)
12
13 # Plot Bars
14 plt.figure(figsize=(16,10), dpi= 80)
http://liyangbit.com 194Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
15 plt.bar(df['manufacturer'], df['counts'], color=c, width=.5)
16 for i, val in enumerate(df['counts'].values):
17 plt.text(i, val, float(val), horizontalalignment='center',
verticalalignment='bottom', fontdict={'fontweight':500, 'size':12})
18
19 # Decoration
20 plt.gca().set_xticklabels(df['manufacturer'], rotation=60, horizontalalignment=
'right')
21 plt.title(”Number of Vehicles by Manaufacturers”, fontsize=22)
22 plt.ylabel('# Vehicles')
23 plt.ylim(0, 45)
24 plt.show()
Figure19.39:条形图
19.8 变化（Change）
19.8.1 时间序列图（TimeSeriesPlot）
时间序列图用于显示给定度量随时间变化的方式。在这里，您可以看到1949年至1969年间航空客运量的变化情
况。
1 # Import Data
2 df = pd.read_csv('https://github.com/selva86/datasets/raw/master/AirPassengers.
csv')
3
http://liyangbit.com 195Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
4 # Draw Plot
5 plt.figure(figsize=(16,10), dpi= 80)
6 plt.plot('date', 'traffic', data=df, color='tab:red')
7
8 # Decoration
9 plt.ylim(50, 750)
10 xtick_location = df.index.tolist()[::12]
11 xtick_labels = [x[-4:] for x in df.date.tolist()[::12]]
12 plt.xticks(ticks=xtick_location, labels=xtick_labels, rotation=0, fontsize=12,
horizontalalignment='center', alpha=.7)
13 plt.yticks(fontsize=12, alpha=.7)
14 plt.title(”Air Passengers Traffic (1949 - 1969)”, fontsize=22)
15 plt.grid(axis='both', alpha=.3)
16
17 # Remove borders
18 plt.gca().spines[”top”].set_alpha(0.0)
19 plt.gca().spines[”bottom”].set_alpha(0.3)
20 plt.gca().spines[”right”].set_alpha(0.0)
21 plt.gca().spines[”left”].set_alpha(0.3)
22 plt.show()
Figure19.40:时间序列图
19.8.2 带波峰波谷标记的时序图（TimeSerieswithPeaksandTroughsAnnotated）
下面的时间序列绘制了所有峰值和低谷，并注释了所选特殊事件的发生。
1 # Import Data
http://liyangbit.com 196Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
2 df = pd.read_csv('https://github.com/selva86/datasets/raw/master/AirPassengers.
csv')
3
4 # Get the Peaks and Troughs
5 data = df['traffic'].values
6 doublediff = np.diff(np.sign(np.diff(data)))
7 peak_locations = np.where(doublediff == -2)[0] + 1
8
9 doublediff2 = np.diff(np.sign(np.diff(-1*data)))
10 trough_locations = np.where(doublediff2 == -2)[0] + 1
11
12 # Draw Plot
13 plt.figure(figsize=(16,10), dpi= 80)
14 plt.plot('date', 'traffic', data=df, color='tab:blue', label='Air Traffic')
15 plt.scatter(df.date[peak_locations], df.traffic[peak_locations], marker=mpl.
markers.CARETUPBASE, color='tab:green', s=100, label='Peaks')
16 plt.scatter(df.date[trough_locations], df.traffic[trough_locations], marker=mpl
.markers.CARETDOWNBASE, color='tab:red', s=100, label='Troughs')
17
18 # Annotate
19 for t, p in zip(trough_locations[1::5], peak_locations[::3]):
20 plt.text(df.date[p], df.traffic[p]+15, df.date[p], horizontalalignment='
center', color='darkgreen')
21 plt.text(df.date[t], df.traffic[t]-35, df.date[t], horizontalalignment='
center', color='darkred')
22
23 # Decoration
24 plt.ylim(50,750)
25 xtick_location = df.index.tolist()[::6]
26 xtick_labels = df.date.tolist()[::6]
27 plt.xticks(ticks=xtick_location, labels=xtick_labels, rotation=90, fontsize=12,
alpha=.7)
28 plt.title(”Peak and Troughs of Air Passengers Traffic (1949 - 1969)”, fontsize
=22)
29 plt.yticks(fontsize=12, alpha=.7)
30
31 # Lighten borders
32 plt.gca().spines[”top”].set_alpha(.0)
33 plt.gca().spines[”bottom”].set_alpha(.3)
34 plt.gca().spines[”right”].set_alpha(.0)
35 plt.gca().spines[”left”].set_alpha(.3)
36
37 plt.legend(loc='upper left')
38 plt.grid(axis='y', alpha=.3)
39 plt.show()
http://liyangbit.com 197Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.41:带波峰波谷标记的时序图
19.8.3 自相关和部分自相关图（Autocorrelation(ACF)andPartialAutocorrelation(PACF)
Plot）
自相关图（ACF图）显示时间序列与其自身滞后的相关性。每条垂直线（在自相关图上）表示系列与滞后0之间的
滞后之间的相关性。图中的蓝色阴影区域是显着性水平。那些位于蓝线之上的滞后是显着的滞后。
那么如何解读呢？对于空乘旅客，我们看到多达14个滞后跨越蓝线，因此非常重要。这意味着，14年前的航空旅
客交通量对今天的交通状况有影响。PACF在另一方面显示了任何给定滞后（时间序列）与当前序列的自相关，但是删
除了滞后的贡献。
1 from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
2
3 # Import Data
4 df = pd.read_csv('https://github.com/selva86/datasets/raw/master/AirPassengers.
csv')
5
6 # Draw Plot
7 fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6), dpi= 80)
8 plot_acf(df.traffic.tolist(), ax=ax1, lags=50)
9 plot_pacf(df.traffic.tolist(), ax=ax2, lags=20)
10
11 # Decorate
12 # lighten the borders
13 ax1.spines[”top”].set_alpha(.3); ax2.spines[”top”].set_alpha(.3)
14 ax1.spines[”bottom”].set_alpha(.3); ax2.spines[”bottom”].set_alpha(.3)
http://liyangbit.com 198Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
15 ax1.spines[”right”].set_alpha(.3); ax2.spines[”right”].set_alpha(.3)
16 ax1.spines[”left”].set_alpha(.3); ax2.spines[”left”].set_alpha(.3)
17
18 # font size of tick labels
19 ax1.tick_params(axis='both', labelsize=12)
20 ax2.tick_params(axis='both', labelsize=12)
21 plt.show()
Figure19.42:自相关和部分自相关图
19.8.4 交叉相关图（CrossCorrelationplot）
交叉相关图显示了两个时间序列相互之间的滞后。
1 import statsmodels.tsa.stattools as stattools
2
3 # Import Data
4 df = pd.read_csv('https://github.com/selva86/datasets/raw/master/mortality.csv'
)
5 x = df['mdeaths']
6 y = df['fdeaths']
7
8 # Compute Cross Correlations
9 ccs = stattools.ccf(x, y)[:100]
10 nlags = len(ccs)
11
12 # Compute the Significance level
13 # ref: https://stats.stackexchange.com/questions/3115/cross-correlation-
significance-in-r/3128#3128
14 conf_level = 2 / np.sqrt(nlags)
15
16 # Draw Plot
17 plt.figure(figsize=(12,7), dpi= 80)
18
19 plt.hlines(0, xmin=0, xmax=100, color='gray') # 0 axis
20 plt.hlines(conf_level, xmin=0, xmax=100, color='gray')
http://liyangbit.com 199Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
21 plt.hlines(-conf_level, xmin=0, xmax=100, color='gray')
22
23 plt.bar(x=np.arange(len(ccs)), height=ccs, width=.3)
24
25 # Decoration
26 plt.title('$Cross\; Correlation\; Plot:\; mdeaths\; vs\; fdeaths$', fontsize
=22)
27 plt.xlim(0,len(ccs))
28 plt.show()
Figure19.43:交叉相关图
19.8.5 时间序列分解图（TimeSeriesDecompositionPlot）
时间序列分解图显示时间序列分解为趋势，季节和残差分量。
1 from statsmodels.tsa.seasonal import seasonal_decompose
2 from dateutil.parser import parse
3
4 # Import Data
5 df = pd.read_csv('https://github.com/selva86/datasets/raw/master/AirPassengers.
csv')
6 dates = pd.DatetimeIndex([parse(d).strftime('%Y-%m-01') for d in df['date']])
7 df.set_index(dates, inplace=True)
8
9 # Decompose
10 result = seasonal_decompose(df['traffic'], model='multiplicative')
11
12 # Plot
13 plt.rcParams.update({'figure.figsize': (10,10)})
http://liyangbit.com 200Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
14 result.plot().suptitle('Time Series Decomposition of Air Passengers')
15 plt.show()
Figure19.44:时间序列分解图
19.8.6 多个时间序列（MultipleTimeSeries）
您可以绘制多个时间序列，在同一图表上测量相同的值，如下所示。
1 # Import Data
2 df = pd.read_csv('https://github.com/selva86/datasets/raw/master/mortality.csv'
)
3
4 # Define the upper limit, lower limit, interval of Y axis and colors
5 y_LL = 100
6 y_UL = int(df.iloc[:, 1:].max().max()*1.1)
7 y_interval = 400
8 mycolors = ['tab:red', 'tab:blue', 'tab:green', 'tab:orange']
http://liyangbit.com 201Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
9
10 # Draw Plot and Annotate
11 fig, ax = plt.subplots(1,1,figsize=(16, 9), dpi= 80)
12
13 columns = df.columns[1:]
14 for i, column in enumerate(columns):
15 # 原 文 此 处 有 误，Python数 据 之 道 备 注
16 # 访 问 liyangbit.com ， 查 看 本 文 完 整 内 容
17 plt.plot(df.date.values, df[column].values, lw=1.5, color=mycolors[i])
18 plt.text(df.shape[0]+1, df[column].values[-1], column, fontsize=14, color=
mycolors[i])
19
20 # Draw Tick lines
21 for y in range(y_LL, y_UL, y_interval):
22 plt.hlines(y, xmin=0, xmax=71, colors='black', alpha=0.3, linestyles=”--”,
lw=0.5)
23
24 # Decorations
25 plt.tick_params(axis=”both”, which=”both”, bottom=False, top=False,
26 labelbottom=True, left=False, right=False, labelleft=True)
27
28 # Lighten borders
29 plt.gca().spines[”top”].set_alpha(.3)
30 plt.gca().spines[”bottom”].set_alpha(.3)
31 plt.gca().spines[”right”].set_alpha(.3)
32 plt.gca().spines[”left”].set_alpha(.3)
33
34 plt.title('Number of Deaths from Lung Diseases in the UK (1974-1979)', fontsize
=22)
35 plt.yticks(range(y_LL, y_UL, y_interval), [str(y) for y in range(y_LL, y_UL,
y_interval)], fontsize=12)
36 plt.xticks(range(0, df.shape[0], 12), df.date.values[::12], horizontalalignment
='left', fontsize=12)
37 plt.ylim(y_LL, y_UL)
38 plt.xlim(-2, 80)
39 plt.show()
http://liyangbit.com 202Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.45:多个时间序列
19.8.7 使用辅助Y轴来绘制不同范围的图形（PlottingwithdifferentscalesusingsecondaryY
axis）
如果要显示在同一时间点测量两个不同数量的两个时间序列，则可以在右侧的辅助Y轴上再绘制第二个系列。
1 # Import Data
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/economics.csv”
)
3
4 x = df['date']
5 y1 = df['psavert']
6 y2 = df['unemploy']
7
8 # Plot Line1 (Left Y Axis)
9 fig, ax1 = plt.subplots(1,1,figsize=(16,9), dpi= 80)
10 ax1.plot(x, y1, color='tab:red')
11
12 # Plot Line2 (Right Y Axis)
13 ax2 = ax1.twinx() # instantiate a second axes that shares the same x-axis
14 ax2.plot(x, y2, color='tab:blue')
15
16 # Decorations
17 # ax1 (left Y axis)
18 ax1.set_xlabel('Year', fontsize=20)
19 ax1.tick_params(axis='x', rotation=0, labelsize=12)
20 ax1.set_ylabel('Personal Savings Rate', color='tab:red', fontsize=20)
21 ax1.tick_params(axis='y', rotation=0, labelcolor='tab:red' )
22 ax1.grid(alpha=.4)
23
http://liyangbit.com 203Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
24 # ax2 (right Y axis)
25 ax2.set_ylabel(”# Unemployed (1000's)”, color='tab:blue', fontsize=20)
26 ax2.tick_params(axis='y', labelcolor='tab:blue')
27 ax2.set_xticks(np.arange(0, len(x), 60))
28 ax2.set_xticklabels(x[::60], rotation=90, fontdict={'fontsize':10})
29 ax2.set_title(”Personal Savings Rate vs Unemployed: Plotting in Secondary Y
Axis”, fontsize=22)
30 fig.tight_layout()
31 plt.show()
Figure19.46:使用辅助Y轴来绘制不同范围的图形
19.8.8 带有误差带的时间序列（TimeSerieswithErrorBands）
如果您有一个时间序列数据集，每个时间点（日期/时间戳）有多个观测值，则可以构建带有误差带的时间序列。您
可以在下面看到一些基于每天不同时间订单的示例。另一个关于45天持续到达的订单数量的例子。
在该方法中，订单数量的平均值由白线表示。并且计算95％置信区间并围绕均值绘制。
1 from scipy.stats import sem
2
3 # Import Data
4 df = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/master/
user_orders_hourofday.csv”)
5 df_mean = df.groupby('order_hour_of_day').quantity.mean()
6 df_se = df.groupby('order_hour_of_day').quantity.apply(sem).mul(1.96)
7
8 # Plot
9 plt.figure(figsize=(16,10), dpi= 80)
10 plt.ylabel(”# Orders”, fontsize=16)
11 x = df_mean.index
http://liyangbit.com 204Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
12 plt.plot(x, df_mean, color=”white”, lw=2)
13 plt.fill_between(x, df_mean - df_se, df_mean + df_se, color=”#3F5D7D”)
14
15 # Decorations
16 # Lighten borders
17 plt.gca().spines[”top”].set_alpha(0)
18 plt.gca().spines[”bottom”].set_alpha(1)
19 plt.gca().spines[”right”].set_alpha(0)
20 plt.gca().spines[”left”].set_alpha(1)
21 plt.xticks(x[::2], [str(d) for d in x[::2]] , fontsize=12)
22 plt.title(”User Orders by Hour of Day (95% confidence)”, fontsize=22)
23 plt.xlabel(”Hour of Day”)
24
25 s, e = plt.gca().get_xlim()
26 plt.xlim(s, e)
27
28 # Draw Horizontal Tick lines
29 for y in range(8, 20, 2):
30 plt.hlines(y, xmin=s, xmax=e, colors='black', alpha=0.5, linestyles=”--”,
lw=0.5)
31
32 plt.show()
Figure19.47:带有误差带的时间序列
1 # ”Data Source: https://www.kaggle.com/olistbr/brazilian-ecommerce#
olist_orders_dataset.csv”
2 from dateutil.parser import parse
3 from scipy.stats import sem
http://liyangbit.com 205Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
4
5 # Import Data
6 df_raw = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master
/orders_45d.csv',
7 parse_dates=['purchase_time', 'purchase_date'])
8
9 # Prepare Data: Daily Mean and SE Bands
10 df_mean = df_raw.groupby('purchase_date').quantity.mean()
11 df_se = df_raw.groupby('purchase_date').quantity.apply(sem).mul(1.96)
12
13 # Plot
14 plt.figure(figsize=(16,10), dpi= 80)
15 plt.ylabel(”# Daily Orders”, fontsize=16)
16 x = [d.date().strftime('%Y-%m-%d') for d in df_mean.index]
17 plt.plot(x, df_mean, color=”white”, lw=2)
18 plt.fill_between(x, df_mean - df_se, df_mean + df_se, color=”#3F5D7D”)
19
20 # Decorations
21 # Lighten borders
22 plt.gca().spines[”top”].set_alpha(0)
23 plt.gca().spines[”bottom”].set_alpha(1)
24 plt.gca().spines[”right”].set_alpha(0)
25 plt.gca().spines[”left”].set_alpha(1)
26 plt.xticks(x[::6], [str(d) for d in x[::6]] , fontsize=12)
27 plt.title(”Daily Order Quantity of Brazilian Retail with Error Bands (95%
confidence)”, fontsize=20)
28
29 # Axis limits
30 s, e = plt.gca().get_xlim()
31 plt.xlim(s, e-2)
32 plt.ylim(4, 10)
33
34 # Draw Horizontal Tick lines
35 for y in range(5, 10, 1):
36 plt.hlines(y, xmin=s, xmax=e, colors='black', alpha=0.5, linestyles=”--”,
lw=0.5)
37
38 plt.show()
http://liyangbit.com 206Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.48:带有误差带的时间序列
19.8.9 堆积面积图（StackedAreaChart）
堆积面积图可以直观地显示多个时间序列的贡献程度，因此很容易相互比较。
1 # Import Data
2 df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/
nightvisitors.csv')
3
4 # Decide Colors
5 mycolors = ['tab:red', 'tab:blue', 'tab:green', 'tab:orange', 'tab:brown', 'tab
:grey', 'tab:pink', 'tab:olive']
6
7 # Draw Plot and Annotate
8 fig, ax = plt.subplots(1,1,figsize=(16, 9), dpi= 80)
9 columns = df.columns[1:]
10 labs = columns.values.tolist()
11
12 # Prepare data
13 x = df['yearmon'].values.tolist()
14 y0 = df[columns[0]].values.tolist()
15 y1 = df[columns[1]].values.tolist()
16 y2 = df[columns[2]].values.tolist()
17 y3 = df[columns[3]].values.tolist()
18 y4 = df[columns[4]].values.tolist()
19 y5 = df[columns[5]].values.tolist()
20 y6 = df[columns[6]].values.tolist()
21 y7 = df[columns[7]].values.tolist()
22 y = np.vstack([y0, y2, y4, y6, y7, y5, y1, y3])
http://liyangbit.com 207Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
23
24 # Plot for each column
25 labs = columns.values.tolist()
26 ax = plt.gca()
27 ax.stackplot(x, y, labels=labs, colors=mycolors, alpha=0.8)
28
29 # Decorations
30 ax.set_title('Night Visitors in Australian Regions', fontsize=18)
31 ax.set(ylim=[0, 100000])
32 ax.legend(fontsize=10, ncol=4)
33 plt.xticks(x[::5], fontsize=10, horizontalalignment='center')
34 plt.yticks(np.arange(10000, 100000, 20000), fontsize=10)
35 plt.xlim(x[0], x[-1])
36
37 # Lighten borders
38 plt.gca().spines[”top”].set_alpha(0)
39 plt.gca().spines[”bottom”].set_alpha(.3)
40 plt.gca().spines[”right”].set_alpha(0)
41 plt.gca().spines[”left”].set_alpha(.3)
42
43 plt.show()
Figure19.49:堆积面积图
19.8.10 未堆积的面积图（AreaChartUnStacked）
未堆积面积图用于可视化两个或更多个系列相对于彼此的进度（起伏）。在下面的图表中，您可以清楚地看到随着
失业中位数持续时间的增加，个人储蓄率会下降。未堆积面积图表很好地展示了这种现象。
1 # Import Data
http://liyangbit.com 208Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
2 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/economics.csv”
)
3
4 # Prepare Data
5 x = df['date'].values.tolist()
6 y1 = df['psavert'].values.tolist()
7 y2 = df['uempmed'].values.tolist()
8 mycolors = ['tab:red', 'tab:blue', 'tab:green', 'tab:orange', 'tab:brown', 'tab
:grey', 'tab:pink', 'tab:olive']
9 columns = ['psavert', 'uempmed']
10
11 # Draw Plot
12 fig, ax = plt.subplots(1, 1, figsize=(16,9), dpi= 80)
13 ax.fill_between(x, y1=y1, y2=0, label=columns[1], alpha=0.5, color=mycolors[1],
linewidth=2)
14 ax.fill_between(x, y1=y2, y2=0, label=columns[0], alpha=0.5, color=mycolors[0],
linewidth=2)
15
16 # Decorations
17 ax.set_title('Personal Savings Rate vs Median Duration of Unemployment',
fontsize=18)
18 ax.set(ylim=[0, 30])
19 ax.legend(loc='best', fontsize=12)
20 plt.xticks(x[::50], fontsize=10, horizontalalignment='center')
21 plt.yticks(np.arange(2.5, 30.0, 2.5), fontsize=10)
22 plt.xlim(-10, x[-1])
23
24 # Draw Tick lines
25 for y in np.arange(2.5, 30.0, 2.5):
26 plt.hlines(y, xmin=0, xmax=len(x), colors='black', alpha=0.3, linestyles=”
--”, lw=0.5)
27
28 # Lighten borders
29 plt.gca().spines[”top”].set_alpha(0)
30 plt.gca().spines[”bottom”].set_alpha(.3)
31 plt.gca().spines[”right”].set_alpha(0)
32 plt.gca().spines[”left”].set_alpha(.3)
33 plt.show()
http://liyangbit.com 209Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.50:未堆积的面积图
19.8.11 日历热力图（CalendarHeatMap）
与时间序列相比，日历地图是可视化基于时间的数据的备选和不太优选的选项。虽然可以在视觉上吸引人，但数值
并不十分明显。然而，它可以很好地描绘极端值和假日效果。（『Python数据之道』注：需要安装calmap库）
1 import matplotlib as mpl
2
3 # pip install calmap
4 # Python数 据 之 道 备 注
5 import calmap
6
7 # Import Data
8 df = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/master/
yahoo.csv”, parse_dates=['date'])
9 df.set_index('date', inplace=True)
10
11 # Plot
12 plt.figure(figsize=(16,10), dpi= 80)
13 calmap.calendarplot(df['2014']['VIX.Close'], fig_kws={'figsize': (16,10)},
yearlabel_kws={'color':'black', 'fontsize':14}, subplot_kws={'title':'Yahoo
Stock Prices'})
14 plt.show()
http://liyangbit.com 210Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.51:未堆积的面积图
19.8.12 季节图（SeasonalPlot）
季节图可用于比较上一季中同一天（年/月/周等）的时间序列。
1 from dateutil.parser import parse
2
3 # Import Data
4 df = pd.read_csv('https://github.com/selva86/datasets/raw/master/AirPassengers.
csv')
5
6 # Prepare data
7 df['year'] = [parse(d).year for d in df.date]
8 df['month'] = [parse(d).strftime('%b') for d in df.date]
9 years = df['year'].unique()
10
11 # Draw Plot
12 mycolors = ['tab:red', 'tab:blue', 'tab:green', 'tab:orange', 'tab:brown', 'tab
:grey', 'tab:pink', 'tab:olive', 'deeppink', 'steelblue', 'firebrick', '
mediumseagreen']
13 plt.figure(figsize=(16,10), dpi= 80)
14
15 for i, y in enumerate(years):
16 plt.plot('month', 'traffic', data=df.loc[df.year==y, :], color=mycolors[i],
label=y)
17 plt.text(df.loc[df.year==y, :].shape[0]-.9, df.loc[df.year==y, 'traffic'
][-1:].values[0], y, fontsize=12, color=mycolors[i])
18
19 # Decoration
20 plt.ylim(50,750)
21 plt.xlim(-0.3, 11)
22 plt.ylabel('$Air Traffic$')
23 plt.yticks(fontsize=12, alpha=.7)
24 plt.title(”Monthly Seasonal Plot: Air Passengers Traffic (1949 - 1969)”,
fontsize=22)
25 plt.grid(axis='y', alpha=.3)
26
27 # Remove borders
28 plt.gca().spines[”top”].set_alpha(0.0)
29 plt.gca().spines[”bottom”].set_alpha(0.5)
30 plt.gca().spines[”right”].set_alpha(0.0)
31 plt.gca().spines[”left”].set_alpha(0.5)
http://liyangbit.com 211Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
32 # plt.legend(loc='upper right', ncol=2, fontsize=12)
33 plt.show()
Figure19.52:季节图
19.9 分组（Groups）
19.9.1 树状图（Dendrogram）
树形图基于给定的距离度量将相似的点组合在一起，并基于点的相似性将它们组织在树状链接中。
1 import scipy.cluster.hierarchy as shc
2
3 # Import Data
4 df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/
USArrests.csv')
5
6 # Plot
7 plt.figure(figsize=(16, 10), dpi= 80)
8 plt.title(”USArrests Dendograms”, fontsize=22)
9 dend = shc.dendrogram(shc.linkage(df[['Murder', 'Assault', 'UrbanPop', 'Rape'
]], method='ward'), labels=df.State.values, color_threshold=100)
10 plt.xticks(fontsize=12)
11 plt.show()
http://liyangbit.com 212Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.53:树状图
19.9.2 簇状图（ClusterPlot）
簇状图（ClusterPlot）可用于划分属于同一群集的点。下面是根据USArrests数据集将美国各州分为5组的代
表性示例。此图使用“谋杀”和“攻击”列作为X和Y轴。或者，您可以将第一个到主要组件用作X轴和Y轴。
1 from sklearn.cluster import AgglomerativeClustering
2 from scipy.spatial import ConvexHull
3
4 # Import Data
5 df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/
USArrests.csv')
6
7 # Agglomerative Clustering
8 cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='
ward')
9 cluster.fit_predict(df[['Murder', 'Assault', 'UrbanPop', 'Rape']])
10
11 # Plot
12 plt.figure(figsize=(14, 10), dpi= 80)
13 plt.scatter(df.iloc[:,0], df.iloc[:,1], c=cluster.labels_, cmap='tab10')
14
15 # Encircle
16 def encircle(x,y, ax=None, **kw):
17 if not ax: ax=plt.gca()
18 p = np.c_[x,y]
http://liyangbit.com 213Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
19 hull = ConvexHull(p)
20 poly = plt.Polygon(p[hull.vertices,:], **kw)
21 ax.add_patch(poly)
22
23 # Draw polygon surrounding vertices
24 encircle(df.loc[cluster.labels_ == 0, 'Murder'], df.loc[cluster.labels_ == 0, '
Assault'], ec=”k”, fc=”gold”, alpha=0.2, linewidth=0)
25 encircle(df.loc[cluster.labels_ == 1, 'Murder'], df.loc[cluster.labels_ == 1, '
Assault'], ec=”k”, fc=”tab:blue”, alpha=0.2, linewidth=0)
26 encircle(df.loc[cluster.labels_ == 2, 'Murder'], df.loc[cluster.labels_ == 2, '
Assault'], ec=”k”, fc=”tab:red”, alpha=0.2, linewidth=0)
27 encircle(df.loc[cluster.labels_ == 3, 'Murder'], df.loc[cluster.labels_ == 3, '
Assault'], ec=”k”, fc=”tab:green”, alpha=0.2, linewidth=0)
28 encircle(df.loc[cluster.labels_ == 4, 'Murder'], df.loc[cluster.labels_ == 4, '
Assault'], ec=”k”, fc=”tab:orange”, alpha=0.2, linewidth=0)
29
30 # Decorations
31 plt.xlabel('Murder'); plt.xticks(fontsize=12)
32 plt.ylabel('Assault'); plt.yticks(fontsize=12)
33 plt.title('Agglomerative Clustering of USArrests (5 Groups)', fontsize=22)
34 plt.show()
Figure19.54:簇状图
http://liyangbit.com 214Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
19.9.3 安德鲁斯曲线（AndrewsCurve）
安德鲁斯曲线有助于可视化是否存在基于给定分组的数字特征的固有分组。如果要素（数据集中的列）无法区分组
（cyl），那么这些线将不会很好地隔离，如下所示。
1 from pandas.plotting import andrews_curves
2
3 # Import
4 df = pd.read_csv(”https://github.com/selva86/datasets/raw/master/mtcars.csv”)
5 df.drop(['cars', 'carname'], axis=1, inplace=True)
6
7 # Plot
8 plt.figure(figsize=(12,9), dpi= 80)
9 andrews_curves(df, 'cyl', colormap='Set1')
10
11 # Lighten borders
12 plt.gca().spines[”top”].set_alpha(0)
13 plt.gca().spines[”bottom”].set_alpha(.3)
14 plt.gca().spines[”right”].set_alpha(0)
15 plt.gca().spines[”left”].set_alpha(.3)
16
17 plt.title('Andrews Curves of mtcars', fontsize=22)
18 plt.xlim(-3,3)
19 plt.grid(alpha=0.3)
20 plt.xticks(fontsize=12)
21 plt.yticks(fontsize=12)
22 plt.show()
http://liyangbit.com 215Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.55:安德鲁斯曲线
19.9.4 平行坐标（ParallelCoordinates）
平行坐标有助于可视化特征是否有助于有效地隔离组。如果实现隔离，则该特征可能在预测该组时非常有用。
1 from pandas.plotting import parallel_coordinates
2
3 # Import Data
4 df_final = pd.read_csv(”https://raw.githubusercontent.com/selva86/datasets/
master/diamonds_filter.csv”)
5
6 # Plot
7 plt.figure(figsize=(12,9), dpi= 80)
8 parallel_coordinates(df_final, 'cut', colormap='Dark2')
9
10 # Lighten borders
11 plt.gca().spines[”top”].set_alpha(0)
12 plt.gca().spines[”bottom”].set_alpha(.3)
13 plt.gca().spines[”right”].set_alpha(0)
14 plt.gca().spines[”left”].set_alpha(.3)
15
16 plt.title('Parallel Coordinated of Diamonds', fontsize=22)
17 plt.grid(alpha=0.3)
18 plt.xticks(fontsize=12)
19 plt.yticks(fontsize=12)
http://liyangbit.com 216Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
20 plt.show()
Figure19.56:平行坐标
原文：
Top50matplotlibVisualizations–TheMasterPlots(withfullpythoncode)
Tips：
（1）本文原文部分代码有不准确的地方，已进行修改；
（2）所有正确的源代码，我已整合到jupyternotebook文件中，可以在公众号『Python数据之道』后台回复
“code”，可获得本文源代码；
（3）运行本文代码，除了安装matplotlib和seaborn可视化库外，还需要安装其他的一些辅助可视化库，已在
代码部分作标注。
对我的文章感兴趣的朋友，欢迎关注我的微信公众号『Python数据之道』（ID：PyDataRoad），接收我的更新通
知。
http://liyangbit.com 217Chapter19. Matplotlib可视化最有价值的50个图表 Python数据之道
Figure19.57:微信公众号:Python数据之道
http://liyangbit.com 21820 轻松用 Seaborn 进行数据可视化
作者|MukulSinghChauhan
译者|Lemon
出品|Python数据之道（ID:PyDataLab）
Seaborn是一个数据可视化库，可帮助在Python中创建有趣的数据可视化。大多数数据分析需要识别趋势和建
立模型。本文将帮助您开始使用Seaborn库创建数据可视化。
Seaborn是一个出色的数据可视化库，它让我们的生活变得轻松。首先，您应该在jupyternotebook中键入以
下命令。
1 import pandas as pd # Pandas
2 import numpy as np # Numpy
3 import matplotlib.pyplot as plt # Matplotlibrary
4 import seaborn as sns # Seaborn Library
5 %matplotlib inline
6 sns.set()
20.1 直方图（Distplot）
sns.distplot（）结合直方图并绘制核密度估计图。这里bin区间大小是自动计算的。
𝑠𝑛𝑠.𝑑𝑖𝑠𝑡𝑝𝑙𝑜𝑡(𝑑𝑎𝑡𝑎[“𝑣𝑎𝑟𝑖𝑎𝑏𝑙𝑒𝑛𝑎𝑚𝑒”])
我们将使用以下代码在jupyternotebook中加载数据集。
1 # Load the Dataset in Python
2 tips = sns.load_dataset(”tips”)
3 tips.head()
219Chapter20. 轻松用Seaborn进行数据可视化 Python数据之道
Figure20.1:tips数据集
现在，由于我们已经加载了数据集，我们将使用“total_bill”变量创建第一个图。让我们从tips数据集创建
“total_bill”变量的distplot。
1 sns.distplot(tips[”total_bill”], bins=16, color=”purple”)
2 # Binsize is calculated using square-root of row count.
Figure20.2:直方图
现在，我们来对上述代码进行进一步描述：
• sns.distplot — 这个命令将启动distplot的初始创建
• tips[“total_bill”] — 从tips数据集（数据框）中取出列（total_bill）。在这里，我们应该观察一下，可以使
用方括号来拉取列值，并且列名应该用引号括起来（双引号/单引号）都被接受。
我们推测 — “total_bill”变量本质上是倾斜的，大多数帐单值都在$10-$20范围内。
http://liyangbit.com 220Chapter20. 轻松用Seaborn进行数据可视化 Python数据之道
20.2 联合分布图（Jointplot）
联合分布图（Jointplot）采用两个变量并一起创建直方图和散点图。让我们看一下jointplot的语法。
𝑠𝑛𝑠.𝑗𝑜𝑖𝑛𝑡𝑝𝑙𝑜𝑡(𝑥 =,𝑦 =,𝑑𝑎𝑡𝑎 =)
让我们从tips数据集创建total_bill和tip变量的联合分布图。通常，任何餐厅的小费金额取决于总账单/账单大
小。让我们看看这个情景下是什么样的。代码如下：
1 sns.jointplot(x = ”total_bill”, y = ”tip”, data = tips, color=”purple”)
Figure20.3:联合分布图
如上所述，散点图似乎显示总账单和小费金额之间的强相关性。在它的顶部，我们可以看到各个变量的直方图。
http://liyangbit.com 221Chapter20. 轻松用Seaborn进行数据可视化 Python数据之道
20.2.1 Jointplot:: kind=”hex”
直方图的双变量类比称为“hexbin”图，因为它显示了六边形区间内的观察计数。此图对于相对较大的数据集最有
效。也称为HexbinPlots。
𝑠𝑛𝑠.𝑗𝑜𝑖𝑛𝑡𝑝𝑙𝑜𝑡(𝑥 =,𝑦 =,𝑑𝑎𝑡𝑎 =,𝑘𝑖𝑛𝑑 = ”ℎ𝑒𝑥”)
1 # Jointplot - Scatterplot and Histogram
2 sns.jointplot(x = ”total_bill”, y = ”tip”, data = tips, kind =”hex”,
3 color=”lightcoral”)
Figure20.4:联合分布图:kind=“hex”
有几种类型的值可以放在sns.jointplot中来创建不同的图。默认情况下，联合分布图显示散点图。现在，在上面
的情节图中，它显示了六边形。六边形的深色表示数据点的高密度，其中较浅的颜色表示较少的点。
kind参数值可以是以下取值：
http://liyangbit.com 222Chapter20. 轻松用Seaborn进行数据可视化 Python数据之道
kind:{“scatter”|“reg”|“resid”|“kde”|“hex”}
下面，我们来看看kind=“kde”的情形。
20.2.2 Jointplot:: kind=”kde”
1 # Jointplot - Scatterplot and Histogram
2 sns.jointplot(x = tips[”total_bill”], y = tips[”tip”],kind = ”kde”,
3 color=”purple”) # contour plot
Figure20.5:联合分布图:kind=“kde”
上面显示的图表称为轮廓图。轮廓图（有时称为“水平图”）是一种在二维平面上显示三维表面的方法。它绘制了y
轴上的两个预测变量XY和轮廓的响应变量Z.
http://liyangbit.com 223Chapter20. 轻松用Seaborn进行数据可视化 Python数据之道
20.3 矩阵图（Pairplot）
矩阵图基本上绘制了变量之间的成对关系。它支持用“hue”来为类别变量绘图着色。
𝑠𝑛𝑠.𝑝𝑎𝑖𝑟𝑝𝑙𝑜𝑡(“𝑑𝑎𝑡𝑎𝑓𝑟𝑎𝑚𝑒”)
1 # Pairplot of Tips
2 sns.pairplot(tips, hue = ”sex”, palette=”Set2”)
3 # this will color the plot gender wise
Figure20.6:矩阵图
下面我们来了解下矩阵图的含义。对角线部分显示了具有核密度估计的distplot图或直方图。矩阵图的上部和下
部显示散点图。“hue”使用列的类别为绘图着色。
• hue=“sex” — 设置为按不同的性别进行着色
• palette=“Set2”-“Set2”是颜色的一个系列。
http://liyangbit.com 224Chapter20. 轻松用Seaborn进行数据可视化 Python数据之道
20.4 条形图（Barplot）
条形图用于绘制分类列和数字列。它在可视化中创建了条形。让我们用“性别”创建一个“total_bill”的条形图，让
我们看看哪类人支付更多。
𝑠𝑛𝑠.𝑏𝑎𝑟𝑝𝑙𝑜𝑡(𝑥 =,𝑦 =,𝑑𝑎𝑡𝑎 =)
1 # Barplot
2 sns.barplot(x =”sex” , y =”total_bill” , data=tips)
3 # Inference - Total Bill Amount for males is more than Females.
Figure20.7:条形图
1 # Lets Plot Smoker Vs Total Bill :: The purpose is to find out if
2 # Smokers pay more bill than Non Smokers
3 sns.barplot(x = ”smoker”, y = ”total_bill”, data =tips)
4 # Inference - More Bill for Smokers
http://liyangbit.com 225Chapter20. 轻松用Seaborn进行数据可视化 Python数据之道
Figure20.8:条形图
1 # Lets Find If There is more Bill In Weekend or Weekdays
2 sns.barplot(x = ”day”, y = ”total_bill”, data =tips)
3 # People tend to visit more on weekends
Figure20.9:条形图
http://liyangbit.com 226Chapter20. 轻松用Seaborn进行数据可视化 Python数据之道
20.5 箱形图（Boxplot）
箱形图（Boxplot）是给定数据集的五点汇总统计的直观表示。五个数字摘要包括：
• Minimum最小值
• FirstQuartile1/4值
• Median(SecondQuartile)中位数
• ThirdQuartile3/4值
• Maximum最大值
此外，值得注意的一点是，为分类-连续变量创建了一个箱线图，这意味着如果x轴是分类的并且y轴是连续的，
则应创建箱线图或小提琴图。
让我们从tips数据集创建一个“day”和“total_bill”的箱线图。
𝑠𝑛𝑠.𝑏𝑜𝑥𝑝𝑙𝑜𝑡(𝑥 =,𝑦 =,𝑑𝑎𝑡𝑎 =)
1 # Boxplot
2 sns.boxplot(x = ”day”, y = ”total_bill”, data=tips)
Figure20.10:箱形图
1 # Add hue to split the barplot. Making it more fancier
2 sns.boxplot(x = ”day”, y = ”total_bill”, data=tips, hue = ”smoker”)
3 # On Friday people have more bill if they are a Non smoker vs smoker
http://liyangbit.com 227Chapter20. 轻松用Seaborn进行数据可视化 Python数据之道
Figure20.11:箱形图
hue=“smoker”：-它为吸烟者和非吸烟者创造了一个箱线图。例如：在星期五的情况下，可以清楚地看到，与当
天的吸烟者相比，非吸烟者的食物费用更多。
1 # Violin Plots
2 sns.violinplot(x = ”day”, y = ”total_bill”, data = tips)
Figure20.12:小提琴图
http://liyangbit.com 228Chapter20. 轻松用Seaborn进行数据可视化 Python数据之道
小提琴图跟箱形图有些类似。他们之间的描述可以参考下面的图示内容：
Figure20.13:小提琴图示意图
20.6 LMPlot
sns.lmplot是绘制一个在散点图中进行线性回归拟合的直线。它遵循普通的最小二乘法，线代表最佳拟合线。为
了更好地理解这一点，建议进一步了解一下线性回归的相关知识。
代码如下：
1 # LM PLot
2 sns.lmplot(x = ”total_bill”, y = ”tip”, data = tips, hue=”day”)
http://liyangbit.com 229Chapter20. 轻松用Seaborn进行数据可视化 Python数据之道
Figure20.14:LMPlot
上图显示了不同日期的total_bill变量的线性回归拟合，如图例中所示，这是在sns.lmplot中使用hue=“day”
获得的。
恭喜！您已完成Seaborn初学者教程。希望这篇文章能够提供有关Seaborn的基本知识，并且可以帮助您创建
所有这些图。
源代码：在微信公众号“Python数据之道”（ID：PyDataLab）后台回复关键字“code”获取。
原文：
DataVisualisationUsingSeaborn
http://liyangbit.com 23021 Seaborn 可视化：图形个性化设置的几个小技巧
21.1 概述
在可视化过程中，经常会对默认的制图效果不满意，希望能个性化进行各种设置。
本文通过一个简单的示例，来介绍seaborn可视化过程中的个性化设置。包括常用的设置，如：
1. 设置图表显示颜色
2. 设置图表标题，包括显示位置，字体大小，颜色等
3. 设置x轴和y轴标题，包括颜色，字体大小
4. 设置x轴和y轴刻度内容，包括颜色、字体大小、字体方向等
5. 将x轴和y轴内容逆序显示
6. 设置x轴或y轴显示位置
本文的运行环境：
1. windows7
2. python3.5
3. jupyternotebook
4. seaborn0.7.1
5. matplotlib2.0.2
21.2 未个性化设置的情形
本文的数据来自UCI的数据集“sonar”，用pandas直接读取数据。如下：
1 import pandas as pd
2 import matplotlib.pyplot as plt
3 import seaborn as sns
4 % matplotlib inline
5
6 target_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/
undocumented/connectionist-bench/sonar/sonar.all-data'
7 df = pd.read_csv(target_url, header=None, prefix='V')
8 corr = df.corr()
首先来看看没有进行个性化设置时的显示情况，如下：
1 f, ax= plt.subplots(figsize = (14, 10))
231Chapter21. Seaborn可视化：图形个性化设置的几个小技巧 Python数据之道
2
3 sns.heatmap(corr,cmap='RdBu', linewidths = 0.05, ax = ax)
4
5 # 设 置Axes的 标 题
6 ax.set_title('Correlation between features')
7
8 f.savefig('sns_style_origin.jpg', dpi=100, bbox_inches='tight')
图片显示效果如下：
seaborn制图的默认效果其实还是不错的。
21.3 进行个性化设置
对于上面这张图，可能让y轴从下到上，从v0开始显示，这样显示出来的对角线可能更符合我们的视觉显示效果。
这就要用到将y轴内容进行可逆显示，涉及的代码如下：
1 # 将y轴 或x轴 进 行 逆 序
2 ax.invert_yaxis()
3 # ax.invert_xaxis()
其他的个性化设置的代码，包括：
将x轴刻度放置在top位置的几种方法
http://liyangbit.com 232Chapter21. Seaborn可视化：图形个性化设置的几个小技巧 Python数据之道
1 # 将x轴 刻 度 放 置 在top位 置 的 几 种 方 法
2 # ax.xaxis.set_ticks_position('top')
3 ax.xaxis.tick_top()
4 # ax.tick_params(axis='x',labelsize=6, colors='b', labeltop=True, labelbottom=
False) # x轴
设置坐标轴刻度参数，“axis”不写的时候，默认是x轴和y轴的参数同时调整。
1 # 设 置 坐 标 轴 刻 度 的 字 体 大 小
2 # matplotlib.axes.Axes.tick_params
3 ax.tick_params(axis='y',labelsize=8) # y轴
旋转轴刻度上文字方向的两种方法
1 # 旋 转 轴 刻 度 上 文 字 方 向 的 两 种 方 法
2 ax.set_xticklabels(ax.get_xticklabels(), rotation=-90)
3 # ax.set_xticklabels(corr.index, rotation=90)
保存图片，设置bbox_inches=“tight”，保存的图片则不会出现部分内容显示不全的现象。
1 f.savefig('sns_style_update.jpg', dpi=100, bbox_inches='tight')
整合好的代码如下，大家可以运行试试效果。
1 f, ax = plt.subplots(figsize = (14, 10))
2
3 # 设 置 颜 色
4 cmap = sns.cubehelix_palette(start = 1, rot = 3, gamma=0.8, as_cmap = True)
5
6 # color: https://matplotlib.org/users/colormaps.html
7 sns.heatmap(corr,cmap='RdBu', linewidths = 0.05, ax = ax)
8
9 # 设 置Axes的 标 题
10 ax.set_title('Correlation between features', fontsize=18, position=(0.5,1.05))
11
12 # 将y轴 或x轴 进 行 逆 序
13 ax.invert_yaxis()
14 # ax.invert_xaxis()
15
16 ax.set_xlabel('X Label',fontsize=10)
17
18 # 设 置Y轴 标 签 的 字 体 大 小 和 字 体 颜 色
19 ax.set_ylabel('Y Label',fontsize=15, color='r')
20
21 # 设 置 坐 标 轴 刻 度 的 字 体 大 小
22 # matplotlib.axes.Axes.tick_params
23 ax.tick_params(axis='y',labelsize=8) # y轴
24 # ax.tick_params(axis='x',labelsize=6, colors='b', labeltop=True, labelbottom=
False) # x轴
25
26 # 将x轴 刻 度 放 置 在top位 置 的 几 种 方 法
http://liyangbit.com 233Chapter21. Seaborn可视化：图形个性化设置的几个小技巧 Python数据之道
27 # ax.xaxis.set_ticks_position('top')
28 ax.xaxis.tick_top()
29 # ax.tick_params(axis='x',labelsize=6, colors='b', labeltop=True, labelbottom=
False) # x轴
30
31 # 修 改tick的 字 体 颜 色
32 # ax.tick_params(axis='x', colors='b') # x轴
33
34 # 旋 转 轴 刻 度 上 文 字 方 向 的 两 种 方 法
35 ax.set_xticklabels(ax.get_xticklabels(), rotation=-90)
36 # ax.set_xticklabels(corr.index, rotation=90)
37
38 # 单 独 设 置y轴 或x轴 刻 度 的 字 体 大 小, 调 整 字 体 方 向
39 # ax.set_yticklabels(ax.get_yticklabels(),fontsize=6)
40 # ax.set_xticklabels(ax.get_xticklabels(), rotation=-90)
41
42 f.savefig('sns_style_update.jpg', dpi=100, bbox_inches='tight')
图形显示效果如下：
这些个性化的设置，其实大部分都是使用的matplotlib的内容，seaborn是基于matplotlib衍生的，所以可
以跟matplotlib进行融合使用。
当然，并不是每次都需要进行个性定制，具体可以根据自己的需求来设置。
http://liyangbit.com 23422 Seaborn 热力图使用进阶
在日常工作中，经常可以见到各种各种精美的热力图，热力图的应用非常广泛，下面一起来学习下Python的
Seaborn库中热力图（heatmap）如何来进行使用。
本次运行的环境为：
• windows64位系统
• python3.5
• jupyternotebook
22.1 构造数据
1 import seaborn as sns
2 import pandas as pd
3 import numpy as np
4 import matplotlib.pyplot as plt
5 % matplotlib inline
1 region = ['Albania', 'Algeria', 'Angola', 'Argentina', 'Armenia', 'Azerbaijan',
2 'Bahamas', 'Bangladesh', 'Belize', 'Bhutan', 'Bolivia',
3 'Bosnia and Herzegovina', 'Brazil', 'Burkina Faso', 'Burundi',
4 'Cambodia', 'Cameroon', 'Cape Verde', 'Chile', 'China', 'Colombia',
5 'Costa Rica', 'Cote d Ivoire', 'Cuba', 'Cyprus',
6 ”Democratic People's Republic of Korea”,
7 'Democratic Republic of the Congo', 'Dominican Republic', 'Ecuador',
8 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Ethiopia', 'Fiji',
9 'Gambia', 'Georgia', 'Ghana', 'Guatemala', 'Guyana', 'Honduras']
10
11
12 kind = ['Afforestation & reforestation', 'Biofuels', 'Biogas',
13 'Biomass', 'Cement', 'Energy efficiency', 'Fuel switch',
14 'HFC reduction/avoidance', 'Hydro power',
15 'Leak reduction', 'Material use', 'Methane avoidance',
16 'N2O decomposition', 'Other renewable energies',
17 'PFC reduction and substitution','PV',
18 'SF6 replacement', 'Transportation', 'Waste gas/heat utilization',
19 'Wind power']
1 print(len(region))
235Chapter22. Seaborn热力图使用进阶 Python数据之道
2 print(len(kind))
1 40
2 20
1 np.random.seed(100)
2 arr_region = np.random.choice(region, size=(10000,))
3 list_region = list(arr_region)
4
5 arr_kind = np.random.choice(kind, size=(10000,))
6 list_kind = list(arr_kind)
7
8 values = np.random.randint(50, 1000, 10000)
9 list_values = list(values)
10
11 df = pd.DataFrame({'region':list_region,
12 'kind': list_kind,
13 'values':list_values})
14 df.head()
Figure22.1:data
1 pt = df.pivot_table(index='kind', columns='region', values='values', aggfunc=np
.sum)
2 pt.head()
Figure22.2:data
http://liyangbit.com 236Chapter22. Seaborn热力图使用进阶 Python数据之道
1 f, ax = plt.subplots(figsize = (10, 4))
2 cmap = sns.cubehelix_palette(start = 1, rot = 3, gamma=0.8, as_cmap = True)
3 sns.heatmap(pt, cmap = cmap, linewidths = 0.05, ax = ax)
4 ax.set_title('Amounts per kind and region')
5 ax.set_xlabel('region')
6 ax.set_ylabel('kind')
7
8 f.savefig('sns_heatmap_normal.jpg', bbox_inches='tight')
9 # ax.set_xticklabels(ax.get_xticklabels(), rotation=-90)
Figure22.3:img-normal
22.2 Seaborn的heatmap各个参数介绍
seaborn.heatmap
seaborn.heatmap(data, vmin=None, vmax=None, cmap=None, center=None, robust=False, an-
not=None,fmt=“.2g”,annot_kws=None,linewidths=0,linecolor=“white”,cbar=True,cbar_kws=None,
cbar_ax=None,square=False,ax=None,xticklabels=True,yticklabels=True,mask=None,**kwargs)
• data：矩阵数据集，可以使numpy的数组（array），如果是pandas的dataframe，则df的index/column
信息会分别对应到heatmap的columns和rows
• linewidths,热力图矩阵之间的间隔大小
• vmax,vmin,图例中最大值和最小值的显示值，没有该参数时默认不显示
http://liyangbit.com 237Chapter22. Seaborn热力图使用进阶 Python数据之道
22.2.1 cmap
• cmap：matplotlib的colormap名称或颜色对象；如果没有提供，默认为cubehelixmap(数据集为连续
数据集时)或RdBu_r(数据集为离散数据集时)
1 f, (ax1,ax2) = plt.subplots(figsize = (10, 8),nrows=2)
2
3 # cubehelix map颜 色
4 cmap = sns.cubehelix_palette(start = 1.5, rot = 3, gamma=0.8, as_cmap = True)
5 sns.heatmap(pt, linewidths = 0.05, ax = ax1, vmax=15000, vmin=0, cmap=cmap)
6 ax1.set_title('cubehelix map')
7 ax1.set_xlabel('')
8 ax1.set_xticklabels([]) #设 置x轴 图 例 为 空 值
9 ax1.set_ylabel('kind')
10
11 # matplotlib colormap
12 sns.heatmap(pt, linewidths = 0.05, ax = ax2, vmax=15000, vmin=0, cmap='rainbow'
)
13 # rainbow为 matplotlib 的colormap名 称
14 ax2.set_title('matplotlib colormap')
15 ax2.set_xlabel('region')
16 ax2.set_ylabel('kind')
17
18 f.savefig('sns_heatmap_cmap.jpg', bbox_inches='tight')
http://liyangbit.com 238Chapter22. Seaborn热力图使用进阶 Python数据之道
Figure22.4:img-camp
22.2.2 center
• center: 将数据设置为图例中的均值数据，即图例中心的数据值；通过设置center值，可以调整生成的图像颜
色的整体深浅；设置center数据时，如果有数据溢出，则手动设置的vmax、vmin会自动改变
1 f, (ax1,ax2) = plt.subplots(figsize = (10, 8),nrows=2)
2
3 cmap = sns.cubehelix_palette(start = 1.5, rot = 3, gamma=0.8, as_cmap = True)
4
5 sns.heatmap(pt, linewidths = 0.05, ax = ax1, vmax=15000, vmin=0, cmap=cmap,
center=None )
6 # center为None时， 由 于 最 小 值 为0， 最 大 值 为15000， 相 当 于center值 为vamx和vmin的 均
值， 即7500
7 ax1.set_title('center=None')
8 ax1.set_xlabel('')
9 ax1.set_xticklabels([]) #设 置x轴 图 例 为 空 值
10 ax1.set_ylabel('kind')
http://liyangbit.com 239Chapter22. Seaborn热力图使用进阶 Python数据之道
11
12 sns.heatmap(pt, linewidths = 0.05, ax = ax2, vmax=15000, vmin=0, cmap=cmap,
center=3000 )
13 # 由 于 均 值 为2000， 当center设 置 为3000时， 大 部 分 数 据 会 比7500大， 所 以center=3000
时， 生 成 的 图 片 颜 色 要 深
14 # 设 置center数 据 时， 如 果 有 数 据 溢 出， 则 手 动 设 置 的vmax或vmin会 自 动 改 变
15 ax2.set_title('center=3000')
16 ax2.set_xlabel('region')
17 ax2.set_ylabel('kind')
18
19 f.savefig('sns_heatmap_center.jpg', bbox_inches='tight')
Figure22.5:img-center
22.2.3 robust
1 f, (ax1,ax2) = plt.subplots(figsize = (10, 8),nrows=2)
2
3 cmap = sns.cubehelix_palette(start = 1.5, rot = 3, gamma=0.8, as_cmap = True)
http://liyangbit.com 240Chapter22. Seaborn热力图使用进阶 Python数据之道
4
5 sns.heatmap(pt, linewidths = 0.05, ax = ax1, cmap=cmap, center=None, robust=
False )
6 # robust默 认 为False
7 ax1.set_title('robust=False')
8 ax1.set_xlabel('')
9 ax1.set_xticklabels([]) #设 置x轴 图 例 为 空 值
10 ax1.set_ylabel('kind')
11
12 sns.heatmap(pt, linewidths = 0.05, ax = ax2, cmap=cmap, center=None, robust=
True )
13 # If True and vmin or vmax are absent, the colormap range is computed with
robust quantiles instead of the extreme values.
14 ax2.set_title('robust=True')
15 ax2.set_xlabel('region')
16 ax2.set_ylabel('kind')
17
18 f.savefig('sns_heatmap_robust.jpg', bbox_inches='tight')
Figure22.6:img-robust
http://liyangbit.com 241Chapter22. Seaborn热力图使用进阶 Python数据之道
22.2.4 mask
1 f, (ax1,ax2) = plt.subplots(figsize = (10, 8),nrows=2)
2
3 cmap = sns.cubehelix_palette(start = 1.5, rot = 3, gamma=0.8, as_cmap = True)
4
5 p1 = sns.heatmap(pt, linewidths = 0.05,ax=ax1, vmax=15000, vmin=0, cmap=cmap,
center=None, robust=False, mask=None )
6 # robust默 认 为False
7 ax1.set_title('mask=None')
8 ax1.set_xlabel('')
9 ax1.set_xticklabels([]) #设 置x轴 图 例 为 空 值
10 ax1.set_ylabel('kind')
11
12 p2 = sns.heatmap(pt, linewidths = 0.05, ax=ax2, vmax=15000, vmin=0, cmap=cmap,
center=None, robust=False, annot=False,mask=pt<10000 )
13 # mask: boolean array or DataFrame
14
15 ax2.set_title('mask: boolean DataFrame')
16 ax2.set_xlabel('region')
17 ax2.set_ylabel('kind')
18
19 f.savefig('sns_heatmap_mask.jpg', bbox_inches='tight')
http://liyangbit.com 242Chapter22. Seaborn热力图使用进阶 Python数据之道
Figure22.7:img-mask
22.2.5 xticklabels,yticklabels
• xticklabels:如果是True，则绘制dataframe的列名。如果是False，则不绘制列名。如果是列表，则绘制列
表中的内容作为xticklabels。如果是整数n，则绘制列名，但每个n绘制一个label。默认为True。
• yticklabels: 如果是True，则绘制dataframe的行名。如果是False，则不绘制行名。如果是列表，则绘制
列表中的内容作为yticklabels。如果是整数n，则绘制列名，但每个n绘制一个label。默认为True。默认为
True。
1 f, (ax1,ax2) = plt.subplots(figsize = (10, 8),nrows=2)
2
3 cmap = sns.cubehelix_palette(start = 1.5, rot = 3, gamma=0.8, as_cmap = True)
4
5 p1 = sns.heatmap(pt, linewidths = 0.05,ax=ax1, vmax=15000, vmin=0, cmap=cmap,
center=None, robust=False, mask=None, xticklabels=False )
6 # robust默 认 为False
7 ax1.set_title('xticklabels=None')
http://liyangbit.com 243Chapter22. Seaborn热力图使用进阶 Python数据之道
8 ax1.set_xlabel('')
9 # ax1.set_xticklabels([]) #设 置x轴 图 例 为 空 值
10 ax1.set_ylabel('kind')
11
12 p2 = sns.heatmap(pt, linewidths = 0.05, ax=ax2, vmax=15000, vmin=0, cmap=cmap,
center=None, robust=False, annot=False,mask=None,xticklabels=3, yticklabels=
list(range(20)) )
13 # mask: boolean array or DataFrame
14
15 ax2.set_title('xticklabels=3, yticklabels is a list')
16 ax2.set_xlabel('region')
17 ax2.set_ylabel('kind')
18
19 f.savefig('sns_heatmap_xyticklabels.jpg', bbox_inches='tight')
Figure22.8:img-ticklabels
22.2.6 annot
• annotate的缩写，annot默认为False，当annot为True时，在heatmap中每个方格写入数据
• annot_kws，当annot为True时，可设置各个参数，包括大小，颜色，加粗，斜体字等
http://liyangbit.com 244Chapter22. Seaborn热力图使用进阶 Python数据之道
1 np.random.seed(0)
2 x = np.random.randn(10, 10)
3 f, (ax1, ax2) = plt.subplots(figsize=(8,8),nrows=2)
4
5 sns.heatmap(x, annot=True, ax=ax1)
6 sns.heatmap(x, annot=True, ax=ax2, annot_kws={'size':9,'weight':'bold', 'color'
:'blue'})
7 # Keyword arguments for ax.text when annot is True.
8 # http://stackoverflow.com/questions/35024475/seaborn-heatmap-key-words
9
10 f.savefig('sns_heatmap_annot.jpg')
Figure22.9:img-annot
关于annot_kws的设置，还有很多值得研究的地方，ax.text有很多属性，有兴趣的可以去研究下；
ax.text可参考官方文档：http://matplotlib.org/api/text_api.html#matplotlib.text.Text
http://liyangbit.com 245Chapter22. Seaborn热力图使用进阶 Python数据之道
22.2.7 fmt
• fmt，格式设置
1 np.random.seed(0)
2 x = np.random.randn(10, 10)
3 f, (ax1, ax2) = plt.subplots(figsize=(8,8),nrows=2)
4
5 sns.heatmap(x, annot=True, ax=ax1)
6 sns.heatmap(x, annot=True, fmt='.1f', ax=ax2)
7
8 f.savefig('sns_heatmap_fmt.jpg')
Figure22.10:img-fmt
http://liyangbit.com 246Chapter22. Seaborn热力图使用进阶 Python数据之道
22.3 案例应用：突出显示某些数据
22.3.1 method1：利用mask来实现
1 f,ax=plt.subplots(figsize=(10,5))
2
3 x = np.random.randn(10, 10)
4 sns.heatmap(x, annot=True, ax=ax)
5 sns.heatmap(x, mask=x < 1, cbar=False, ax=ax,
6 annot=True, annot_kws={”weight”: ”bold”})
7
8 f.savefig('sns_heatmap_eg1.jpg')
Figure22.11:img-mask
22.3.2 method2：利用ax.texts来实现
1 f,ax=plt.subplots(figsize=(10,5))
2
3 flights = sns.load_dataset(”flights”)
4 flights = flights.pivot(”month”, ”year”, ”passengers”)
5 pic = sns.heatmap(flights, annot=True, fmt=”d”, ax=ax)
6
7 for text in pic.texts:
8 text.set_size(8)
9 if text.get_text() == '118':
10 text.set_size(12)
11 text.set_weight('bold')
12 text.set_style('italic')
13
14 f.savefig('sns_heatmap_eg2.jpg')
http://liyangbit.com 247Chapter22. Seaborn热力图使用进阶 Python数据之道
Figure22.12:img-eg2
你可能会发现本文中seaborn的heatmap中还有些参数没有进行介绍，介于篇幅，这里就不在啰嗦了，建议各
位小伙伴自己可以研究下其他参数如何使用。
http://liyangbit.com 24823 Bokeh 入门
一直觉得Bokeh的图很漂亮，今天我们来开始用bokeh制图，本文是开篇，希望大家喜欢~
本次运行环境为：
• win7
• jupyternotebook
• python3.6
• bokeh0.13.0
23.1 常规步骤
bokeh中绘图有两类主要的接口，bokeh.plotting和bokeh.models，其中plotting是高级接口，一般情况
下我们使用bokeh,plotting较多。下面，我们介绍的内容，主要是针对bokeh.plotting
在bokeh中绘制图表的常规步骤：
1. 加载bokeh库，声明在notebook或html文件中显示或输出绘制的图表
2. 绘制图表框架figure()
3. 在figure上绘制具体的图形，比如circle，line，bar等
4. 显示图片，show()
23.2 绘制circle()
首先，在绘制图表框架figure()时，可以设置图表大小（plot_wihth,plot_height），也可以设置图表框架旁边
所用的工具(tools)
• 工具可以是以下value，可以根据实际情况来选择合作的工具使用。
• “crosshair,pan,wheel_zoom,box_zoom,reset,box_select,lasso_select,save”
1 from bokeh.plotting import output_notebook, figure, show
2 import pandas as pd
3 import numpy as np
4
5 output_notebook()
6
7 # tools=”pan,box_zoom,wheel_zoom,reset,save”
8
249Chapter23. Bokeh入门 Python数据之道
9 # 工 具 可 以 是 以 下 value， 可 以 根 据 实 际 情 况 来 选 择 合 作 的 工 具 使 用
10 tools = ”crosshair,pan,wheel_zoom,box_zoom,reset,box_select,lasso_select,save”
11
12 p = figure(plot_width=400, plot_height = 400, tools=tools)
13
14 p.circle([1,2,3,4],[5,6,7,8],size=20, color='red', alpha=0.5)
15
16 show(p)
其中，参数：alpha，数值越小，透明度越高
参数angle,对于circle而言，angle参数似乎没有用……（因为本身就是圆的，旋转角度没有影响）
1 p = figure(plot_width=400, plot_height = 400)
2
3 p.circle([1,2,3,4],[5,6,7,8],size=40, color='#6aa84f', alpha=0.8, angle=0.8)
4
5 show(p)
http://liyangbit.com 250Chapter23. Bokeh入门 Python数据之道
fill_color,fill_alpha,这两个参数用来设置圆体的填充颜色和透明度
1 p = figure(plot_width=400, plot_height = 400)
2
3 p.circle([1,2,3,4],[5,6,7,8],size=40, color='#6aa84f', alpha=0.8,
4 angle=0.8,fill_alpha=0.9, fill_color='blue')
5
6 show(p)
http://liyangbit.com 251Chapter23. Bokeh入门 Python数据之道
• line_alpha线条透明度
• line_color线条颜色
• line_cap的参数似乎没有效果，line_cap的值可以是(butt,round,square)
• line_width,圆圈边缘线条的宽度
1 p = figure(plot_width=400, plot_height = 400)
2
3 p.circle([1,2,3,4],[5,6,7,8],size=40, color='#6aa84f', alpha=0.8,
4 angle=0.8,fill_alpha=0.1, fill_color='blue',
5 line_color='red', line_alpha=0.9, line_width=4,
6 line_cap='square')
7
8 # LineCap = Enumeration(butt, round, square)
9 # Specify how stroked lines should be terminated
10
11 show(p)
http://liyangbit.com 252Chapter23. Bokeh入门 Python数据之道
line_dash
官方描述如下
Acceptlinedashspecifications.
Expresspatternsthatdescribelinedashes. DashPatternvaluescanbespecifiedinavarietyof
ways:
Anenum:“solid”,“dashed”,“dotted”,“dotdash”,“dashdot”atupleorlistofintegersintheHTML5
Canvasdashspecificationstyle. Notethatifthelistofintegershasanoddnumberofelements,
thenitisduplicated,andthatduplicatedlistbecomesthenewdashlist.Toindicatethatdashing
isturnedoff(solidlines),specifytheemptylist[].
1 p = figure(plot_width=400, plot_height = 400)
2
3 p.circle([1,2,3,4],[5,6,7,8],size=40, color='#6aa84f', alpha=0.8,
4 angle=0.8,fill_alpha=0.1, fill_color='blue',
5 line_color='red', line_alpha=0.9, line_width=4,
6 line_cap='square', line_dash='dashed')
7
8 show(p)
http://liyangbit.com 253Chapter23. Bokeh入门 Python数据之道
当然，circle()中还有其他的参数可以设置，有兴趣的童鞋不妨自行摸索下。
对我的文章感兴趣的朋友，可以关注我的微信公众号『Python数据之道』（ID：PyDataLab），接收我的更新通
知。
http://liyangbit.com 25424 Bokeh：figure 详细解读
figure在bokeh绘图中有着重要的作用，今天我们来开始了解figure的用法，希望对大家有所帮助。
本次运行环境为：
• win7
• jupyternotebook
• python3.6
• bokeh0.13.0
24.1 Bokeh中绘图的一般步骤
1. 加载bokeh库，声明在notebook或html文件中显示或输出绘制的图表
2. 绘制图表框架figure()
3. 在figure上绘制具体的图形，比如circle，line，bar等
4. 显示图片，show()
本文主要来介绍figure()的基本用法。
首先，加载bokeh库
1 from bokeh.plotting import figure, output_notebook, show
2
3 output_notebook()
24.1.1 plot_width,plot_weight
plot_width,plot_weight可以设置绘图区的宽度和高度。
设置参数值，如下：
1 p = figure(plot_width=400, plot_height = 400)
2
3 p.circle([1,2,3,4],[5,6,7,8],size=20, color='red', alpha=0.5)
4
5 show(p)
图示如下：
255Chapter24. Bokeh：figure详细解读 Python数据之道
width,weight
width,weight也可以设置绘图区的宽度和高度。
查看官方文档，用的是plot_width和plot_weight两个参数，但实际上width和height也有同样的效果，大
家不妨探索下。
代码如下：
1 p = figure(width=400, height = 400)
2
3 p.circle([1,2,3,4],[5,6,7,8],size=20, color='red', alpha=0.5)
4
5 show(p)
24.1.2 tools
工具可以是以下value，可以根据实际情况来选择合作的工具使用。
“crosshair,pan,wheel_zoom,box_zoom,reset,box_select,lasso_select,save,help”
1 # 工 具 可 以 是 以 下 value， 可 以 根 据 实 际 情 况 来 选 择 合 作 的 工 具 使 用
2 # tools = ”crosshair,pan,wheel_zoom,box_zoom,reset,box_select,lasso_select,save
,help”
http://liyangbit.com 256Chapter24. Bokeh：figure详细解读 Python数据之道
3 tools = ”crosshair,pan,wheel_zoom,reset,save”
4
5
6 p = figure(width=400, height = 400, tools=tools)
7
8 p.circle([1,2,3,4],[5,6,7,8],size=20, color='red', alpha=0.5)
9
10 show(p)
图示如下：
24.1.3 toolbar_location
toolbar_location，设置工具栏显示的位置，参数值可以是：“below,above,left,right”，默认值应该是
“auto”
1 tools = ”crosshair,pan,wheel_zoom,reset”
2
3
4 p = figure(width=400, height = 400, tools=tools, toolbar_location='above')
5
6 p.circle([1,2,3,4],[5,6,7,8],size=20, color='red', alpha=0.5)
7
8 show(p)
图示如下：
http://liyangbit.com 257Chapter24. Bokeh：figure详细解读 Python数据之道
24.1.4 x_minor_ticks,y_minor_ticks
x_minor_ticks,y_minor_ticks默认值是“auto”，其他值可以是大于1的整数
示例如下：
1 p = figure(width=400, height = 400,
2 x_minor_ticks=2, y_minor_ticks=10)
3
4 p.circle([1,2,3,4],[5,6,7,8],size=20, color='red', alpha=0.5)
5
6 show(p)
图示如下：
http://liyangbit.com 258Chapter24. Bokeh：figure详细解读 Python数据之道
24.1.5 x_range,y_range
x_range,y_range可以是list或tuple表示范围的形式数值
示例如下：
1 p = figure(width=400, height = 400, x_range=[2,4], y_range=[5.5, 7.5])
2
3 p.circle([1,2,3,4],[5,6,7,8],size=20, color='red', alpha=0.5)
4
5 show(p)
或者
1 p = figure(width=400, height = 400, x_range=(2,4), y_range=(5.5, 7.5))
2
3 p.circle([1,2,3,4],[5,6,7,8],size=20, color='red', alpha=0.5)
4
5 show(p)
图示如下：
http://liyangbit.com 259Chapter24. Bokeh：figure详细解读 Python数据之道
24.1.6 x_axis_label,x_axis_location
x_axis_label,设置x轴的名称
x_axis_location,设置x轴的位置，有两个值，在上面显示（“above”）和在下面显示（“below”），默认值是
“below”
1 p = figure(width=400, height = 400,
2 x_axis_label='x label above', x_axis_location='above',
3 y_axis_label='y label')
4
5 # x_axis_location, default value: ”below”, values are [ above, below]
6
7 p.circle([1,2,3,4],[5,6,7,8],size=20, color='red', alpha=0.5)
8
9 show(p)
图示如下：
http://liyangbit.com 260Chapter24. Bokeh：figure详细解读 Python数据之道
24.1.7 y_axis_label,y_axis_location
y_axis_label,设置y轴的名称
y_axis_location,设置y轴的位置，有两个值，在左边显示（“left”）和在右边显示（“right”），默认值是“left”
1 p = figure(width=400, height = 400,
2 x_axis_label='x label', x_axis_location='below',
3 y_axis_label='y label right', y_axis_location='right')
4
5 # x_axis_location, default value: ”below”, values are [ above, below]
6 # y_axis_location, default value: ”left”, values are [ left, right]
7
8 p.circle([1,2,3,4],[5,6,7,8],size=20, color='red', alpha=0.5)
9
10 show(p)
图示如下：
http://liyangbit.com 261Chapter24. Bokeh：figure详细解读 Python数据之道
24.1.8 x_aixs_type，y_axis_type
（1）x_aixs_type:“datetime”
若是时间数据，可以将x轴或者y轴的数值设置为日期形式
1 import pandas as pd
2
3 rng = pd.date_range('2018-1-1', periods=7)
4
5 rng
out:
1 DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',
2 '2018-01-05', '2018-01-06', '2018-01-07'],
3 dtype='datetime64[ns]', freq='D')
1 x = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
2 # y = [10**(i**2) for i in x]
3 y = [i**2 for i in x]
4
5 p = figure(width=400, height = 400, title='datetime type example',
http://liyangbit.com 262Chapter24. Bokeh：figure详细解读 Python数据之道
6 x_axis_label='x label datetime', x_axis_location='below',
7 y_axis_label='y label', y_axis_location='left',
8 y_axis_type='linear', x_axis_type='datetime')
9 # y_axis_type, ”auto” 和 ”linear” 的 效 果 是 一 样 的
10
11 p.line(rng,y,color='red',line_width=3, alpha=0.9)
12
13 show(p)
图示如下：
（2）y_aixs_type:“log”
有时候，我们需要将x或y轴的数据设置成对数（log）形式，如下：
1 x = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
2 y = [10**(i**2) for i in x]
3
4 p = figure(width=400, height = 400, title='log type example',
5 x_axis_label='x label', x_axis_location='below',
6 y_axis_label='y label log', y_axis_location='left',
7 y_axis_type='log')
8
9 p.line(x,y,color='red',line_width=3, alpha=0.9)
http://liyangbit.com 263Chapter24. Bokeh：figure详细解读 Python数据之道
10
11 show(p)
图示如下：
（3）x_axis_type,y_aixs_type:“mercator”
1 k = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
2 x = [i*10000 for i in k]
3 y = [i*10000+30000 for i in k]
4
5 p = figure(width=400, height = 400, title='example',
6 x_axis_label='x label', x_axis_location='below',
7 y_axis_label='y label', y_axis_location='left' )
8
9 p.line(x,y,color='red',line_width=3, alpha=0.9)
10
11 show(p)
没有设置mercator的图示如下：
http://liyangbit.com 264Chapter24. Bokeh：figure详细解读 Python数据之道
1 k = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
2 x = [i*10000 for i in k]
3 y = [i*10000+30000 for i in k]
4
5 p = figure(width=400, height = 400, title='mercator type example',
6 x_axis_label='x label', x_axis_location='below',
7 y_axis_label='y label', y_axis_location='left',
8 x_axis_type='mercator', y_axis_type='mercator')
9
10 p.line(x,y,color='red',line_width=3, alpha=0.9)
11
12 show(p)
设置为mercator后的图示如下：
http://liyangbit.com 265Chapter24. Bokeh：figure详细解读 Python数据之道
从上图的结果来看，mercator好像代表的是数值10万，简写为1。
当然，我不知道我的这个理解是否准确。
关于mercator，我暂时没有看懂是什么用途，在网上查到了mercatorprojection。
各位有兴趣的可以探索下。
麦卡托投影法Mercatorprojection*https://en.wikipedia.org/wiki/Mercator_projection*https://
zh.wikipedia.org/wiki/%E9%BA%A5%E5%8D%A1%E6%89%98%E6%8A%95%E5%BD%B1%E6%B3%95
24.2 综合小结
最后，我们来小结下前面所述内容。
figure的部分参数：*plot_width,plot_weight或width,height，设置绘图区的长度和宽度*tools，设置绘图
区旁边所用的工具(tools)，其值可以是“crosshair,pan,wheel_zoom,box_zoom,reset,box_select,lasso_select,save,help”
*toolbar_location，设置工具栏显示的位置，参数值可以是：“below,above,left,right”，默认值“right”*
x_minor_ticks,y_minor_ticks默认值是“auto”，其他值可以是大于1的整数*x_range,y_range可以是list
或tuple表示范围的形式数值*x_axis_label,y_axis_label,设置x轴和y轴的名称*x_axis_location,default
value: “below”, values are [ above, below] * y_axis_location, default value: “left”, values are [ left,
http://liyangbit.com 266Chapter24. Bokeh：figure详细解读 Python数据之道
right]*x_aixs_type，y_axis_type，x和y轴数据的表现形式，其值可以是”linear,log,datetime,mercator“,
默认是”auto”
当然，还有一些参数没有提到，各位可以自行研究下。
http://liyangbit.com 26725 Bokeh: 29 种基础可视化图形
前面，我们分享了关于bokeh入门和figure使用的内容。今天，我们在前文的基础上，主要来分享29种基本的
图形绘制方法，很多复杂的图形，都是基于这些基础图形的组合，所以，这些基础图形是我们进阶路上的必由之路。
下面，我们一起来看看都有哪些基础图形吧。
本次运行环境为：*win7*jupyternotebook*python3.6*bokeh0.13.0
25.1 Bokeh中绘图的一般步骤
1. 加载bokeh库，声明在notebook或html文件中显示或输出绘制的图表
2. 绘制图表框架figure()
3. 在figure上绘制具体的图形，比如circle，line，bar等
4. 显示图片，show()
本文主要来介绍29种基础图形的绘制用法。
首先，加载bokeh库，以及准备基础数据
1 from bokeh.plotting import figure, output_notebook, show
2 from bokeh.layouts import gridplot
3 import numpy as np
4
5 output_notebook()
6
7 np.random.seed(15)
8
9 x=np.random.randint(1,20,size=6)
10 y=np.random.randint(20,50,size=6)
11
12 print(x)
13 print(y)
25.1.1 circle,circle_cross,circle_x,cross
首先，我们来看看一组关于圆形及其相关的图形
1 p1 = figure(title='circle')
2 p1.circle(x,y,size=20, color='#0071c1')
3
268Chapter25. Bokeh:29种基础可视化图形 Python数据之道
4 p2 = figure(title='circle_cross')
5 p2.circle_cross(x,y,size=20, color='#0071c1',fill_alpha=0.2, line_width=2)
6
7 p3 = figure(title='circle_x')
8 p3.circle_x(x,y,size=20, color='#0071c1',fill_alpha=0.2, line_width=2)
9
10 p4 = figure(title='cross')
11 p4.cross(x,y,size=20, color='#0071c1', line_width=2)
12
13 grid=gridplot([p1,p2,p3,p4],ncols=2, plot_width=300,plot_height=300)
14
15 show(grid)
图示如下：
25.1.2 diamond,diamond_cross,asterisk,x
这是一组跟钻石还有星型符号相关的图形绘制，如下：
http://liyangbit.com 269Chapter25. Bokeh:29种基础可视化图形 Python数据之道
1 p1 = figure(title='diamond')
2 p1.diamond(x,y,size=20, color='#0071c1')
3
4 p2 = figure(title='diamond_cross')
5 p2.diamond_cross(x,y,size=20, color='#0071c1',fill_alpha=0.2, line_width=2)
6
7 p3 = figure(title='asterisk')
8 p3.asterisk(x,y,size=20, color='#0071c1',fill_alpha=0.2, line_width=2)
9
10 p4 = figure(title='x')
11 p4.x(x,y,size=20, color='#0071c1', line_width=2)
12
13
14 grid=gridplot([p1,p2,p3,p4],ncols=2, plot_width=300,plot_height=300)
15
16 show(grid)
图示如下：
http://liyangbit.com 270Chapter25. Bokeh:29种基础可视化图形 Python数据之道
25.1.3 square,square_cross,square_x
这里，还有跟方块相关的一组图形
1 p1 = figure(title='square')
2 p1.square(x,y,size=20, color='#0071c1')
3
4 p2 = figure(title='square_cross')
5 p2.square_cross(x,y,size=20, color='#0071c1',fill_alpha=0.2, line_width=2)
6
7 p3 = figure(title='square_x')
8 p3.square_x(x,y,size=20, color='#0071c1',fill_alpha=0.2, line_width=2)
9
10
11 grid=gridplot([p1,p2,p3],ncols=2, plot_width=300,plot_height=300)
12
13 show(grid)
图示如下：
http://liyangbit.com 271Chapter25. Bokeh:29种基础可视化图形 Python数据之道
25.1.4 triangle,inverted_triangle
接下来，是三角形的绘制
1 p3 = figure(title='triangle')
2 p3.triangle(x,y,size=20, color='#0071c1', line_width=2)
3
4 p4 = figure(title='inverted_triangle')
5 p4.inverted_triangle(x,y,size=20, color='#0071c1', line_width=2)
6
7 grid=gridplot([p3,p4],ncols=2, plot_width=300,plot_height=300)
8
9 show(grid)
图示如下：
25.1.5 annulus,ellipse,wedge,oval
• annulus,绘制圆环
• wedge,楔形图
• ellipse,绘制椭圆
• oval，绘制椭圆
1 p1 = figure(title='annulus')
2 p1.annulus(x,y,color='#0071c1', inner_radius=0.8, outer_radius=1.2)
3
4 p2 = figure(title='wedge')
5 p2.wedge(x,y,color='#0071c1', radius=0.8, start_angle=0.5, end_angle=4.5,
direction='anticlock')
6
7 p3 = figure(title='ellipse')
8 p3.ellipse(x,y,color='#0071c1', width=2, height=3.6, angle=30)
http://liyangbit.com 272Chapter25. Bokeh:29种基础可视化图形 Python数据之道
9
10 p4 = figure(title='oval')
11 p4.oval(x,y,color='#0071c1', width=2, height=3.6, angle=30)
12
13
14 grid=gridplot([p1,p2,p3,p4],ncols=2, plot_width=300,plot_height=300)
15
16 show(grid)
图示如下：
从上图来看，同样的数据源，ellipse和oval绘制出来的椭圆形状，看起来是有些差异的。有兴趣的同学可以研究
下其中的原因。
25.1.6 hbar,vbar
绘制柱状图，分为水平柱状图和垂直柱状图，这个也是平时我们用的比较多的类型之一。
1 p2 = figure(title='hbar')
2 p2.hbar(y=y, height=0.3, left=0, right=x, color='#0071c1')
http://liyangbit.com 273Chapter25. Bokeh:29种基础可视化图形 Python数据之道
3
4 p3 = figure(title='vbar')
5 p3.vbar(x=x, width=0.3, bottom=0, top=y, color='#0071c1')
6
7 grid=gridplot([p2,p3],ncols=2, plot_width=300,plot_height=300)
8
9 show(grid)
图示如下：
25.1.7 line,multi_line
线状图或者称为折线图，单独只绘制线条时，可能视觉效果没有那么好看，有时候会配合圆形、方形等上面描述过
的图形一起使用。
1 p1 = figure(title='line')
2 p1.line(x,y,color='#0071c1', line_width=2)
3 p1.circle(x,y,size=10,color='#0071c1',fill_color='white')
4
5 from bokeh.plotting import figure, output_file, show
6
7 p2 = figure(title='multi_line')
8 p2.multi_line(xs=[[1, 2, 3], [2, 3, 4]], ys=[[6, 7, 2], [4, 5, 7]],
9 color=['red','green'])
10
11 grid=gridplot([p1,p2],ncols=2, plot_width=300,plot_height=300)
12
13 show(grid)
图示如下：
http://liyangbit.com 274Chapter25. Bokeh:29种基础可视化图形 Python数据之道
来看看关于multi_line的一个稍微复杂一些的例子，如下
1 from bokeh.palettes import Spectral11
2 import pandas as pd
3
4 df = pd.DataFrame(data=np.random.rand(5,3), columns = ('a', 'b' ,'c'),
5 index = pd.DatetimeIndex(start='01-01-2018',periods=5, freq='
d'))
6
7 df
1 numlines=len(df.columns)
2 mypalette=Spectral11[0:numlines]
3
4 p = figure(width=500, height=300, x_axis_type=”datetime”)
5 p.multi_line(xs=[df.index.values]*numlines,
6 ys=[df[name].values for name in df],
7 line_color=mypalette,
8 line_width=2)
9 show(p)
图示如下：
http://liyangbit.com 275Chapter25. Bokeh:29种基础可视化图形 Python数据之道
关于这个multi_line，总觉得在bokeh里过于复杂，在pandas里绘制类似的图形，相对来说比较容易。
25.1.8 patch,patches
patch/patches，中文翻译不确定如何解释为好，这里姑且称之为“块状图”吧。
• patch(x,y,**kwargs)
• patches(xs,ys,**kwargs)
1 p1 = figure(title='patch')
2 p1.patch(x,y,color='#0071c1')
3
4
5 p2 = figure(title='patches')
6 p2.patches(xs=[[1, 2, 3, 4], [2, 3, 4]], ys=[[6, 7, 2, 3], [4, 5, 7]],
7 color=['red','green'])
8
9 grid=gridplot([p1,p2],ncols=2, plot_width=300,plot_height=300)
10
11 show(grid)
图示如下：
http://liyangbit.com 276Chapter25. Bokeh:29种基础可视化图形 Python数据之道
25.1.9 quad,quadratic
• quad(left,right,top,bottom,**kwargs)，四方的
• quadratic(x0,y0,x1,y1,cx,cy,**kwargs)，二次方程的
1 p1 = figure(title='quad')
2 p1.quad(left=[1,3,5],right=[2,4,7], top=[3,7,8], bottom=[2,3,4], color='#0071c1
')
3
4
5 p2 = figure(title='quadratic')
6 p2.quadratic(x0=[1,3,5],x1=[2,4,7], y1=[3,7,8], y0=[2,3,4], cx=8 , cy=2, color=
'#0071c1')
7
8 grid=gridplot([p1,p2],ncols=2, plot_width=300,plot_height=300)
9
10 show(grid)
图示如下：
http://liyangbit.com 277Chapter25. Bokeh:29种基础可视化图形 Python数据之道
25.1.10 ray,rect,segment,step
• ray(x,y,length,angle,**kwargs)，射线
• rect(x,y,width,height,angle=0.0,dilate=False,**kwargs)，矩形
• segment(x0,y0,x1,y1,**kwargs)，分段，段落
1 p1 = figure(title='ray', x_range=[1,3.6])
2 p1.ray(x=[1, 2, 3], y=[1, 2, 3], length=45, angle=20, color=[”#0071c1”, 'black'
, 'red'],
3 line_width=[2,2,5])
4
5 p2 = figure(title='rect')
6 p2.rect(x=[1, 2, 3], y=[1, 8, 3], width=0.3, height=[5,10,20],angle=[0,10,0],
color=”#0071c1”)
7
8 p3= figure(title='segment')
9 p3.segment(x0=[1,3,5],x1=[2,4,7], y1=[3,7,8], y0=[2,3,4], color='#0071c1',
line_width=3)
10
11 p4 = figure(title='step')
12 p4.step(x, y, color=”#0071c1”,line_width=2)
13
14 grid=gridplot([p1,p2,p3,p4],ncols=2, plot_width=300,plot_height=300)
15
16 show(grid)
图示如下：
http://liyangbit.com 278Chapter25. Bokeh:29种基础可视化图形 Python数据之道
25.2 综合小结
这29种图形，是bokeh中比较基础的类型，其中一部分，我们会在后续分享中多次应用，希望对大家有所启发。
当然，还有一些图形没有提到，各位可以自行研究下。
http://liyangbit.com 27926 Bokeh 中独特的数据类型简介：ColumnDataSource
本文的环境为
• window7系统
• python3.6
• JupyterNotebook
• bokeh0.13.0
数据是进行数据可视化的必要基础，在bokeh中，数据有几种呈现方式。
（1）直接提供数据
（2）通过ColumnDataSource来提供数据
26.1 直接提供数据
首先加载相关Python库。
1 from bokeh.plotting import figure, output_notebook, show
2 from bokeh.layouts import gridplot
3 from bokeh.models import ColumnDataSource
4 import numpy as np
5 import pandas as pd
6
7 output_notebook()
可以通过数据列的形式（list）直接提供数据
1 np.random.seed(15)
2
3 x=np.random.randint(1,20,size=6)
4 y=np.random.randint(20,50,size=6)
5
6 print(x)
7 print(y)
8
9 p = figure(plot_width=300, plot_height=300)
10 p.circle(x, y,size=y)
11
12 show(p)
280Chapter26. Bokeh中独特的数据类型简介：ColumnDataSource Python数据之道
1 [ 9 13 6 1 8 12]
2 [41 42 35 49 37 33]
图示如下：
26.2 通过ColumnDataSource来提供数据
ColumnDataSource是Bokeh中一种重要的数据形式，ColumnDataSource()方法有一个参数为“data”，
“data”主要有以下三种类型：
（1）data为字典
（2）data为Pandas的DataFrame
（3）data为Pandas的DataFrame的groupby对象
26.2.1 data为字典
data的表现形式是一个字典的形式，一般情况下，字典的key值是一个字符串，代表列名称，而value则是list
形式或者numpy的array形式。演示如下：
1 data = {'x_values': [1, 2, 9, 4, 5],
2 'y_values': [6, 7, 2, 3, 6]}
3
4 source = ColumnDataSource(data=data)
5 source
http://liyangbit.com 281Chapter26. Bokeh中独特的数据类型简介：ColumnDataSource Python数据之道
1 type(source)
1 bokeh.models.sources.ColumnDataSource
从上面结果来看，source是一个ColumnDataSource对象，不能直接打印出来，后续可以在绘图是传入参数
进行使用。
1 data = {'x_values': [1, 2, 9, 4, 5],
2 'y_values': [6, 7, 2, 3, 6]}
3
4 source = ColumnDataSource(data=data)
5
6 p = figure(plot_width=300, plot_height=300)
7 p.circle(x='x_values', y='y_values', source=source, size=20)
8 show(p)
图示如下：
26.2.2 data为pandas的DataFrame
ColumnDataSource（简称为CDS）的data参数，也可以是pandas的DataFrame。当CDS的参数是
DataFrame时，参数中可以直接用DataFrame的列名称，索引名称，也可以直接用DataFrame已有的名称，如果
没有索引名称，默认情况下，索引名称用“index”就可以。
1 data = {'x_column': [1, 2, 9, 4, 5, 8],
2 'y_column': [6, 7, 2, 3, 6, 2]}
3
4 df = pd.DataFrame(data=data)
http://liyangbit.com 282Chapter26. Bokeh中独特的数据类型简介：ColumnDataSource Python数据之道
5 df
x_column y_column
0 1 6
1 2 7
2 9 2
3 4 3
4 5 6
5 8 2
1 source_df = ColumnDataSource(df)
2
3 p = figure(plot_width=300, plot_height=300)
4 p.circle(x='x_column', y='y_column', source=source_df, size=15)
5 show(p)
图示如下：
1 source_df = ColumnDataSource(df)
2
3 p = figure(plot_width=300, plot_height=300)
4
5 # 使 用 “index” 作 为 DataFrame 的 默 认 索 引 名 称
6 p.circle(x= 'index', y='y_column', source=source_df, size=15)
http://liyangbit.com 283Chapter26. Bokeh中独特的数据类型简介：ColumnDataSource Python数据之道
7 show(p)
图示如下：
26.2.3 data为pandas的DataFrame的groupby对象
ColumnDataSource（简称为CDS）的data参数，还也可以是pandas的DataFrame的groupby对象。
当 CDS 的参数是 DataFrame 的 groupby 对象时，在绘图时使用的列名为 groupby 对象的
groupby.describe()方法中的列名称。
由于groupby会有多个统计参数，在引用时，列表会合并到一起，形式如：column_mean等。
1 dates = pd.date_range('20180101', periods=360)
2
3 df = pd.DataFrame(np.random.randn(360,2), index=dates, columns=list('AB'))
4
5 df['C'] = ['Good', 'Bad', 'Common', 'Good','Good']*72
6 df['month'] = df.index.month
7
8 df
图示如下：
http://liyangbit.com 284Chapter26. Bokeh中独特的数据类型简介：ColumnDataSource Python数据之道
1 g = df.groupby('month')
2
3 g.describe()
图示如下：
1 source_g = ColumnDataSource(g)
2 p = figure(plot_width=400, plot_height=300)
3 p.vbar(x='month', width=0.3, bottom=0, top='A_mean',source=source_g)
4 show(p)
图示如下：
http://liyangbit.com 285Chapter26. Bokeh中独特的数据类型简介：ColumnDataSource Python数据之道
26.3 小结
相对于matplotlib，pandas，seaborn等Python会图库，Bokeh提供了特有的数据源，掌握好Column-
DataSource的应用，对于Bokeh绘图是至关重要的。后续，我们还会陆续接触到ColumnDataSource的相关用
法。
http://liyangbit.com 28627 Bokeh 中数据的添加、修改和筛选
本文的环境为
• window7系统
• python3.6
• JupyterNotebook
• bokeh0.13.0
数据是进行数据可视化的必要基础，前文介绍了bokeh中数据呈现的几种方式。
本文主要介绍在bokeh中对ColumnDataSource类型数据中进行：
（1）添加新的数据（2）修改更新已有数据（3）筛选数据。
首先加载相关Python库。
1 from bokeh.plotting import figure, output_notebook, show
2 from bokeh.layouts import gridplot
3 from bokeh.models import ColumnDataSource
4 import numpy as np
5 import pandas as pd
6
7 output_notebook()
27.1 添加新的数据
ColumnDataSource通过stream()方法来添加新的数据
1 data1 = {'x_values': [1, 2, 9, 4, 5],
2 'y_values': [6, 7, 2, 3, 6]}
3
4 source = ColumnDataSource(data=data1)
5
6 p1 = figure(plot_width=300, plot_height=300, title= 'origin data')
7 p1.circle(x='x_values', y='y_values', source=source, size=20)
8
9 show(p1)
图示如下：
287Chapter27. Bokeh中数据的添加、修改和筛选 Python数据之道
1 new_data = {'x_values': [6, 7, 2, 3, 6],
2 'y_values': [1, 2, 9, 4, 5]}
3
4 # 在 已 有 数 据 基 础 上 添 加 新 的 数 据 （append）
5 source.stream(new_data)
6
7 p2 = figure(plot_width=300, plot_height=300,title= 'append data with stream')
8 p2.circle(x='x_values', y='y_values', source=source, size=20)
9
10 show(p2)
图示如下：
http://liyangbit.com 288Chapter27. Bokeh中数据的添加、修改和筛选 Python数据之道
27.2 数据更新
用patch方法可以更新ColumnDataSource的数据。
1 data = {'x_column': [1, 2, 9, 4, 5, 8],
2 'y_column': [6, 7, 2, 3, 6, 2]}
3
4 df = pd.DataFrame(data=data)
5
6 df
如下：
1 source = ColumnDataSource(data=df)
2
3 p1 = figure(plot_width=300, plot_height=300, title= 'origin data')
4 p1.circle(x='x_column', y='y_column', source=source, size=20)
5
http://liyangbit.com 289Chapter27. Bokeh中数据的添加、修改和筛选 Python数据之道
6 show(p1)
图示如下：
27.2.1 更新单个数据
1 # 更 新 单 个 数 据
2 # {column:(index, new_value) }
3 source.patch({'x_column':[(0,15)]})
4
5 p2 = figure(plot_width=300, plot_height=300,title= 'revise single value with
patch')
6 p2.circle(x='x_column', y='y_column', source=source, size=20)
7
8 show(p2)
图示如下：
http://liyangbit.com 290Chapter27. Bokeh中数据的添加、修改和筛选 Python数据之道
27.2.2 更新多个数据
1 # 更 新 多 个 数 据
2 # {column:(slice, new_values) }
3
4 s = slice(2,4)
5 source.patch({'x_column':[(s,[20,15])]})
6
7 p2 = figure(plot_width=300, plot_height=300,title= 'revise multiple values with
patch')
8 p2.circle(x='x_column', y='y_column', source=source, size=20)
9
10 show(p2)
图示如下：
http://liyangbit.com 291Chapter27. Bokeh中数据的添加、修改和筛选 Python数据之道
27.3 筛选数据
在Bokeh中，ColumnDataSource（CDS）提供了CDSView来对数据进行筛选
其一般用法如下：
frombokeh.modelsimportColumnDataSource,CDSView
source=ColumnDataSource(some_data)
view=CDSView(source=source,filters=[filter1,filter2])
其中filters包括IndexFilter,BooleanFilter,GroupFilter等
27.3.1 Indexfilter
根据数据的索引来筛选数据
1 from bokeh.plotting import figure
2 from bokeh.models import ColumnDataSource, CDSView, IndexFilter
3 from bokeh.layouts import gridplot
4
5 data = {'x_column': [1, 2, 9, 4, 5, 8],
6 'y_column': [6, 7, 2, 3, 6, 2]}
7
8 df = pd.DataFrame(data=data)
9 source = ColumnDataSource(data=df)
10 view = CDSView(source=source, filters=[IndexFilter([0,2,4])])
http://liyangbit.com 292Chapter27. Bokeh中数据的添加、修改和筛选 Python数据之道
11
12 p1 = figure(plot_width=300, plot_height=300, title='origin state')
13 p1.circle(x='x_column', y='y_column', source=source, size=20)
14
15 p2 = figure(plot_width=300, plot_height=300, title='IndexFilter')
16 p2.circle(x='x_column', y='y_column', source=source, size=20, view=view)
17
18 grid=gridplot([p1,p2],ncols=2, plot_width=300,plot_height=300)
19
20 show(grid)
图示如下：
27.3.2 BooleanFilter
根据布尔值，True或False来筛选数据
1 from bokeh.models import BooleanFilter
2 from bokeh.layouts import row
3
4 booleans = [True if y_val>4 else False for y_val in source.data['y_column']]
5 view_booleans = CDSView(source=source, filters=[BooleanFilter(booleans)])
6
7 p1 = figure(plot_width=300, plot_height=300,title='origin state')
8 p1.circle(x='x_column', y='y_column', source=source, size=20)
9
10 p2 = figure(plot_width=300, plot_height=300, title='BooleanFilter')
11 p2.circle(x='x_column', y='y_column', source=source, size=20, view=
view_booleans)
12
13 grid=gridplot([p1,p2],ncols=2,plot_width=300,plot_height=300)
14
15 show(grid)
http://liyangbit.com 293Chapter27. Bokeh中数据的添加、修改和筛选 Python数据之道
图示如下：
27.3.3 GroupFilter
使用GroupFilter可以筛选出包含特定类型的所有行数据。GroupFilter有两个参数，即Column_name和
group，也就是列名和类别名称。
如下面的官方例子所述，如果想筛选iris数据中特定类别的花，可以使用GroupFilter方法。
1 from bokeh.plotting import figure, show
2 from bokeh.layouts import gridplot
3 from bokeh.models import ColumnDataSource, CDSView, GroupFilter
4
5 from bokeh.sampledata.iris import flowers
6
7 # output_file(”group_filter.html”)
8
9 source = ColumnDataSource(flowers)
10 view1 = CDSView(source=source, filters=[GroupFilter(column_name='species',
group='versicolor')])
11
12 plot_size_and_tools = {'plot_height': 300, 'plot_width': 300,
13 'tools':['box_select', 'reset', 'help']}
14
15 p1 = figure(title=”Full data set”, **plot_size_and_tools)
16 p1.circle(x='petal_length', y='petal_width', source=source, color='black')
17
18 p2 = figure(title=”Setosa only”, x_range=p1.x_range, y_range=p1.y_range, **
plot_size_and_tools)
19 p2.circle(x='petal_length', y='petal_width', source=source, view=view1, color='
red')
http://liyangbit.com 294Chapter27. Bokeh中数据的添加、修改和筛选 Python数据之道
20
21 show(gridplot([[p1, p2]]))
图示如下：
当然，还有一些其他的筛选方法，有兴趣的同学可以自己挖掘下~~
http://liyangbit.com 29528 Bokeh 中图形与组件的布局简介
28.1 图形的布局
图形（plot）的布局可以通过column()、row()和gridplot()方法来实现，其中：
1、column()方法是将所有图形(plots)在一列中分布；
2、row()方法是将所有图形(plots)在一行中分布；
3、gridplot()方法，可以按需求进行行列分布。
28.1.1 column
把所有图形放在一列中分布，其基本用法为column([plot_1,plot_2,……,plot_n])
1 from bokeh.io import output_notebook, show
2 from bokeh.layouts import column, row, gridplot
3 from bokeh.plotting import figure
4 import numpy as np
5
6 output_notebook()
准备基础数据和图形
1 np.random.seed(15)
2
3 x=np.random.randint(1,20,size=6)
4 y=np.random.randint(20,50,size=6)
5
6 p1 = figure(title='circle',plot_width=300,plot_height=300)
7 p1.circle(x,y,size=20, color='#0071c1')
8
9 p2 = figure(title='circle_cross',plot_width=300,plot_height=300)
10 p2.circle_cross(x,y,size=20, color='#0071c1',fill_alpha=0.2, line_width=2)
11
12 p3 = figure(title='circle_x',plot_width=300,plot_height=300)
13 p3.circle_x(x,y,size=20, color='#0071c1',fill_alpha=0.2, line_width=2)
14
15 p4 = figure(title='cross',plot_width=300,plot_height=300)
16 p4.cross(x,y,size=20, color='#0071c1', line_width=2)
将图形按列进行布局
296Chapter28. Bokeh中图形与组件的布局简介 Python数据之道
1 column_layout = column([p1, p2, p3])
2 show(column_layout)
如下图所示：
28.1.2 row
把所有图形按行分布，其基本用法为row([plot_1,plot_2,……,plot_n])
http://liyangbit.com 297Chapter28. Bokeh中图形与组件的布局简介 Python数据之道
1 row_layout = row(p1,p2,p3)
2 show(row_layout)
如下图所示：
28.1.3 gridplot
使用gridplot来进行个性化布局，gridplot的参数如下：
gridplot(\*args, \*\*kwargs)
Createagridofplotsrenderedonseparatecanvases. gridplotbuildsasingletoolbarforallthe
plotsinthegrid. gridplotisdesignedtolayoutasetofplots. Forgeneralgridlayout, usethe
layout()function.
Parameters:
• children(listoflistsofPlot)–Anarrayofplotstodisplayinagrid, givenasalistoflists
of Plot objects. To leave a position in the grid empty, pass None for that position in the
childrenlist.ORlistofPlotifcalledwithncols.ORaninstanceofGridSpec.
• sizing_mode(“fixed”,“stretch_both”,“scale_width”,“scale_height”,“scale_both”)–How
will the items in the layout resize to fill the available space. Default is “fixed”. For more
informationonthedifferentmodesseesizing_modedescriptiononLayoutDOM.
• toolbar_location(above,below,left,right)–Wherethetoolbarwillbelocated,withrespect
tothegrid.Defaultisabove.IfsettoNone,notoolbarwillbeattachedtothegrid.
• ncols(int,optional)–Specifythenumberofcolumnsyouwouldlikeinyourgrid.Youmust
onlypassanun-nestedlistofplots(asopposedtoalistoflistsofplots)whenusingncols.
• plot_width(int,optional)–Thewidthyouwouldlikeallyourplotstobe
• plot_height(int,optional)–Theheightyouwouldlikeallyourplotstobe.
• toolbar_options(dict,optional)–Adictionaryofoptionsthatwillbeusedtoconstructthe
grid’stoolbar(aninstanceofToolbarBox).Ifnoneissupplied,ToolbarBox’sdefaultswillbe
used.
• merge_tools(True,False)–Combinetoolsfromallchildplotsintoasingletoolbar.
可以在gridplot()方法中，以列表的形式将plots分组按行列的形式表示出来，如果要预留一个空置的位置，
http://liyangbit.com 298Chapter28. Bokeh中图形与组件的布局简介 Python数据之道
可以用“None”来表示。
1 grid1=gridplot([p1,p2],[p3,])
2
3 show(grid1)
如下图所示：
1 grid2=gridplot([p1,p2],[None,p3])
2
3 show(grid2)
如下图所示：
http://liyangbit.com 299Chapter28. Bokeh中图形与组件的布局简介 Python数据之道
在gridplot()方法中，还可以引入参数ncols来控制显示的列数，这里所有的plots放在一个列表中即可。
P.S.官方文档中，提到有“ncols”参数时，不能同时使用“None”，但我尝试了一下，是可以同时使用“None”
的。有兴趣的小伙伴也可以试试。
官方的原文如下：
YoucannotuseNonewiththencolsargument.ItmustonlybealistofPlotobjectsatonce.
1 grid3=gridplot([p1,p2,p4],ncols=2, plot_width=300,plot_height=300)
2
3 show(grid3)
如下图所示：
http://liyangbit.com 300Chapter28. Bokeh中图形与组件的布局简介 Python数据之道
1 grid4=gridplot([p1,p2,None,p4],ncols=2, plot_width=300,plot_height=300)
2
3 show(grid4)
如下图所示：
http://liyangbit.com 301Chapter28. Bokeh中图形与组件的布局简介 Python数据之道
28.2 组件的布局
bokeh中，组件（widgets）包括按钮（button），选项（Group），滑动块（slider）等等；组件的布局通过
widgerbox()方法来实现
1 from bokeh.layouts import widgetbox
2 from bokeh.models.widgets import Button, RadioButtonGroup, Select, Slider
3 from bokeh.models.widgets import Dropdown, Toggle
4
5 # 创 建 一 些 组 件
6 slider = Slider(start=0, end=20, value=1, step=0.5, title=”Slider”)
7 button_group = RadioButtonGroup(labels=[”Option 1”, ”Option 2”, ”Option 3”],
active=0)
8 select = Select(title=”Option:”, value=”Lemon”, options=[”Lemon”, ”Python”, ”
Java”, ”PHP”])
9 button_1 = Button(label=”Button 1”)
http://liyangbit.com 302Chapter28. Bokeh中图形与组件的布局简介 Python数据之道
10 button_2 = Button(label=”Button 2”)
11
12 menu = [(”Item 1”, ”item_1”), (”Item 2”, ”item_2”), None, (”Item 3”, ”item_3”)]
13 dropdown = Dropdown(label=”Dropdown button”, button_type=”warning”, menu=menu)
14
15 # put the results in a row
16 show(widgetbox(button_1, slider,
17 button_group, select,
18 button_2, dropdown, width=300))
如下图所示：
上图在文章中是gif的动态图，可以在下面文章中查看：
http://liyangbit.com/pythonvisualization/Bokeh-Laying-out-Plots-and-Widgets/
关于组件的具体内容介绍，我们会在后续进一步学习。
28.3 图形和组件混合布局
通过layout()方法，可以实现图形（plots）和组件（widgets）的混合布局。
1 from bokeh.layouts import layout
2
3 layout_01 =layout([slider],[p1,p2])
4
5 show(layout_01)
http://liyangbit.com 303Chapter28. Bokeh中图形与组件的布局简介 Python数据之道
如下图所示：
这里需要注意的是，slider和plot是放置在一起，但它们之间是没有内在联系的。
对比Python中常用的可视化库Matplotlib，在Bokeh中，对图形和组件进行布局还是比较方便的。布局的功
能，会在以后的实践中经常进行使用。
http://liyangbit.com 30429 Bokeh 可视化-项目实战经验分享
作者：WillKoehrsen
翻译：Lemonbit
译文出品：Python数据之道（ID：PyDataLab）
本文通过一个项目案例，详细的介绍了如何从Bokeh基础到构建Bokeh交互式应用程序的过程，内容循序渐进
且具有很高的实用性。本文共有两万字左右，属于纯干货分享，强烈推荐大家阅读后续内容。
本文由以下几个大的部分组成：
1. Bokeh基础介绍
2. 在Bokeh中添加主动交互功能
3. 在Bokeh中创建交互式可视化应用程序
可用于数据科学的资源正在迅速发展，这在可视化领域尤其明显，似乎每周都有另一种选择。随着所有这些进步，
有一个共同的趋势：增加交互性。人们喜欢在静态图中查看数据，但他们更喜欢的是使用数据来查看更改参数如何影响
结果。关于我的研究，一份报告告诉建筑物所有者他们可以通过改变他们的空调（AC）使用计划表节省多少电力是很好
的，但是给他们一个交互式图表更有效，他们可以选择不同的使用计划表，看看他们的选择如何影响用电量。最近，受
到互动图的趋势和不断学习新工具的渴望的启发，我一直在使用Bokeh，一个Python库。我为我的研究项目构建的
仪表板中显示了Bokeh交互功能的一个示例，如下：
Figure29.1:image1-能耗项目示例.gif
305Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
虽然我不能分享这个项目背后的代码，但我可以通过一个使用公开数据构建完全交互式Bokeh应用程序的例子。
本文将介绍使用Bokeh创建应用程序的整个过程。首先，我们将介绍Bokeh的基础内容，我们将使用nycflights13
数据集，该数据集记录了2013年超过300,000个航班。首先，我们将专注于可视化单个变量，在这种情况下，航班的
到达延迟时间为几分钟，我们将从构造基本直方图开始。
29.1 Bokeh基础
Bokeh的主要概念是图形一次构建一层。我们首先创建一个图形(figure)，然后在图形中添加称为图形符号
(glyphs)的元素。glyphs可以根据所需的用途呈现多种形状：圆形(circles)，线条(lines)，补丁(patches)，条
形(bars)，弧形(arcs)等。让我们通过制作带有正方形和圆形的基本图表来说明glyphs的概念。首先，我们使用
figure方法创建一个图，然后通过调用适当的方法并传入数据将我们的glyphs附加到figure中。最后，我们展示了
所做的图表。
1 # bokeh basics
2 from bokeh.plotting import figure
3 from bokeh.io import show, output_notebook
4
5 # Create a blank figure with labels
6 p = figure(plot_width = 600, plot_height = 600,
7 title = 'Example Glyphs',
8 x_axis_label = 'X', y_axis_label = 'Y')
9
10 # Example data
11 squares_x = [1, 3, 4, 5, 8]
12 squares_y = [8, 7, 3, 1, 10]
13 circles_x = [9, 12, 4, 3, 15]
14 circles_y = [8, 4, 11, 6, 10]
15
16 # Add squares glyph
17 p.square(squares_x, squares_y, size = 12, color = 'navy', alpha = 0.6)
18 # Add circle glyph
19 p.circle(circles_x, circles_y, size = 12, color = 'red')
20
21 # Set to output the plot in the notebook
22 output_notebook()
23 # Show the plot
24 show(p)
图示如下：
http://liyangbit.com 306Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
Figure29.2:image2-Bokeh基础图
现在让我们开始展示航班延误数据，在进入图表之前，应该加载数据并对其进行简要检查：
1 # Read the data from a csv into a dataframe
2 flights = pd.read_csv('../data/flights.csv', index_col=0)
3 # Summary stats for the column of interest
4 flights['arr_delay'].describe()
5
6 out[]:
7 count 327346.000000
8 mean 6.895377
9 std 44.633292
10 min -86.000000
11 25% -17.000000
12 50% -5.000000
13 75% 14.000000
14 max 1272.000000
上述统计数据提供了可以用来决策的信息：共有327,346次航班，最短延误时间为-86分钟（意味着航班提前86
分钟），最长延迟时间为1272分钟，惊人的21小时！75％的分位数仅在14分钟，因此我们可以假设超过1000分钟的
数字可能是异常值（这并不意味着它们是非法的，只是极端的）。下面将重点关注直方图的-60分钟到+120分钟之间的
http://liyangbit.com 307Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
延迟。
直方图是单个变量的初始可视化的常见选择，因为它显示了数据的分布。x位置是被称为区间（bins）的变量的值，
并且每个柱子的高度表示每个区间中的数据点的计数（数量）。在我们的例子中，x位置将代表以分钟为单位的到达延迟，
高度是相应bin中的航班数量。Bokeh没有内置的直方图，但是我们可以使用quad来制作我们自己的直方图。
为条形图（bars）创建数据，我们将使用Numpy的histogram函数来计算每个指定bin中的数据点数。我们
将使用5分钟长度的时间间隔（bins），这意味着该功能将计算每五分钟延迟间隔的航班数量。生成数据后，我们将其
放在Pandas的dataframe中，以将所有数据保存在一个对象中。
1 ”””Bins will be five minutes in width, so the number of bins
2 is (length of interval / 5). Limit delays to [-60, +120] minutes using the
range.”””
3 arr_hist, edges = np.histogram(flights['arr_delay'],
4 bins = int(180/5),
5 range = [-60, 120])
6 # Put the information in a dataframe
7 delays = pd.DataFrame({'arr_delay': arr_hist,
8 'left': edges[:-1],
9 'right': edges[1:]})
数据如下：
Figure29.3:image3-flights数据
flights列是从left到right的每个延迟间隔内的航班数量。从这里开始，我们可以创建一个新的Bokeh图
形，并添加一个指定适当参数的quad：
1 # Create the blank plot
2 p = figure(plot_height = 600, plot_width = 600,
3 title = 'Histogram of Arrival Delays',
4 x_axis_label = 'Delay (min)]',
5 y_axis_label = 'Number of Flights')
6
7 # Add a quad glyph
http://liyangbit.com 308Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
8 p.quad(bottom=0, top=delays['flights'],
9 left=delays['left'], right=delays['right'],
10 fill_color='red', line_color='black')
11
12 # Show the plot
13 show(p)
Figure29.4:image4-Bokeh绘制直方图
从上述图表来看，我们看到到达延迟几乎正态分布，右侧有轻微的正偏斜或重尾。
当然，其实有更简单的方法可以在Python中创建基本直方图，比如可以使用几行matplotlib代码完成相同的结
果。但是，我们想在Bokeh图中添加直方图并进行交互演示。
http://liyangbit.com 309Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
29.1.1 增加交互性
本文介绍的第一种交互方式是被动交互。这些是读者可以采取的不会改变所显示数据的动作。这些被称为检查员
（inspectors），因为它们允许读者更详细地“查看”数据。一个有用的检查器是当用户将鼠标悬停在数据点上时出现的
提示工具，在Bokeh中称为HoverTool。
Figure29.5:image5-HoverTool
为了添加提示工具(tooltips)，我们需要将数据源从dataframe更改为ColumnDataSource（CDS），这是
Bokeh中的一个关键概念。CDS是一个专门用于绘图的对象，包括数据以及多个方法和属性。CDS允许我们为图形添
加注释和交互性，并且可以从pandas的dataframe构建。实际数据本身保存在可通过CDS的data属性访问的字
典中。在这里，我们从dataframe创建源代码，并查看数据字典中与dataframe列对应的键。
1 # Import the ColumnDataSource class
2 from bokeh.models import ColumnDataSource
3 # Convert dataframe to column data source
4 src = ColumnDataSource(delays)
5 src.data.keys()
6
7 out:
8 dict_keys(['flights', 'left', 'right', 'index'])
当我们使用CDS添加glyphs时，我们传入CDS作为source参数并使用字符串引用列名：
1 # Add a quad glyph with source this time
2 p.quad(source = src, bottom=0, top='flights',
3 left='left', right='right',
4 fill_color='red', line_color='black')
注意代码如何通过单个字符串而不是之前的df ['column']格式引用特定数据列，例如’flights’，“left”和
http://liyangbit.com 310Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
“right”。
29.1.2 Bokeh中的HoverTool
HoverTool的语法起初可能看起来有些复杂，但通过练习它们很容易创建。我们将HoverTool实例作为Python
元组的“tooltips”列表传递，其中第一个元素是数据的标签，第二个元素引用我们想要突出显示的特定数据。我们可以
使用$引用图表的任一属性，例如x或y位置，或使用@引用我们数据源中的特定字段。这可能听起来有点令人困惑，
所以这里有一个HoverTool的例子：
1 # Hover tool referring to our own data field using @ and
2 # a position on the graph using $
3 h = HoverTool(tooltips = [('Delay Interval Left ', '@left'),
4 ('(x,y)', '($x, $y)')])
在这里，我们使用@引用ColumnDataSource中的left数据字段（对应于原始dataframe的“left”列），并
使用$引用光标的（x，y）位置。结果如下：
Figure29.6:image06-HoverTool
（x，y）位置是图表上鼠标的位置，对我们的直方图不是很有帮助，因为我们要找到给定条形中对应于条形顶部的航
班数量。为了解决这个问题，我们将改变我们的tooltip实例以引用正确的列。格式化提示工具中显示的数据可能令人
沮丧，因此我通常在dataframe中使用正确的格式创建另一列。例如，如果我希望我的提示工具显示给定栏的整个间
隔，我在dataframe中创建一个格式化的列：
1 # Add a column showing the extent of each interval
2 delays['f_interval'] = ['%d to %d minutes' % (left, right) for left, right in
zip(delays['left'], delays['right'])]
然后，我将此dataframe转换为ColumnDataSource并在我的HoverTool调用中访问此列。下面的代码使
用悬停工具创建绘图，引用两个格式化的列并将工具添加到绘图中：
1 # Create the blank plot
2 p = figure(plot_height = 600, plot_width = 600,
http://liyangbit.com 311Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
3 title = 'Histogram of Arrival Delays',
4 x_axis_label = 'Delay (min)]',
5 y_axis_label = 'Number of Flights')
6
7 # Add a quad glyph with source this time
8 p.quad(bottom=0, top='flights', left='left', right='right', source=src,
9 fill_color='red', line_color='black', fill_alpha = 0.75,
10 hover_fill_alpha = 1.0, hover_fill_color = 'navy')
11
12 # Add a hover tool referring to the formatted columns
13 hover = HoverTool(tooltips = [('Delay', '@f_interval'),
14 ('Num of Flights', '@f_flights')])
15
16 # Style the plot
17 p = style(p)
18
19 # Add the hover tool to the graph
20 p.add_tools(hover)
21
22 # Show the plot
23 show(p)
在Bokeh样式中，通过将元素添加到原始图形中来包含元素。注意在p.quad调用中，还有一些额外的参数，
hover_fill_alpha和hover_fill_color，当将鼠标悬停在条形图上时会改变glyph的外观。我还使用style
函数添加了样式。当使用样式时，我会保持简单并专注于标签的可读性。图的主要观点是显示数据，添加不必要的元素
只会减少图形的用处！最终的图形如下：
http://liyangbit.com 312Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
Figure29.7:image07-带HoverTool的直方图
当将鼠标悬停在不同的栏上时，会得到该栏的精确统计数据，显示该区间内的间隔和航班数。如果我们为图形感到
自豪，可以将其保存到html文件中进行分享：
1 # Import savings function
2 from bokeh.io import output_file
3 # Specify the output file and save
4 output_file('hist.html')
5 show(p)
上面这张图完成了工作，但它不是很吸引人！读者可以看到航班延误的分布接近正态分布（略有正偏斜），但他们
没有理由再花费更多的时间来分析该图。
如果想要创建更具吸引力的可视化图表，我们可以允许用户通过交互自己来探索数据。例如，在直方图中，一个有
价值的特征是能够选择特定航空公司进行比较，或者选择更改bins的宽度以更精细地检查数据。幸运的是，这些都是
可以使用Bokeh在现有绘图之上添加的功能。直方图的初始开发可能似乎涉及一个简单的绘图，但现在我们看到使用
像Bokeh这样强大的库的回报！
http://liyangbit.com 313Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
29.2 在Bokeh中添加主动交互
Bokeh中有两类交互：被动交互和主动交互。前面介绍的被动交互也称为检查器（inspectors），因为它们允许
用户更详细地查阅图表中的信息，但不会更改显示的信息。一个示例是当用户将鼠标悬停在数据点上时显示的提示信息，
如下：
Figure29.8:image10-被动交互
第二类交互称为主动交互，因为它会更改绘图上显示的实际数据。这可以是从选择数据子集（例如特定航空公司）
到改变多项式回归拟合自由度的任何事情。Bokeh中有多种类型的主动交互，但在这里我们将重点关注所谓的“小部件”
（“widgets”），可以点击的元素，并让用户控制图形的某些方面。
Figure29.9:image11-主动交互
当查看图表时，我喜欢使用主动交互，因为它们允许我自己探索数据。我发现从我自己的数据（来自设计师的某个
方向）而不是从完全静态的图表中发现数据的结论更具洞察力。此外，为用户提供一定的自由度使他们能够略微不同的
解释，从而产生有关数据集的有益讨论。
http://liyangbit.com 314Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
29.2.1 主动互动的实现方法
一旦我们开始添加主动交互，我们需要超越单行代码并进入封装特定操作的函数。对于Bokeh小部件（widgets）
交互，有三个主要功能要实现：
• make_dataset():按特定格式整理要显示的特定数据
• make_plot():使用指定的数据绘图
• update():根据用户选择更新绘图
29.2.1.1 整理数据
在制作绘图之前，需要设计将要显示的数据。对于交互式直方图，将为用户提供三个可控参数：
1. 航空公司（在代码中称为carriers）
2. 延迟的时间范围，比如：-60至+120分钟
3. 直方图的宽度（即bin大小），默认值为5分钟
对于为绘图创建数据集的函数，我们需要允许指定每个参数。为了告知我们如何在make_dataset函数中转换数
据，我们可以加载所有相关数据并进行检查。
Figure29.10:image12-加载数据
在此数据集中，每行是一个单独的航班。arr_delay列是以分钟为单位的航班到达延迟（负数表示航班早到）。从
前面的描述中我们知道有327,236个航班，最小延迟为-86分钟，最大延迟为+1272分钟。在make_dataset函数
中，我们希望根据dataframe中的name列选择航空公司，并通过arr_delay列限制航班数量。
为了生成直方图的数据，我们使用numpy中的histogram函数来计算每个bin中的数据点数。在示例中，这
是每个指定延迟间隔内的航班数量。在前面内容中，为所有航班制作了直方图，但现在我们将针对每个航空公司进行。
由于每个航空公司的航班数量差异很大，我们可以按比例显示延迟，而不是原始计数。也就是说，图上的高度表示的是，
在相应的bin区间，特定航空公司中该航班相对应于所有航班的延迟比例。为了从计数到比例，我们将计数除以该航空
公司的航班总数。
下面是制作数据集的完整代码，该函数接收我们想要包括的航空公司列表，要绘制的最小和最大延迟，以及以分钟
为单位的指定bin宽度。
http://liyangbit.com 315Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
1 def make_dataset(carrier_list, range_start = -60, range_end = 120, bin_width =
5):
2
3 # Check to make sure the start is less than the end!
4 assert range_start < range_end, ”Start must be less than end!”
5 by_carrier = pd.DataFrame(columns=['proportion', 'left', 'right',
6 'f_proportion', 'f_interval',
7 'name', 'color'])
8 range_extent = range_end - range_start
9 # Iterate through all the carriers
10 for i, carrier_name in enumerate(carrier_list):
11
12 # Subset to the carrier
13 subset = flights[flights['name'] == carrier_name]
14
15 # Create a histogram with specified bins and range
16 arr_hist, edges = np.histogram(subset['arr_delay'],
17 bins = int(range_extent / bin_width),
18 range = [range_start, range_end])
19
20 # Divide the counts by the total to get a proportion and create df
21 arr_df = pd.DataFrame({'proportion': arr_hist / np.sum(arr_hist),
22 'left': edges[:-1], 'right': edges[1:] })
23
24 # Format the proportion
25 arr_df['f_proportion'] = ['%0.5f' % proportion for proportion in arr_df
['proportion']]
26
27 # Format the interval
28 arr_df['f_interval'] = ['%d to %d minutes' % (left, right) for left,
29 right in zip(arr_df['left'], arr_df['right'])]
30
31 # Assign the carrier for labels
32 arr_df['name'] = carrier_name
33
34 # Color each carrier differently
35 arr_df['color'] = Category20_16[i]
36
37 # Add to the overall dataframe
38 by_carrier = by_carrier.append(arr_df)
39
40 # Overall dataframe
41 by_carrier = by_carrier.sort_values(['name', 'left'])
42 # Convert dataframe to column data source
43 return ColumnDataSource(by_carrier)
上述运行结果如下：
http://liyangbit.com 316Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
Figure29.11:image13-整理好的数据
提醒一下，我们使用Bokeh中quad函数来制作直方图，因此我们需要提供该图形符号的左、右和顶部（底部将
固定为0）参数。它们分别位于“left”，“right”和“proportion”列中。color列为每个显示的航空公司提供了唯一
的颜色，f_列为tooltips提供了格式化文本。
下一个要实现的功能是make_plot。该函数应该采用ColumnDataSource（Bokeh中用于绘图的特定类型的
对象）并返回绘图对象：
1 def make_plot(src):
2 # Blank plot with correct labels
3 p = figure(plot_width = 700, plot_height = 700,
4 title = 'Histogram of Arrival Delays by Carrier',
5 x_axis_label = 'Delay (min)', y_axis_label = 'Proportion')
6
7 # Quad glyphs to create a histogram
8 p.quad(source = src, bottom = 0, top = 'proportion', left = 'left',
right = 'right',
9 color = 'color', fill_alpha = 0.7, hover_fill_color = 'color',
legend = 'name',
10 hover_fill_alpha = 1.0, line_color = 'black')
11
12 # Hover tool with vline mode
13 hover = HoverTool(tooltips=[('Carrier', '@name'),
14 ('Delay', '@f_interval'),
15 ('Proportion', '@f_proportion')],
16 mode='vline')
17
18 p.add_tools(hover)
19
20 # Styling
21 p = style(p)
22
23 return p
如果我们导入所有航空公司的数据，绘制的图形如下：
http://liyangbit.com 317Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
Figure29.12:image14-所有航线的延迟图
这个直方图非常混乱，因为有16家航空公司在同一图表上绘制！如果想比较航空公司，由于信息重叠，这几乎是
不可能的。幸运的是，我们可以添加小部件（widgets）以使绘图更清晰并实现快速比较。
29.2.1.2 创建交互的小部件
一旦我们在Bokeh中创建基本图形，通过窗口小部件添加交互相对简单。我们想要的第一个小部件是一个选择
框，允许读者选择要显示的航空公司。该控件将是一个复选框，允许根据需要进行尽可能多的选择，并在Bokeh中称为
“CheckboxGroup”。为了制作选择工具，我们导入CheckboxGroup类并使用两个参数来创建一个实例：labels
是想要在每个框旁边显示的值和active：初始选择的值。以下是包括所有航空公司的CheckboxGroup的代码。
1 from bokeh.models.widgets import CheckboxGroup
2 # Create the checkbox selection element, available carriers is a
3 # list of all airlines in the data
4 carrier_selection = CheckboxGroup(labels=available_carriers,
5 active = [0, 1])
http://liyangbit.com 318Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
Figure29.13:image15-CheckboxGroup
Bokeh复选框中的标签必须是字符串，而活动值是整数。这意味着在图形中“AirTranAirwaysCorporation”
对应数字0，“AlaskaAirlinesInc.” 对应数值1。当想要将所选复选框与航空公司匹配时，需要确保查找与所选整数
活动值关联的字符串名称。我们可以使用小部件的.labels和.active属性来做到这一点：
1 # Select the airlines names from the selection values
2 [carrier_selection.labels[i] for i in carrier_selection.active]
3
4
5 out:
6 ['AirTran Airways Corporation', 'Alaska Airlines Inc.']
制作复选的小部件后，需要将选定的航空公司复选框链接到图表上显示的信息。这是使用CheckboxGroup的
.on_change方法和我们定义的update函数完成的。update函数总是有三个参数：attr，old，new并根据选择
控件更新绘图。我们更改图表上显示的数据的方法是改变我们传递给make_plot函数中的glyph(s)的数据源。这可
能听起来有点抽象，所以这里是有一个update函数的例子，它改变了直方图以显示所选的航空公司：
1 # Update function takes three default parameters
2 def update(attr, old, new):
3 # Get the list of carriers for the graph
http://liyangbit.com 319Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
4 carriers_to_plot = [carrier_selection.labels[i] for i in carrier_selection.
active]
5 # Make a new dataset based on the selected carriers and the
6 # make_dataset function defined earlier
7 new_src = make_dataset(carriers_to_plot,
8 range_start = -60,
9 range_end = 120,
10 bin_width = 5)
11 # Update the source used in the quad glpyhs
12 src.data.update(new_src.data)
在这里，我们将检查基于 CheckboxGroup 中所选航空公司显示的航空公司列表。此列表将传递给
make_dataset函数，该函数返回一个新的列数据源。我们通过调用src.data.update并从新数据源传入数据来更
新glyphs中使用的源的数据。最后，为了将carrier_selection小部件中的更改链接到update函数，我们必须
使用.on_change方法（称为事件处理程序）。
1 # Link a change in selected buttons to the update function
2 carrier_selection.on_change('active', update)
只要选择或取消选择不同的航空公司，就会调用更新功能。最终结果是在直方图上仅绘制了与所选航空公司相对应
的图形，如下所示：
Figure29.14:image16-交互图.gif
http://liyangbit.com 320Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
29.2.2 更多的交互式控制
现在我们知道了创建控件的基本工作流程，可以添加更多元素。每次，我们创建窗口小部件，编写更新函数以更改
绘图上显示的数据，并使用事件处理程序将更新功能链接到窗口小部件。我们甚至可以通过重写函数来从多个元素中使
用相同的更新函数，以从小部件中提取需要的值。为了练习，我们将添加两个额外的控件：一个Slider，用于选择直方
图的bin宽度;一个RangeSlider，用于设置要显示的最小和最大延迟。以下是制作这些小部件和新的update函数的
代码：
1 # Slider to select the binwidth, value is selected number
2 binwidth_select = Slider(start = 1, end = 30,
3 step = 1, value = 5,
4 title = 'Delay Width (min)')
5 # Update the plot when the value is changed
6 binwidth_select.on_change('value', update)
7
8 # RangeSlider to change the maximum and minimum values on histogram
9 range_select = RangeSlider(start = -60, end = 180, value = (-60, 120),
10 step = 5, title = 'Delay Range (min)')
11
12 # Update the plot when the value is changed
13 range_select.on_change('value', update)
14
15
16 # Update function that accounts for all 3 controls
17 def update(attr, old, new):
18
19 # Find the selected carriers
20 carriers_to_plot = [carrier_selection.labels[i] for i in carrier_selection.
active]
21
22 # Change binwidth to selected value
23 bin_width = binwidth_select.value
24
25 # Value for the range slider is a tuple (start, end)
26 range_start = range_select.value[0]
27 range_end = range_select.value[1]
28
29 # Create new ColumnDataSource
30 new_src = make_dataset(carriers_to_plot,
31 range_start = range_start,
32 range_end = range_end,
33 bin_width = bin_width)
34
35 # Update the data on the plot
36 src.data.update(new_src.data)
标准的slider和rangeslider如下所示：
http://liyangbit.com 321Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
Figure29.15:image17-滑动块
除了使用更新功能显示的数据之外，还可以更改绘图的其他方面。例如，要更改标题文本以匹配bin宽度，可以执
行以下操作：
1 # Change plot title to match selection
2 bin_width = binwidth_select.value
3 p.title.text = 'Delays with %d Minute Bin Width' % bin_width
在Bokeh中还有许多其他类型的交互，但是现在，我们的三个控件允许用户在图表上“玩”很多！
29.2.3 把它们放在一起
我们的互动图表的所有元素都已到位。我们有三个必要的函数：make_dataset，make_plot和update来根据
控件和小部件本身改变绘图。我们通过定义布局将所有这些元素连接到一个页面上。
1 from bokeh.layouts import column, row, WidgetBox
2 from bokeh.models import Panel
3 from bokeh.models.widgets import Tabs
4 # Put controls in a single element
5 controls = WidgetBox(carrier_selection, binwidth_select, range_select)
6
7 # Create a row layout
8 layout = row(controls, p)
9
10 # Make a tab with the layout
11 tab = Panel(child=layout, title = 'Delay Histogram')
12 tabs = Tabs(tabs=[tab])
我将整个布局放在一个选项卡上，当我们完成一个完整的应用程序时，我们可以将每个绘图放在一个单独的选项卡
上。所有这些工作的最终结果如下：
http://liyangbit.com 322Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
Figure29.16:image18-带选项卡的交互图.gif
29.3 在Bokeh中创建交互式可视化应用程序
接下来将重点介绍Bokeh应用程序的结构，而不是绘图细节，但后续会提供所有内容的完整代码。我们将继续使
用NYCFlights13数据集，这是2013年纽约3个机场的航班的真实航班信息集合。
要自己运行完整的应用程序，首先请确保安装了Bokeh（使用pip install bokeh）。
其次，请在公众号『Python数据之道』后台回复“code”，获取本项目的源代码地址，然后从该地址中下载
bokeh_app.zip文件夹，解压缩，打开目录中的命令窗口，然后键入bokeh serve --show bokeh_app。这将
设置一个本地Bokeh服务器并在浏览器中打开该应用程序。
29.3.1 最终的产品
在进入细节之前，让我们来看看我们的目标是什么，这样可以看到这些产品是如何组合在一起的。以下是一个简短
的剪辑，展示了我们如何与整个仪表板进行交互：
在这里，我在浏览器中使用Bokeh应用程序（在Chrome的全屏模式下），该应用程序在本地服务器上运行。在
顶部，我们看到许多选项卡，每个选项卡包含应用程序的不同部分。仪表板的初衷是，虽然每个选项卡可以独立存在，
但我们可以将它们中的许多连接在一起，以便能够完整地探索数据。该视频显示了我们可以使用Bokeh制作的图表范
围，从直方图和密度图，到我们可以按列排序的数据表，再到完全交互式地图。除了我们可以在Bokeh中创建的图形
范围之外，使用Bokeh库的另一个好处是交互。每个选项卡都有一个交互元素，使用户可以访问数据并进行自己的发
现。根据经验，在探索数据集时，人们喜欢自己探索，我们可以允许他们通过各种控制选择和筛选数据。
http://liyangbit.com 323Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
现在我们已经了解了我们的目标，让我们来看看如何创建一个Bokeh应用程序。强烈建议您自己下载代码来运行
（在公众号『Python数据之道』后台回复“code”，获取本项目的源代码地址）！
29.3.2 Bokeh应用程序的文件结构
在编写任何代码之前，为我们的应用程序建立一个框架很重要。在任何项目中，很容易被代码带走，很快就会丢失
在一堆半完成的脚本和不合适的数据文件中，因此我们希望事先为我们所有的代码和数据创建一个结构。该结构将帮助
我们跟踪应用程序中的所有元素，并在出现不可避免的错误时协助调试。此外，我们可以将此框架重新用于未来的项目，
因此我们在规划阶段的初始投资将获得回报。
要设置Bokeh应用程序，我创建一个父目录来保存名为bokeh_app的所有内容。在这个目录中，我们将有一个
数据子目录（称为data），我们脚本的子目录（scripts）和一个main.py脚本将所有内容整合到一起。通常，为了
管理所有代码，我发现最好将每个选项卡的代码保存在单独的Python脚本中，并从单个主脚本中调用它们。以下是我
用于Bokeh应用程序的文件结构，该文件结构改编自官方文档。
1 bokeh_app
2 |
3 +--- data
4 | +--- info.csv
5 | +--- info2.csv
6 |
7 +--- scripts
8 | +--- plot.py
9 | +--- plot2.py
10 |
11 +--- main.py
对于这次我们分析的航班程序项目，文件结构遵循一般大纲，如下：
http://liyangbit.com 324Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
Figure29.17:image20-航班程序项目结构
在一个bokeh_app目录下有三个主要部分：data，scripts和main.py。当运行服务器时，我们告诉Bokeh
服务于bokeh_app目录，它将自动搜索并运行main.py脚本。有了一般的结构，让我们来看看main.py，这就是我
喜欢称之为Bokeh应用程序的执行者！
29.3.3 主程序文件（main.py）
main.py脚本就像一个Bokeh应用程序的执行程序。它加载数据，将其传递给其他脚本，返回结果图，并将它们
组织到一个显示中。这将是我完整展示的唯一脚本，因为它对应用程序尤其重要。
1 # Pandas for data management
2 import pandas as pd
3
4 # os methods for manipulating paths
5 from os.path import dirname, join
6
7 # Bokeh basics
8 from bokeh.io import curdoc
9 from bokeh.models.widgets import Tabs
10
11
12 # Each tab is drawn by one script
13 from scripts.histogram import histogram_tab
14 from scripts.density import density_tab
http://liyangbit.com 325Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
15 from scripts.table import table_tab
16 from scripts.draw_map import map_tab
17 from scripts.routes import route_tab
18
19 # Using included state data from Bokeh for map
20 from bokeh.sampledata.us_states import data as states
21
22 # Read data into dataframes
23 flights = pd.read_csv(join(dirname(__file__), 'data', 'flights.csv'),
index_col=0).dropna()
24
25 # Formatted Flight Delay Data for map
26 map_data = pd.read_csv(join(dirname(__file__), 'data', 'flights_map.csv'),
27 header=[0,1], index_col=0)
28
29 # Create each of the tabs
30 tab1 = histogram_tab(flights)
31 tab2 = density_tab(flights)
32 tab3 = table_tab(flights)
33 tab4 = map_tab(map_data, states)
34 tab5 = route_tb(flights)
35
36 # Put all the tabs into one application
37 tabs = Tabs(tabs = [tab1, tab2, tab3, tab4, tab5])
38
39 # Put the tabs in the current document for display
40 curdoc().add_root(tabs)
我们从必要的导入开始，包括制作选项卡的函数，每个函数都存储在scripts目录中的单独脚本中。如果查看文
件结构，请注意scripts目录中有一个__init __.py文件。这是一个完全空白的文件，需要放在目录中，以便我们
使用相对语句导入相应的函数（例如from scripts.histogram import histogram_tab）。我不太确定为什么
需要它，但是它有效。
在Python库和脚本导入之后，我们在Python__file__属性的帮助下读取必要的数据。在这种情况下，我们
使用两个pandasdataframe（flights和map_data）以及Bokeh中包含的美国各州的数据。一旦读入数据，脚
本就会进行委托：它将适当的数据传递给每个函数，每个函数都绘制并返回一个选项卡，主脚本将所有这些选项卡组织
在一个名为tabs的布局中。作为每个单独的选项卡函数的功能示例，让我们看一下绘制map_tab的函数。
此函数包含map_data（航班数据的格式化版本）和美国各州的数据，并为选定的航空公司生成航班路线图：
http://liyangbit.com 326Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
Figure29.18:image21-航班图
1 def map_tab(map_data, states):
2 ...
3 def make_dataset(airline_list):
4 ...
5 return new_src
6 def make_plot(src):
7 ...
8 return p
9
10 def update(attr, old, new):
11 ...
12 new_src = make_dataset(airline_list)
13 src.data.update(new_src.data)
14
15 controls = ...
16 tab = Panel(child = layout, title = 'Flight Map')
17 return tab
我们看到熟悉的make_dataset，make_plot和update函数用于绘制带有交互式控件的图。一旦我们设置了
绘图，最后一行将整个绘图返回到主脚本。每个单独的脚本（5个选项卡中有5个）遵循相同的模式。
接下来返回主脚本，最后一步是收集选项卡并将它们添加到单个文档中。
1 # Put all the tabs into one application
2 tabs = Tabs(tabs = [tab1, tab2, tab3, tab4, tab5])
3 # Put the tabs in the current document for display
4 curdoc().add_root(tabs)
选项卡显示在应用程序的顶部，就像任何浏览器中的选项卡一样，我们可以轻松地在它们之间切换以探索数据。
http://liyangbit.com 327Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
Figure29.19:image22-带选项卡的交互图
29.3.4 运行Bokeh服务器
在制作绘图所需的所有设置和代码编写完成之后，在本地运行Bokeh服务器非常简单。我们打开一个命令行界
面（我更喜欢GitBash，但任何一个都可以工作），切换到包含bokeh_app的目录并运行bokeh serve --show
bokeh_app。假设一切都正确，应用程序将在我们的浏览器中自动打开地址http：// localhost：5006 /
bokeh_app。然后我们可以访问该应用程序并浏览我们的仪表板，效果如下：
http://liyangbit.com 328Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
Figure29.20:image23-程序运行后的动态图.gif
29.3.5 在JupyterNotebook中进行调试
如果出现问题（因为毫无疑问，我们最初几次编写仪表板），必须停止服务器，更改文件，然后重新启动服务器以查
看我们的更改是否具有所需效果，这可能会令人沮丧。为了快速迭代和解决问题，我通常在JupyterNotebook中开
发。JupyterNotebook是Bokeh开发的理想环境，因为您可以在notebook中创建和测试完全交互式的图形。语
法略有不同，但是一旦你有一个完整的绘图，代码只需要稍加修改，然后可以复制并粘贴到一个独立的.py脚本中。
要了解这一点，请查看用于开发应用程序的JupyterNotebook（请在公号『Python数据之道』后台回复
“code”，找到本项目的源代码地址，获取相应的JupyterNotebook代码文件）。
29.3.6 总结
完全交互式的Bokeh仪表板使任何数据科学项目都脱颖而出。通常情况下，我看到我的同事做了很多很棒的统计
工作，但却未能清楚地传达结果，这意味着所有工作都没有得到应有的认可。从个人经验来看，我也看到了Bokeh应
用程序在传达结果方面的有效性。虽然制作完整的仪表板需要做很多工作，但结果是值得的。此外，一旦我们有了一个
应用程序，可以将该框架重新用于其他项目。
从这个项目中，我们可以总结出几个关键点，以适用于许多类似的数据科学项目：
1. 在开始数据科学任务（Bokeh或其他任何东西）之前，拥有适当的框架/结构至关重要。这样，你就不会发现自
己迷失在试图查找错误的代码的泥潭中。此外，一旦我们开发出一个有效的框架，它可以用最少的努力重复使用。
2. 找到一个允许您快速迭代思路的调试工具至关重要。编写代码-查看结果-修复错误，这种循环在Jupyter
Notebook可以实现高效的开发（尤其是对于小规模项目）。
3. Bokeh中的交互式应用程序将提升您的项目并鼓励用户参与。仪表板可以是一个独立的探索项目，或突出您已
经完成的所有艰难的分析工作！
http://liyangbit.com 329Chapter29. Bokeh可视化-项目实战经验分享 Python数据之道
4. 估计你永远不知道在哪里可以找到你将在工作或辅助项目中使用的下一个工具。所以，不要害怕尝试新的软件和
技术！
以上是本文的全部内容，通过像Bokeh和plot.ly这样的Python库，制作交互式图表变得更加容易，并且能够
以引人注目的方式呈现数据科学成果。
本文的源代码，请在公号「Python数据之道」后台回复“code”来获取。
本文来源
作者：WillKoehrsen
• DataVisualizationwithBokehinPython,PartI:GettingStarted
• DataVisualizationwithBokehinPython,PartII:Interactions
• DataVisualizationwithBokehinPython,PartIII:MakingaCompleteDashboard
http://liyangbit.com 33030 Plotly Express 入门
PlotlyExpress之于Plotly.py类似Seaborn之于matplotlib：PlotlyExpress是一个高级封装库，允许您
快速创建图表，然后使用底层API和生态系统的强大功能进行修改。
PlotlyExpress是一个新的高级Python可视化库：它是Plotly.py的高级封装，它为复杂的图表提供了一个简
单的语法。受Seaborn和ggplot2的启发，它专门设计为具有简洁，一致且易于学习的API：只需一次导入，您就可
以在一个函数调用中创建丰富的交互式绘图，包括分面绘图（faceting）、地图、动画和趋势线。它带有数据集、颜色面
板和主题，就像Plotly.py一样。PlotlyExpress完全免费：凭借其宽松的开源MIT许可证，您可以随意使用它（是
的，甚至在商业产品中！）。最重要的是，PlotlyExpress与Plotly生态系统的其他部分完全兼容：在您的Dash应用
程序中使用它，使用Orca将您的数据导出为几乎任何文件格式，或使用JupyterLab图表编辑器在GUI中编辑它们！
用pip install plotly_express命令可以安装PlotlyExpress。
30.1 使用PlotlyExpress轻松地进行数据可视化
一旦导入PlotlyExpress（通常是px），大多数绘图只需要一个函数调用，接受一个整洁的Pandasdataframe，
并简单描述你想要制作的图。如果你想要一个基本的散点图，它只是px.scatter（data，x =“column_name”，y
=“column_name”）。
以下是内置的Gapminder数据集的示例，显示2007年按国家/地区的人均预期寿命和人均GDP之间的趋势：
1 import plotly_express as px
2
3 gapminder = px.data.gapminder()
4 gapminder2007 = gapminder.query('year == 2007')
5 px.scatter(gapminder2007, x='gdpPercap', y='lifeExp')
331Chapter30. PlotlyExpress入门 Python数据之道
Figure30.1:image01
如果你想通过大陆区分它们，你可以使用color参数为你的点着色，由px负责设置默认颜色，设置图例等：
Figure30.2:image02
这里的每一点都是一个国家，所以也许我们想要按国家人口来衡量这些点……没问题：这里也有一个参数来设置，
它被称为size：
http://liyangbit.com 332Chapter30. PlotlyExpress入门 Python数据之道
Figure30.3:image03
如果你好奇哪个国家对应哪个点？可以添加一个hover_name，您可以轻松识别任何一点：只需将鼠标放在您感
兴趣的点上即可！事实上，即使没有hover_name，整个图表也是互动的：
Figure30.4:image04
也可以通过facet_col =”continent“来轻松划分各大洲，就像着色点一样容易，并且让我们使用x轴对数
（log_x）以便在我们在图表中看的更清晰：
http://liyangbit.com 333Chapter30. PlotlyExpress入门 Python数据之道
Figure30.5:image06
也许你不仅仅对2007年感兴趣，而且你想看看这张图表是如何随着时间的推移而演变的。可以通过设置
animation_frame =“year”（以及animation_group =“country”来标识哪些圆与控制条中的年份匹配）来
设置动画。在这个最终版本中，让我们在这里调整一些显示，因为像“gdpPercap”这样的文本有点难看，即使它是我
们的数据框列的名称。我们可以提供更漂亮的“标签”（labels），可以在整个图表、图例、标题轴和悬停（hovers）中
应用。我们还可以手动设置边界，以便动画在整个过程中看起来更棒：
http://liyangbit.com 334Chapter30. PlotlyExpress入门 Python数据之道
Figure30.6:image07
因为这是地理数据，我们也可以将其表示为动画地图，因此这清楚地表明PlotlyExpress不仅仅可以绘制散点图
（不过这个数据集缺少前苏联的数据）。
Figure30.7:image08
事实上，PlotlyExpress支持三维散点图、三维线形图、极坐标和地图上三元坐标以及二维坐标。条形图（Bar）
http://liyangbit.com 335Chapter30. PlotlyExpress入门 Python数据之道
有二维笛卡尔和极坐标风格。进行可视化时，您可以使用单变量设置中的直方图（histograms）和箱形图（box）或小
提琴图（violinplots），或双变量分布的密度等高线图（densitycontours）。大多数二维笛卡尔图接受连续或分类数
据，并自动处理日期/时间数据。可以查看我们的图库(ref-3)来了解每个图表的例子。
Figure30.8:image09
上述动态图包含10多张图片的可视化，「Python数据之道」已将代码整合到jupyternotebook文件中，在公
号回复“code”即可获得源代码。
下图即是其中的一个图形：
Figure30.9:image
30.2 可视化分布
数据探索的主要部分是理解数据集中值的分布，以及这些分布如何相互关联。PlotlyExpress有许多功能来处理
这些任务。
使用直方图（histograms），箱形图（box）或小提琴图（violinplots）可视化单变量分布：
http://liyangbit.com 336Chapter30. PlotlyExpress入门 Python数据之道
直方图：
Figure30.10:image10
箱形图：
Figure30.11:image11
小提琴图：
http://liyangbit.com 337Chapter30. PlotlyExpress入门 Python数据之道
Figure30.12:image12
还可以创建联合分布图（marginalrugs），使用直方图，箱形图（box）或小提琴来显示双变量分布，也可以添加
趋势线。PlotlyExpress甚至可以帮助你在悬停框中添加线条公式和R²值！它使用statsmodels进行普通最小二乘
（OLS）回归或局部加权散点图平滑（LOWESS）。
Figure30.13:image13
http://liyangbit.com 338Chapter30. PlotlyExpress入门 Python数据之道
30.3 颜色面板和序列
在上面的一些图中你会注意到一些不错的色标。在PlotlyExpress中，px.colors模块包含许多有用的色标和序
列：定性的、序列型的、离散的、循环的以及所有您喜欢的开源包：ColorBrewer、cmocean和Carto。我们还提供
了一些功能来制作可浏览的样本供您欣赏（ref-3）：
定性的颜色序列：
Figure30.14:image14
众多内置顺序色标中的一部分：
http://liyangbit.com 339Chapter30. PlotlyExpress入门 Python数据之道
Figure30.15:image15
30.4 用一行Python代码进行交互式多维可视化
我们特别为我们的交互式多维图表感到自豪，例如散点图矩阵（SPLOMS）、平行坐标和我们称之为并行类别的并
行集。通过这些，您可以在单个图中可视化整个数据集以进行数据探索。在你的Jupyter笔记本中查看这些单行及其启
用的交互：
http://liyangbit.com 340Chapter30. PlotlyExpress入门 Python数据之道
Figure30.16:image16
散点图矩阵（SPLOM）允许您可视化多个链接的散点图：数据集中的每个变量与其他变量的关系。数据集中的每
一行都显示为每个图中的一个点。你可以进行缩放、平移或选择操作，你会发现所有图都链接在一起！
Figure30.17:image17
平行坐标允许您同时显示3个以上的连续变量。dataframe中的每一行都是一行。您可以拖动尺寸以重新排序它
们并选择值范围之间的交叉点。
http://liyangbit.com 341Chapter30. PlotlyExpress入门 Python数据之道
Figure30.18:image18
并行类别是并行坐标的分类模拟：使用它们可视化数据集中多组类别之间的关系。
30.5 Plotly生态系统的一部分
PlotlyExpress至于Plotly.py类似Seaborn之于matplotlib：PlotlyExpress是一个高级封装库，允许您
快速创建图表，然后使用底层API和生态系统的强大功能进行修改。对于Plotly生态系统，这意味着一旦您使用Plotly
Express创建了一个图形，您就可以使用Themes，使用FigureWidgets进行命令性编辑，使用Orca将其导出为几
乎任何文件格式，或者在我们的GUIJupyterLab图表编辑器中编辑它。
主题（Themes）允许您控制图形范围的设置，如边距、字体、背景颜色、刻度定位等。您可以使用模板参数应用
任何命名的主题或主题对象：
http://liyangbit.com 342Chapter30. PlotlyExpress入门 Python数据之道
Figure30.19:image19
有三个内置的Plotly主题可以使用，分别是plotly，plotly_white和plotly_dark
px输出继承自Plotly.py的Figure类ExpressFigure的对象，这意味着你可以使用任何Figure的访问
器和方法来改变px生成的绘图。例如，您可以将.update（）调用链接到px调用以更改图例设置并添加注释。
.update（）现在返回修改后的数字，所以你仍然可以在一个很长的Python语句中执行此操作：
http://liyangbit.com 343Chapter30. PlotlyExpress入门 Python数据之道
Figure30.20:image20
在这里，在使用PlotlyExpress生成原始图形之后，我们使用Plotly.py的API来更改一些图例设置并添加注释。
30.6 能够与Dash完美匹配
Dash是Plotly的开源框架，用于构建具有Plotly.py图表的分析应用程序和仪表板。PlotlyExpress产生
的对象与Dash100％兼容，只需将它们直接传递到dash_core_components.Graph，如下所示：dcc.Graph（
figure = px.scatter（...））。这是一个非常简单的50行Dash应用程序的示例，它使用px生成其中的图
表：
http://liyangbit.com 344Chapter30. PlotlyExpress入门 Python数据之道
Figure30.21:image21
这个50行的Dash应用程序使用PlotlyExpress生成用于浏览数据集的UI。
30.7 设计理念：为什么我们创建PlotlyExpress？
可视化数据有很多原因：有时您想要提供一些想法或结果，并且您希望对图表的每个方面施加很多控制，有时您希
望快速查看两个变量之间的关系。这是交互与探索的范畴。
Plotly.py已经发展成为一个非常强大的可视化交互工具：它可以让你控制图形的几乎每个方面，从图例的位置到
刻度的长度。不幸的是，这种控制的代价是冗长的：有时可能需要多行Python代码才能用Plotly.py生成图表。
我们使用PlotlyExpress的主要目标是使Plotly.py更容易用于探索和快速迭代。
我们想要构建一个库，它做出了不同的权衡：在可视化过程的早期牺牲一些控制措施来换取一个不那么详细的API，
允许你在一行Python代码中制作各种各样的图表。然而，正如我们上面所示，该控件并没有消失：你仍然可以使用底
层的Plotly.py的API来调整和优化用PlotlyExpress制作的图表。
支持这种简洁API的主要设计决策之一是所有PlotlyExpress的函数都接受“整洁”的dataframe作为输入。每
个PlotlyExpress函数都体现了dataframe中行与单个或分组标记的清晰映射，并具有图形启发的语法签名，可让
您直接映射这些标记的变量，如x或y位置、颜色、大小、facet-column甚至是动画帧到数据框（dataframe）中的
列。当您键入px.scatter（data，x ='col1'，y ='col2'）时，PlotlyExpress会为数据框中的每一行创建一
个小符号标记-这就是px.scatter的作用-并将“col1”映射到x位置（类似于y位置）。这种方法的强大之处在于它
以相同的方式处理所有可视化变量：您可以将数据框列映射到颜色，然后通过更改参数来改变您的想法并将其映射到大
小或进行行分面（facet-row）。
http://liyangbit.com 345Chapter30. PlotlyExpress入门 Python数据之道
接受整个整洁的dataframe的列名作为输入（而不是原始的numpy向量）也允许px为你节省大量的时间，因为
它知道列的名称，它可以生成所有的Plotly.py配置用于标记图例、轴、悬停框、构面甚至动画帧。但是，如上所述，如
果你的dataframe的列被笨拙地命名，你可以告诉px用每个函数的labels参数替换更好的。
仅接受整洁输入所带来的最终优势是它更直接地支持快速迭代：您整理一次数据集，从那里可以使用px创建数十
种不同类型的图表，包括在SPLOM中可视化多个维度、使用平行坐标、在地图上绘制，在二维、三维极坐标或三维坐
标中使用等，所有这些都不需要重塑您的数据！
我们没有以权宜之计的名义牺牲控制的所有方面，我们只关注您想要在数据可视化过程的探索阶段发挥的控制类型。
您可以对大多数函数使用category_orders参数来告诉px您的分类数据“好”、“更好”、“最佳”等具有重要的非字母
顺序，并且它将用于分类轴、分面绘制和图例的排序。您可以使用color_discrete_map（以及其他* _map参数）将
特定颜色固定到特定数据值（如果这对您的示例有意义）。当然，你可以在任何地方重构color_discrete_sequence
或color_continuous_scale（和其他* _sequence参数）。
在API级别，我们在px中投入了大量的工作，以确保所有参数都被命名，以便在键入时最大限度地发现：所有
scatter-类似的函数都以scatter开头（例如scatter_polar，scatter_ternary）所以你可以通过自动补全
来发现它们。我们选择拆分这些不同的散点图函数，因此每个散点图函数都会接受一组定制的关键字参数，特别是它们
的坐标系。也就是说，共享坐标系的函数集（例如scatter，line＆bar，或scatter_polar，line_polar和
bar_polar）也有相同的参数，以最大限度地方便学习。我们还花了很多精力来提出简短而富有表现力的名称，这些
名称很好地映射到底层的Plotly.py属性，以便于在工作流程中稍后调整到交互的图表中。
最后，PlotlyExpress作为一个新的Python可视化库，在Plotly生态系统下，将会迅速发展。所以不要犹豫，
立即开始使用PlotlyExpress吧！
「Python数据之道」已将代码整合到jupyternotebook文件中，在公号回复“code”即可获得源代码。
英文原文来源：
[https:// medium.com/ @plotlygraphs/ introducing-plotly-express-808df010143d](https://
medium.com/@plotlygraphs/introducing-plotly-express-808df010143d)
参考文献：
ref-1：https://nbviewer.jupyter.org/github/plotly/plotly_express/blob/master/walkthrough.
ipynb
ref-2：https://mybinder.org/v2/gh/plotly/plotly_express/master?filepath=walkthrough.ipynb
ref-3：https://plotly.github.io/plotly_express/
http://liyangbit.com 34631 在 Pycharm 等编辑器下使用 Plotly Express
在上面这篇文章里，展示了PlotlyExpress在交互式可视化方面的强大功能，但同时也遇到一些小问题，比如，
上述代码是在JupyterNotebook中运行的，有不少童鞋问到在Pycharm中如何运行？
刚开始遇到这个问题时，我基本回复的都是“建议参考Plotly来进行修改”。
但估计很多童鞋还是不清楚怎么运行，这里，我提供一种方法，大家可以在Pycharm、VScode等IDE中运行
代码，由于Pycharm、VSCode我用的相对较多，只测试了这两种。其他的IDE，个人觉得也是差不多的，大家可以
自己试试。
31.1 第一个示例
我们以iris数据集为例来演示，代码如下（.py文件）：
1 # Code based on Python 3.x
2 # _*_ coding: utf-8 _*_
3 # Author: ”Lemonbit”
4 # Website: http://liyangbit.com
5 # 公 众 号: Python数 据 之 道
6 # ID: PyDataLab
7
8 import plotly_express as px
9 import plotly
10 import plotly.graph_objs as go
11 plotly.offline.init_notebook_mode(connected=True)
12
13 iris = px.data.iris()
14
15 iris_plot = px.scatter(iris, x='sepal_width', y='sepal_length',
16 color='species', marginal_y='histogram',
17 marginal_x='box', trendline='ols')
18
19 plotly.offline.plot(iris_plot)
代码运行后，结果会出现在浏览器端，效果如下：
347Chapter31. 在Pycharm等编辑器下使用PlotlyExpress Python数据之道
Figure31.1:image-动态gif
31.2 第二个示例
以wind数据集为例来演示，代码如下（.py文件）：
1 # Code based on Python 3.x
2 # _*_ coding: utf-8 _*_
3 # Author: ”Lemonbit”
4 # Website: http://liyangbit.com
5 # 公 众 号: Python数 据 之 道
6 # ID: PyDataLab
7
8 import plotly_express as px
9 import plotly
10 plotly.offline.init_notebook_mode(connected=True)
11
12 wind = px.data.wind()
13 wind_plot = px.bar_polar(wind, r=”value”, theta=”direction”, color=”strength”,
template=”plotly_dark”,
14 color_discrete_sequence= px.colors.sequential.Plotly[-2::-1])
15
16 plotly.offline.plot(wind_plot)
代码运行后，结果会出现在浏览器端，图示如下：
http://liyangbit.com 348Chapter31. 在Pycharm等编辑器下使用PlotlyExpress Python数据之道
Figure31.2:image
看完上面的代码，是不是觉得soeasy，赶紧动手试试吧。
http://liyangbit.com 34932 Plotly Express 合并到 Plotly，安装过程有点小尴尬
前些时候，「Python数据之道」发布了关于PlotlyExpress的两篇文章，发现大家都很喜欢这个新的可视化神器。
2019年7月17日，Plotly官方宣布将Plotly更新到version4，并将plotly_express包含到其中，所以，今
后可以直接在Plotly中在运行PlotlyExpress了。
PlotlyExpressisnowpartofPlotly.pyversion4andsotheplotly_expressmodulenowjustre-
exportsthecontentsofplotly.express
今天，我们紧跟时代步伐，来畅览新的版本，let’sgo!!!
32.1 如何安装？
Method-1：直接更新Plotly
在终端输入代码如下：
1 pip install plotly --upgrade
Method-2:更新plotly_express
在终端输入代码如下：
1 pip install plotly_express==0.4.0
安装完成后，后续使用plotlyexpress就可以用下面的方式来运行
1 # 在 plotly v4.0 版 本 中
2 import plotly.express as px
3
4 # 在 plotly_express v0.1.1 版 本 中
5 # import plotly_express as px
更多说明可以在PlotlyExpress的github项目仓库中来查找
https://github.com/plotly/plotly_express
32.2 小小插曲
然而，在安装过程中，发现了一些小小的尴尬。
350Chapter32. PlotlyExpress合并到Plotly，安装过程有点小尴尬 Python数据之道
在执行以下命令的时候：
1 pip install plotly --upgrade
会提示cufflinks的安装环境与Plotly的更新有冲突，图示如下：
Figure32.1:img01
cufflinks（version0.16）安装时要求plotly的版本小于4.0版本，所以当更新plotly到v4版本的时候，
cufflinksv0.16就不能运行了。
为了让cufflinks能顺利运行，我们更新安装这个库试试
1 pip install cufflinks --upgrade
过程图示如下：
http://liyangbit.com 351Chapter32. PlotlyExpress合并到Plotly，安装过程有点小尴尬 Python数据之道
Figure32.2:img02
Figure32.3:img03
在更新完成后，会提示Plotly版本由v4.0降级到了v3.1版本。
http://liyangbit.com 352Chapter32. PlotlyExpress合并到Plotly，安装过程有点小尴尬 Python数据之道
在这种情况下，如果还想运行PlotlyExprss，需要安装以前的单独的版本，在终端执行以下代码
1 pip install plotly_express==0.1.1
如果想在不同的环境中使用plotly_express的最新版本（即Plotlyv4）或者使用cufflinks，可以考虑安装两
个虚拟环境，分别安装plotlyv4版本和cufflinks，来暂时解决这个问题。
当然，考虑到plotly4.0刚刚发布没多久（7月17日），cufflinks后续应该会修改这个安装的要求，以适应
plotly的最新版。
32.3 最后
如果你还不知道cufflinks是什么，建议先自行了解下，跟plotlyexpress一样，cufflinks也是一个神器哦。
Cufflinks是基于Plotly来运行的，后续会介绍下，这里先上一张用cufflinks绘制的图，大家可以先过过眼瘾。
Figure32.4:img04
http://liyangbit.com 35333 推荐一个牛逼的生物信息 Python 库 - Dash Bio
DashBio是一个免费的开源Python库，用于生物信息学和药物开发应用。
要开始使用 Dash Bio，请使用 pip install dash_bio 安装，然后转到 Dash Bio 的文档: http://
dash.plot.ly/dash-bio
如果您是Dash的新手，您可能需要从Dash的入门指南开始：https://dash.plot.ly/getting-started
自发布以来，Plotly用于Python的开源应用程序构建软件Dash在生物信息学家和药物开发人员中一直在稳步
上升。交互式Web图形、Python创作和易用性的结合使Dash成为生物信息学家工具箱的自然之选。
例如，麻省理工大学的吉福德实验室（GiffordLab）已经发布了一个完全用Dash编写的CRISPR预测工具。麻
省理工大学的团队在Nature中详细介绍了他们的方法，并免费提供在线的Dash应用程序和Python代码。
354Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
Figure33.1:image01
图片来源：theinDelphiCRISPRpredictionapp
同样，南卡罗来纳医科大学的Hammer实验室正在开发一种带有Dash的开源细胞显微镜工具，并在bioRxiv
上发表了他们的方法：
http://liyangbit.com 355Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
Figure33.2:image02
去年夏天，Plotly与加拿大研究机构CanadianResearchChairAïdaOuangraou合作，利用Dash开发新
的基因组数据可视化开源工具。该研究尚未发布，但您可以在该机构网站上查看新闻。
今年，Plotly正在利用DashBio重建其对生命科学的承诺-DashBio是一个用于在Python中构建生物信
息学和药物开发应用程序的开源工具包。
许多DashBio组件都建立在JavaScript库之上，这些库已经在全栈、生物信息学应用程序开发人员中流行。我
们重新设计了这些JavaScript小部件，以便Python开发人员现在可以访问它们。换句话说，科学计算的Python开
发人员现在可以将这些小部件工作，而无需了解JavaScript。所有你需要知道的是Python和Dash。
生命科学软件的许多重量级工具都是基于Java的桌面程序-PyMol用于生物制剂开发，ImageJ用于显微镜图像
分析，IGV用于基因组数据查看是一些例子。使用Dash可以为Web编写未来版本的应用程序，完全使用Python，
在移动设备上工作，并根据特定的研究目标进行定制。由于Dash是开源的，因此整个软件应用程序的代码可以以同行
评审的方式自由分发和发布。CRISPR、NGS和生物制剂在生命科学领域引发了新的创新和商业化浪潮–Dash是基于
Python的分析库，可以跟上。
以下是12个Dash应用程序，这些应用程序显示了这种基于Web、敏捷和交互式的生物信息学和药物开发分析方
法。有了DashBio，我们希望尽可能扩大可能性。
33.1 探索3d状态下的小分子
这个Dash应用程序从磁盘、数据库或Python中的API读取PDB（“蛋白质数据库”）文件，然后在Dash中可
视化3d结构。当您单击原子，旋转分子或更改结构时，Dash会触发Python回调函数。您还可以突出显示单个原子
http://liyangbit.com 356Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
（如蛋白质的活性位点）。
• 查看Dashapp：https://dash-bio.plotly.host/dash-molecule3d
• 查看 Dash app 的 Python 代码：https://github.com/plotly/dash-bio/blob/master/tests/dash-
bio_demos/app_molecule3d.py
Figure33.3:image03
用Dash3d分子组分可视化的DNA螺旋
33.2 分析显微镜图像中的细胞
这个Dash应用程序是由“scikit-image”背后的主要开发人员之一EmmaGouillart制作的。将鼠标悬停在图
像中的白细胞上以突出显示相邻表格中的细胞属性。您还可以使用该表来过滤具有特定属性的单元格（例如，面积小于
1500μm²的单元格）。你需要安装Dash Canvashttps://dash.plot.ly/canvas来运行这个应用程序。
• Dash程序地址：https://dash-canvas.plotly.host/object-properties/
• Python 代 码 地 址：https:// github.com/ plotly/ canvas-portal/ blob/ master/ apps/ object-
properties/app.py
• DashCanvas程序地址：https://dash-canvas.plotly.host/Portal/
http://liyangbit.com 357Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
Figure33.4:image04
光学显微镜下的单核细胞-用DashCanvas进行交互式探索。
33.3 进行药代动力学分析
此Dash应用程序旨在允许进行药代动力学研究的人员输入数据（手动或通过复制粘贴）。然后显示浓度-时间曲
线以及用Python计算的各种参数表。
• 程序地址：https://dash-bio.plotly.host/dash-pk-calc/
• Python 代码地址：https://github.com/plotly/dash-sample-apps/blob/master/apps/dash-pk-
calc/app.py
http://liyangbit.com 358Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
Figure33.5:image05
使用自定义的Dash应用程序简化了pk分析
33.4 可视化FASTA数据
有许多非常好的、交互式的多序列比对（MSA）工具。我们从这3个中获取灵感：
• MATLAB 的 seqalignviewer https:// www.mathworks.com/ help/ bioinfo/ ref/ seqalign-
viewer.html
• SebWilzbach’sJavaScriptMSA：https://msa.biojs.net/
• Jalview：http://www.jalview.org/
对于DashMSAViewer，我们使用WebGL在浏览器中实现超快的交互式性能。由于DashMSAViewer是
Dash组件，因此您只需要知道Python即可使用它。下面的Dash应用程序从Python中读取FASTA文件中的序列
数据，然后使用DashMSA查看器绘制数据。
• Dash应用程序：https://dash-bio.plotly.host/dash-alignment-viewer/
• Python 代 码：https:// github.com/ plotly/ dash-bio/ blob/ master/ tests/ dashbio_demos/
app_alignment_viewer.py
http://liyangbit.com 359Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
Figure33.6:image06
使用DashMSAViewer以交互方式可视化FASTA数据
33.5 突出基因组相似性
Circos图通常用于比较基因组学。在下面的Dash应用程序中，21个染色体以圆圈绘制，并且它们的基因组区域之间
的关系与线或带相关联。根据circos.ca[http://circos.ca/guide/genomic/img/circos-conservation.png]，
可以定义这些关系：
[基因组]位置之间的关系可以反映任何类型的对应关系。例如，它可以根据相似性（序列或蛋白质）或类别（功能
或结构）来定义
由于DashCircos是一个Dash组件，它会在Web浏览器中显示，而您只需要知道Python即可使用它构建应
用程序。
• Dash程序地址：https://dash-bio.plotly.host/dash-circos/
• Python 代 码：https:// github.com/ plotly/ dash-bio/ blob/ master/ tests/ dashbio_demos/
app_circos.py
http://liyangbit.com 360Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
Figure33.7:image07
使用DashCircos突出基因组相似性
33.6 可视化微阵列结果
集群图是具有树形图的热图，其可视化分层数据聚类。它们通常与微阵列数据一起使用。DashClustergram响
应单击、悬停和缩放事件。使用DashClustergram创建应用程序，你只需要了解Python即可。
• Dash程序：https://dash-bio.plotly.host/dash-clustergram/
• Python 代 码：https:// github.com/ plotly/ dash-bio/ blob/ master/ tests/ dashbio_demos/
app_clustergram.py
http://liyangbit.com 361Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
Figure33.8:image08
使用DashClustergram可视化微阵列结果
33.7 搜索和选择序列
Dash序列查看器简化了序列搜索和选择的UI。该组件的核心是瑞士生物信息学研究所最初开发的JavaScript
库。我们为Dash重新设计了它，以便它可以在Python中轻松使用。
• Dash应用程序：https://dash-bio.plotly.host/dash-sequence-viewer/
• Python 代 码：https:// github.com/ plotly/ dash-bio/ blob/ master/ tests/ dashbio_demos/
app_sequence_viewer.py
http://liyangbit.com 362Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
Figure33.9:image09
使用DashSequenceViewer搜索和选择生物序列
33.8 3d分子的环境遮挡
这个3d分子查看器使用WebGL和环境遮挡来提供更好的深度感知。最初的JavaScript库-Speck-由Rye
Terrell开发。我们为Dash重新设计了这个库，使构建分析应用程序的Python用户可以访问它。
• 应用程序：https://dash-bio.plotly.host/dash-speck/
• 代码：https://github.com/plotly/dash-bio/blob/master/tests/dashbio_demos/app_speck.py
http://liyangbit.com 363Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
Figure33.10:image10
DashSpeck显示的比较优美的交互式3D分子
33.9 使用DashIdeogram可视化染色体
DashIdeogram的核心是BroadInstitute开发的JavaScript库。我们将它重新设计为Dash组件，以便科
学计算的Python开发人员可以轻松访问它。
• 应用程序：https://dash-bio.plotly.host/dash-ideogram/
• 代码：https://github.com/plotly/dash-bio/blob/master/tests/dashbio_demos/app_ideogram.py
http://liyangbit.com 364Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
Figure33.11:image11
在Dash应用程序中嵌入交互式染色体可视化
33.10 可视化基因突变
交互式针线图现在可以在Python中轻松组合并嵌入到Dash应用程序中。这个Dash组件的设计灵感来自
BarcelonaBiomedicalGenomicsLab的JavaScript库“muts-needle-plot”。
• 应用程序：https://dash-bio.plotly.host/dash-needle-plot/
• Python 代 码：https:// github.com/ plotly/ dash-bio/ blob/ master/ tests/ dashbio_demos/
app_needle_plot.py
http://liyangbit.com 365Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
Figure33.12:image12
33.11 测量和注释医学图像
成像和图像分析是生命科学研究的基础。DashCanvas允许您以交互方式注释医学图像，并根据用户与图像的交
互运行Python程序。此Dash应用程序显示如何使用DashCanvas和Python计算X射线图像上的距离。
• 应用程序：https://dash-canvas.plotly.host/measure-length/
• Python 代码：https://github.com/plotly/canvas-portal/blob/master/apps/measure-length/
app.py
• DashCanvas：https://dash-canvas.plotly.host/Portal/
http://liyangbit.com 366Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
Figure33.13:image13
使用DashCanvas在医学图像上注释，保存和运行Python程序。
33.12 构建系统发育树和网络图
BioJS上最受欢迎的JavaScript库是Cytoscape，一个高性能网络图库。去年，Plotly与Cytoscape作者密
切合作，为Dash和Python用户提供了这个库。与本文中的所有Dash组件一样，DashCytoscape是免费的开源
软件。
• Dash Cytoscape 发 布：https:// medium.com/ @plotlygraphs/ introducing-dash-cytoscape-
ce96cac824e4
• DashCytoscape文档：https://dash.plot.ly/cytoscape
http://liyangbit.com 367Chapter33. 推荐一个牛逼的生物信息Python库-DashBio Python数据之道
Figure33.14:image14
使用DashCytoscape构建交互式网络图和系统发育树
33.13 Final
最后，如果你对上述内容有兴趣，可以去访问下述Github的内容:
• (cid:0)https://github.com/plotly/dash-bio
• (cid:0)https://github.com/plotly/dash-canvas
翻译:Lemonbit
译文出品:Python数据之道(PyDataLab)
来源：Plotly
http://liyangbit.com 36834 项目实战：智联求职系列
34.1 第一篇：数据采集并保存到MongoDB数据库
34.1.1 前言
本次主题分两篇文章来介绍：
• 一、数据采集
• 二、数据分析
第一篇先来介绍数据采集，即用python爬取网站数据。
34.1.2 运行环境和python库
先说下运行环境：
• python3.5
• windows7，64位系统
本次智联招聘的网站爬取，主要涉及以下一些python库：
• requests
• BeautifulSoup
• multiprocessing
• pymongo
• itertools
34.1.3 爬取的主要步骤
• 根据关键字、城市、以及页面编号生成需要爬取的网页链接
• 用requests获取相应的网页内容
• 用BeautifulSoup解析，获取需要的关键信息
• 将爬取的信息存入MongoDB数据库中，插入新记录或更新已有记录
• 用multiprocessing启动多进程进行爬取，提高运行效率
34.1.4 文件组成
• 信息配置文件“zhilian_kw_config.py”
369Chapter34. 项目实战：智联求职系列 Python数据之道
• 爬虫主运行文件“zhilian_kw_spider.py”
在配置文件中设置需要爬取的信息，然后运行主程序进行内容抓取。
配置文件“zhilian_kw_config.py”的内容如下：
1 # Code based on Python 3.x
2 # _*_ coding: utf-8 _*_
3 # __Author: ”LEMON”
4
5 TOTAL_PAGE_NUMBER = 90 # PAGE_NUMBER: total number of pages， 可 进 行 修 改
6
7 KEYWORDS = ['大 数 据', 'python', '投 资 经 理'] # 需 爬 取 的 关 键 字 可 以 自 己 添 加 或 修 改
8
9 # 爬 取 主 要 城 市 的 记 录
10 ADDRESS = ['全 国', '北 京', '上 海', '广 州', '深 圳',
11 '天 津', '武 汉', '西 安', '成 都', '大 连',
12 '长 春', '沈 阳', '南 京', '济 南', '青 岛',
13 '杭 州', '苏 州', '无 锡', '宁 波', '重 庆',
14 '郑 州', '长 沙', '福 州', '厦 门', '哈 尔 滨',
15 '石 家 庄', '合 肥', '惠 州', '太 原', '昆 明',
16 '烟 台', '佛 山', '南 昌', '贵 阳', '南 宁']
17
18 MONGO_URI = 'localhost'
19 MONGO_DB = 'zhilian'
爬虫主运行文件“zhilian_kw_spider.py”的内容如下：
1 # Code based on Python 3.x
2 # _*_ coding: utf-8 _*_
3 # __Author: ”LEMON”
4
5 from datetime import datetime
6 from urllib.parse import urlencode
7 from multiprocessing import Pool
8 import requests
9 from bs4 import BeautifulSoup
10 import pymongo
11 from zhilian.zhilian_kw_config import *
12 import time
13 from itertools import product
14
15 client = pymongo.MongoClient(MONGO_URI)
16 db = client[MONGO_DB]
17
18
19 def download(url):
20 headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0)
Gecko/20100101 Firefox/51.0'}
21 response = requests.get(url, headers=headers)
22 return response.text
23
http://liyangbit.com 370Chapter34. 项目实战：智联求职系列 Python数据之道
24
25 def get_content(html):
26 # 记 录 保 存 日 期
27 date = datetime.now().date()
28 date = datetime.strftime(date, '%Y-%m-%d') # 转 变 成str
29
30 soup = BeautifulSoup(html, 'lxml')
31 body = soup.body
32 data_main = body.find('div', {'class': 'newlist_list_content'})
33
34 if data_main:
35 tables = data_main.find_all('table')
36
37 for i, table_info in enumerate(tables):
38 if i == 0:
39 continue
40 tds = table_info.find('tr').find_all('td')
41 zwmc = tds[0].find('a').get_text() # 职 位 名 称
42 zw_link = tds[0].find('a').get('href') # 职 位 链 接
43 fkl = tds[1].find('span').get_text() # 反 馈 率
44 gsmc = tds[2].find('a').get_text() # 公 司 名 称
45 zwyx = tds[3].get_text() # 职 位 月 薪
46 gzdd = tds[4].get_text() # 工 作 地 点
47 gbsj = tds[5].find('span').get_text() # 发 布 日 期
48
49 tr_brief = table_info.find('tr', {'class': 'newlist_tr_detail'})
50 # 招 聘 简 介
51 brief = tr_brief.find('li', {'class': 'newlist_deatil_last'}).
get_text()
52
53 # 用 生 成 器 获 取 信 息
54 yield {'zwmc': zwmc, # 职 位 名 称
55 'fkl': fkl, # 反 馈 率
56 'gsmc': gsmc, # 公 司 名 称
57 'zwyx': zwyx, # 职 位 月 薪
58 'gzdd': gzdd, # 工 作 地 点
59 'gbsj': gbsj, # 公 布 时 间
60 'brief': brief, # 招 聘 简 介
61 'zw_link': zw_link, # 网 页 链 接
62 'save_date': date # 记 录 信 息 保 存 的 日 期
63 }
64
65
66 def main(args):
67 basic_url = 'http://sou.zhaopin.com/jobs/searchresult.ashx?'
68
69 for keyword in KEYWORDS:
70 mongo_table = db[keyword]
71 paras = {'jl': args[0],
72 'kw': keyword,
73 'p': args[1] # 第X页
http://liyangbit.com 371Chapter34. 项目实战：智联求职系列 Python数据之道
74 }
75 url = basic_url + urlencode(paras)
76 # print(url)
77 html = download(url)
78 # print(html)
79 if html:
80 data = get_content(html)
81 for item in data:
82 if mongo_table.update({'zw_link': item['zw_link']}, {'$set':
item}, True):
83 print('已 保 存 记 录：', item)
84
85 if __name__ == '__main__':
86 start = time.time()
87 number_list = list(range(TOTAL_PAGE_NUMBER))
88 args = product(ADDRESS, number_list)
89 pool = Pool()
90 pool.map(main, args) # 多 进 程 运 行
91 end = time.time()
92 print('Finished, task runs %s seconds.' % (end - start))
34.2 第二篇：python求职Top10城市分析
从智联招聘爬取相关信息后，我们关心的是如何对内容进行分析，获取用用的信息。本次以上篇文章中爬取的数据
为基础，分析关键词为“python”的爬取数据的情况，获取包括全国python招聘数量Top10的城市列表以及其他相
关信息。
34.2.1 主要分析步骤
• 数据读取
• 数据整理
• 对职位数量在全国主要城市的分布情况进行分析
• 对全国范围内的职位月薪情况进行分析
• 对该职位招聘岗位要求描述进行词云图分析，获取频率最高的关键字
• 选取两个城市，分别分析月薪分布情况以及招聘要求的词云图分析
34.2.2 具体分析过程
1 import pymongo
2 import pandas as pd
3 import matplotlib.pyplot as plt
4 import numpy as np
5 % matplotlib inline
6 plt.style.use('ggplot')
http://liyangbit.com 372Chapter34. 项目实战：智联求职系列 Python数据之道
1 # 解 决matplotlib显 示 中 文 问 题
2 plt.rcParams['font.sans-serif'] = ['SimHei'] # 指 定 默 认 字 体
3 plt.rcParams['axes.unicode_minus'] = False # 解 决 保 存 图 像 是 负 号'-'显 示 为 方 块 的
问 题
34.2.2.1 读取数据
1 client = pymongo.MongoClient('localhost')
2 db = client['zhilian']
3 table = db['python']
4
5 columns = ['zwmc',
6 'gsmc',
7 'zwyx',
8 'gbsj',
9 'gzdd',
10 'fkl',
11 'brief',
12 'zw_link',
13 '_id',
14 'save_date']
15
16 # url_set = set([records['zw_link'] for records in table.find()])
17 # print(url_set)
18
19 df = pd.DataFrame([records for records in table.find()], columns=columns)
20
21 # columns_update = ['职 位 名 称',
22 # '公 司 名 称',
23 # '职 位 月 薪',
24 # '公 布 时 间',
25 # '工 作 地 点',
26 # '反 馈 率',
27 # '招 聘 简 介',
28 # '网 页 链 接',
29 # '_id',
30 # '信 息 保 存 日 期']
31 # df.columns = columns_update
32 print('总 行 数 为：{}行'.format(df.shape[0]))
33 df.head(2)
结果如下图所示：
http://liyangbit.com 373Chapter34. 项目实战：智联求职系列 Python数据之道
34.2.2.2 数据整理
将str格式的日期变为datatime
1 df['save_date'] = pd.to_datetime(df['save_date'])
2 print(df['save_date'].dtype)
3 # df['save_date']
1 datetime64[ns]
筛选月薪格式为“XXXX-XXXX”的信息
1 df_clean = df[['zwmc',
2 'gsmc',
3 'zwyx',
4 'gbsj',
5 'gzdd',
6 'fkl',
7 'brief',
8 'zw_link',
9 'save_date']]
10
11 # 对 月 薪 的 数 据 进 行 筛 选， 选 取 格 式 为 “XXXX-XXXX” 的 信 息， 方 面 后 续 分 析
12 df_clean = df_clean[df_clean['zwyx'].str.contains('\d+-\d+', regex=True)]
13 print('总 行 数 为：{}行'.format(df_clean.shape[0]))
14 # df_clean.head()
1 总 行 数 为：22605行
分割月薪字段，分别获取月薪的下限值和上限值
1 # http://stackoverflow.com/questions/14745022/pandas-dataframe-how-do-i-split-a
-column-into-two
2
http://liyangbit.com 374Chapter34. 项目实战：智联求职系列 Python数据之道
3 # http://stackoverflow.com/questions/20602947/append-column-to-pandas-dataframe
4
5 # df_temp.loc[: ,'zwyx_min'],df_temp.loc[: , 'zwyx_max'] = df_temp.loc[: , '
zwyx'].str.split('-',1).str #会 有 警 告
6 s_min, s_max = df_clean.loc[: , 'zwyx'].str.split('-',1).str
7 df_min = pd.DataFrame(s_min)
8 df_min.columns = ['zwyx_min']
9 df_max = pd.DataFrame(s_max)
10 df_max.columns = ['zwyx_max']
11
12 df_clean_concat = pd.concat([df_clean, df_min, df_max], axis=1)
13 # df_clean['zwyx_min'].astype(int)
14 df_clean_concat['zwyx_min'] = pd.to_numeric(df_clean_concat['zwyx_min'])
15 df_clean_concat['zwyx_max'] = pd.to_numeric(df_clean_concat['zwyx_max'])
16 # print(df_clean['zwyx_min'].dtype)
17 print(df_clean_concat.dtypes)
18 df_clean_concat.head(2)
运行结果如下图所示：
• 将数据信息按职位月薪进行排序
1 df_clean_concat.sort_values('zwyx_min',inplace=True)
2 # df_clean_concat.tail()
• 判断爬取的数据是否有重复值
1 # 判 断 爬 取 的 数 据 是 否 有 重 复 值
2 print(df_clean_concat[df_clean_concat.duplicated('zw_link')==True])
1 Empty DataFrame
2 Columns: [zwmc, gsmc, zwyx, gbsj, gzdd, fkl, brief, zw_link, save_date,
zwyx_min, zwyx_max]
3 Index: []
http://liyangbit.com 375Chapter34. 项目实战：智联求职系列 Python数据之道
• 从上述结果可看出，数据是没有重复的。
34.2.3 对全国范围内的职位进行分析
34.2.3.1 主要城市的招聘职位数量分布情况
1 # from IPython.core.display import display, HTML
2 ADDRESS = [ '北 京', '上 海', '广 州', '深 圳',
3 '天 津', '武 汉', '西 安', '成 都', '大 连',
4 '长 春', '沈 阳', '南 京', '济 南', '青 岛',
5 '杭 州', '苏 州', '无 锡', '宁 波', '重 庆',
6 '郑 州', '长 沙', '福 州', '厦 门', '哈 尔 滨',
7 '石 家 庄', '合 肥', '惠 州', '太 原', '昆 明',
8 '烟 台', '佛 山', '南 昌', '贵 阳', '南 宁']
9 df_city = df_clean_concat.copy()
10
11 # 由 于 工 作 地 点 的 写 上， 比 如 北 京， 包 含 许 多 地 址 为 北 京-朝 阳 区 等
12 # 可 以 用 替 换 的 方 式 进 行 整 理， 这 里 用pandas的replace()方 法
13 for city in ADDRESS:
14 df_city['gzdd'] = df_city['gzdd'].replace([(city+'.*')],[city],regex=True)
15
16 # 针 对 全 国 主 要 城 市 进 行 分 析
17 df_city_main = df_city[df_city['gzdd'].isin(ADDRESS)]
18
19 df_city_main_count = df_city_main.groupby('gzdd')['zwmc','gsmc'].count()
20 df_city_main_count['gsmc'] = df_city_main_count['gsmc']/(df_city_main_count['
gsmc'].sum())
21 df_city_main_count.columns = ['number', 'percentage']
22
23 # 按 职 位 数 量 进 行 排 序
24 df_city_main_count.sort_values(by='number', ascending=False, inplace=True)
25
26 # 添 加 辅 助 列， 标 注 城 市 和 百 分 比， 方 面 在 后 续 绘 图 时 使 用
27 df_city_main_count['label']=df_city_main_count.index+ ' '+ ((
df_city_main_count['percentage']*100).round()).astype('int').astype('str')+'
%'
28 print(type(df_city_main_count))
29
30 # 职 位 数 量 最 多 的Top10城 市 的 列 表
31 print(df_city_main_count.head(10))
1 <class 'pandas.core.frame.DataFrame'>
2 number percentage label
3 gzdd
4 北 京 6936 0.315948 北 京 32%
5 上 海 3213 0.146358 上 海 15%
6 深 圳 1908 0.086913 深 圳 9%
7 成 都 1290 0.058762 成 都 6%
8 杭 州 1174 0.053478 杭 州 5%
http://liyangbit.com 376Chapter34. 项目实战：智联求职系列 Python数据之道
9 广 州 1167 0.053159 广 州 5%
10 南 京 826 0.037626 南 京 4%
11 郑 州 741 0.033754 郑 州 3%
12 武 汉 552 0.025145 武 汉 3%
13 西 安 473 0.021546 西 安 2%
• 对结果进行绘图：
1 from matplotlib import cm
2
3 label = df_city_main_count['label']
4 sizes = df_city_main_count['number']
5
6 # 设 置 绘 图 区 域 大 小
7 fig, axes = plt.subplots(figsize=(10,6),ncols=2)
8 ax1, ax2 = axes.ravel()
9
10 colors = cm.PiYG(np.arange(len(sizes))/len(sizes)) # colormaps: Paired, autumn,
rainbow, gray,spring,Darks
11
12 # 由 于 城 市 数 量 太 多， 饼 图 中 不 显 示labels和 百 分 比
13 patches, texts = ax1.pie(sizes,labels=None, shadow=False, startangle=0, colors=
colors)
14
15 ax1.axis('equal')
16
17 ax1.set_title('职 位 数 量 分 布', loc='center')
18
19 # ax2 只 显 示 图 例 （legend）
20 ax2.axis('off')
21 ax2.legend(patches, label, loc='center left', fontsize=9)
22
23 plt.savefig('job_distribute.jpg')
24 plt.show()
运行结果如下述饼图所示：
http://liyangbit.com 377Chapter34. 项目实战：智联求职系列 Python数据之道
34.2.3.2 月薪分布情况（全国）
1 from matplotlib.ticker import FormatStrFormatter
2
3 fig, (ax1, ax2) = plt.subplots(figsize=(10,8), nrows=2)
4
5 x_pos = list(range(df_clean_concat.shape[0]))
6 y1 = df_clean_concat['zwyx_min']
7
8 ax1.plot(x_pos, y1)
9 ax1.set_title('Trend of min monthly salary in China', size=14)
10 ax1.set_xticklabels('')
11 ax1.set_ylabel('min monthly salary(RMB)')
12
13 bins = [3000,6000, 9000, 12000, 15000, 18000, 21000, 24000, 100000]
14 counts, bins, patches = ax2.hist(y1, bins, normed=1, histtype='bar', facecolor=
'g', rwidth=0.8)
15 ax2.set_title('Hist of min monthly salary in China', size=14)
16 ax2.set_yticklabels('')
17 # ax2.set_xlabel('min monthly salary(RMB)')
18
19 # http://stackoverflow.com/questions/6352740/matplotlib-label-each-bin
20 ax2.set_xticks(bins) #将bins设 置 为xticks
21 ax2.set_xticklabels(bins, rotation=-90) # 设 置 为xticklabels的 方 向
22
23 # Label the raw counts and the percentages below the x-axis...
24 bin_centers = 0.5 * np.diff(bins) + bins[:-1]
25 for count, x in zip(counts, bin_centers):
26 # # Label the raw counts
27 # ax2.annotate(str(count), xy=(x, 0), xycoords=('data', 'axes fraction'),
http://liyangbit.com 378Chapter34. 项目实战：智联求职系列 Python数据之道
28 # xytext=(0, -70), textcoords='offset points', va='top', ha='center',
rotation=-90)
29
30 # Label the percentages
31 percent = '%0.0f%%' % (100 * float(count) / counts.sum())
32 ax2.annotate(percent, xy=(x, 0), xycoords=('data', 'axes fraction'),
33 xytext=(0, -40), textcoords='offset points', va='top', ha='center',
rotation=-90, color='b', size=14)
34
35 fig.savefig('salary_quanguo_min.jpg')
运行结果如下述图所示：
不考虑部分极值后，分析月薪分布情况
1 df_zwyx_adjust = df_clean_concat[df_clean_concat['zwyx_min']<=20000]
2
3 fig, (ax1, ax2) = plt.subplots(figsize=(10,8), nrows=2)
4
5 x_pos = list(range(df_zwyx_adjust.shape[0]))
6 y1 = df_zwyx_adjust['zwyx_min']
7
8 ax1.plot(x_pos, y1)
9 ax1.set_title('Trend of min monthly salary in China (adjust)', size=14)
10 ax1.set_xticklabels('')
11 ax1.set_ylabel('min monthly salary(RMB)')
12
http://liyangbit.com 379Chapter34. 项目实战：智联求职系列 Python数据之道
13 bins = [3000,6000, 9000, 12000, 15000, 18000, 21000]
14 counts, bins, patches = ax2.hist(y1, bins, normed=1, histtype='bar', facecolor=
'g', rwidth=0.8)
15 ax2.set_title('Hist of min monthly salary in China (adjust)', size=14)
16 ax2.set_yticklabels('')
17 # ax2.set_xlabel('min monthly salary(RMB)')
18
19 # http://stackoverflow.com/questions/6352740/matplotlib-label-each-bin
20 ax2.set_xticks(bins) #将bins设 置 为xticks
21 ax2.set_xticklabels(bins, rotation=-90) # 设 置 为xticklabels的 方 向
22
23 # Label the raw counts and the percentages below the x-axis...
24 bin_centers = 0.5 * np.diff(bins) + bins[:-1]
25 for count, x in zip(counts, bin_centers):
26 # # Label the raw counts
27 # ax2.annotate(str(count), xy=(x, 0), xycoords=('data', 'axes fraction'),
28 # xytext=(0, -70), textcoords='offset points', va='top', ha='center',
rotation=-90)
29
30 # Label the percentages
31 percent = '%0.0f%%' % (100 * float(count) / counts.sum())
32 ax2.annotate(percent, xy=(x, 0), xycoords=('data', 'axes fraction'),
33 xytext=(0, -40), textcoords='offset points', va='top', ha='center',
rotation=-90, color='b', size=14)
34
35 fig.savefig('salary_quanguo_min_adjust.jpg')
运行结果如下述图所示：
http://liyangbit.com 380Chapter34. 项目实战：智联求职系列 Python数据之道
34.2.3.3 相关技能要求
1 brief_list = list(df_clean_concat['brief'])
2 brief_str = ''.join(brief_list)
3 print(type(brief_str))
4 # print(brief_str)
5 # with open('brief_quanguo.txt', 'w', encoding='utf-8') as f:
6 # f.write(brief_str)
1 <class 'str'>
对获取到的职位招聘要求进行词云图分析，代码如下：
1 # -*- coding: utf-8 -*-
2 ”””
3 Created on Wed May 17 2017
4
5 @author: lemon
6 ”””
7
8 import jieba
9 from wordcloud import WordCloud, ImageColorGenerator
10 import matplotlib.pyplot as plt
11 import os
http://liyangbit.com 381Chapter34. 项目实战：智联求职系列 Python数据之道
12 import PIL.Image as Image
13 import numpy as np
14
15
16 with open('brief_quanguo.txt', 'rb') as f: # 读 取 文 件 内 容
17 text = f.read()
18 f.close()
19
20
21 # 首 先 使 用 jieba 中 文 分 词 工 具 进 行 分 词
22 wordlist = jieba.cut(text, cut_all=False)
23 # cut_all, True为 全 模 式，False为 精 确 模 式
24
25 wordlist_space_split = ' '.join(wordlist)
26
27 d = os.path.dirname(__file__)
28 alice_coloring = np.array(Image.open(os.path.join(d,'colors.png')))
29 my_wordcloud = WordCloud(background_color='#F0F8FF', max_words=100, mask=
alice_coloring,
30 max_font_size=300, random_state=42).generate(
wordlist_space_split)
31
32 image_colors = ImageColorGenerator(alice_coloring)
33
34 plt.show(my_wordcloud.recolor(color_func=image_colors))
35 plt.imshow(my_wordcloud) # 以 图 片 的 形 式 显 示 词 云
36 plt.axis('off') # 关 闭 坐 标 轴
37 plt.show()
38
39 my_wordcloud.to_file(os.path.join(d, 'brief_quanguo_colors_cloud.png'))
得到结果如下：
http://liyangbit.com 382Chapter34. 项目实战：智联求职系列 Python数据之道
34.2.4 北京
34.2.4.1 月薪分布情况
1 df_beijing = df_clean_concat[df_clean_concat['gzdd'].str.contains('北 京.*',
regex=True)]
2 df_beijing.to_excel('zhilian_kw_python_bj.xlsx')
3 print('总 行 数 为：{}行'.format(df_beijing.shape[0]))
4 # df_beijing.head()
1 总 行 数 为：6936行
参考全国分析时的代码，月薪分布情况图如下：
http://liyangbit.com 383Chapter34. 项目实战：智联求职系列 Python数据之道
34.2.4.2 相关技能要求
1 brief_list_bj = list(df_beijing['brief'])
2 brief_str_bj = ''.join(brief_list_bj)
3 print(type(brief_str_bj))
4 # print(brief_str_bj)
5 # with open('brief_beijing.txt', 'w', encoding='utf-8') as f:
6 # f.write(brief_str_bj)
1 <class 'str'>
词云图如下：
http://liyangbit.com 384Chapter34. 项目实战：智联求职系列 Python数据之道
34.2.5 长沙
34.2.5.1 月薪分布情况
1 df_changsha = df_clean_concat[df_clean_concat['gzdd'].str.contains('长 沙.*',
regex=True)]
2 # df_changsha = pd.DataFrame(df_changsha, ignore_index=True)
3 df_changsha.to_excel('zhilian_kw_python_cs.xlsx')
4 print('总 行 数 为：{}行'.format(df_changsha.shape[0]))
5 # df_changsha.tail()
1 总 行 数 为：280行
参考全国分析时的代码，月薪分布情况图如下：
http://liyangbit.com 385Chapter34. 项目实战：智联求职系列 Python数据之道
34.2.5.2 相关技能要求
1 brief_list_cs = list(df_changsha['brief'])
2 brief_str_cs = ''.join(brief_list_cs)
3 print(type(brief_str_cs))
4 # print(brief_str_cs)
5 # with open('brief_changsha.txt', 'w', encoding='utf-8') as f:
6 # f.write(brief_str_cs)
1 <class 'str'>
词云图如下：
http://liyangbit.com 386Chapter34. 项目实战：智联求职系列 Python数据之道
http://liyangbit.com 38735 项目实战：Python 数据分析，UFO 长啥样？
真心讲，长这么大，还没有见过UFO长啥样，偶然看到美国UFO报告中心有关于UFO时间记录的详细信息，突
然想分析下这些记录里都包含了那些有趣的信息，于是有了这次的分析过程。
当然，原始数据包含的记录信息比较多，我只是进了了比较简单的分析，有兴趣的童鞋可以一起来分析，别忘了也
给大家分享下您的分析情况哦。
本次分析的主要内容涉及以下几个方面：
• UFO长啥样？
• UFO在哪些地方出现的次数较多？
• UFO在哪些年份出现的次数较多？
• 热力图同时显示哪些州和哪些年UFO出现次数最多
1 import pandas as pd
2 import numpy as np
3 import matplotlib.pyplot as plt
4 % matplotlib inline
5 plt.style.use('ggplot')
35.1 数据整理与清洗
1 df = pd.read_csv('nuforc_events.csv')
1 print(df.shape) # 查 看 数 据 的 结 构
2 print(df.head())
1 (110265, 13)
2 Event_Time Event_Date Year Month Day Hour Minute \
3 0 2017-04-20T14:15:00Z 2017-04-20 2017.0 4.0 20.0 14.0 15.0
4 1 2017-04-20T04:56:00Z 2017-04-20 2017.0 4.0 20.0 4.0 56.0
5 2 2017-04-19T23:55:00Z 2017-04-19 2017.0 4.0 19.0 23.0 55.0
6 3 2017-04-19T23:50:00Z 2017-04-19 2017.0 4.0 19.0 23.0 50.0
7 4 2017-04-19T23:29:00Z 2017-04-19 2017.0 4.0 19.0 23.0 29.0
8
9 City State Shape Duration \
10 0 Palmyra NJ Other 5 minutes
11 1 Bridgeview IL Light 20 seconds
12 2 Newton AL Triangle 5 seconds
13 3 Newton AL Triangle 5-6 minutes
388Chapter35. 项目实战：Python数据分析，UFO长啥样？ Python数据之道
14 4 Denver CO Light 1 hour
15
16 Summary \
17 0 I observed an aircraft that seemed to look odd.
18 1 Bridgeview, IL, blue light. ((anonymous report))
19 2 Silent triangle UFO.
20 3 My friend and I stepped outside hoping to catc...
21 4 Moved slow but made quick turns staying and ci...
22
23 Event_URL
24 0 http://www.nuforc.org/webreports/133/S133726.html
25 1 http://www.nuforc.org/webreports/133/S133720.html
26 2 http://www.nuforc.org/webreports/133/S133724.html
27 3 http://www.nuforc.org/webreports/133/S133723.html
28 4 http://www.nuforc.org/webreports/133/S133721.html
• 由于存在许多包含NaN的数据信息，在进行分析之前，先用dropna()方法去除包含NaN的行数
1 df_clean = df.dropna()
2 print(df_clean.shape) # 查 看 去 除Nan后 还 有 多 少 行
3 print(df_clean.head())
1 (95004, 13)
2 Event_Time Event_Date Year Month Day Hour Minute \
3 0 2017-04-20T14:15:00Z 2017-04-20 2017.0 4.0 20.0 14.0 15.0
4 1 2017-04-20T04:56:00Z 2017-04-20 2017.0 4.0 20.0 4.0 56.0
5 2 2017-04-19T23:55:00Z 2017-04-19 2017.0 4.0 19.0 23.0 55.0
6 3 2017-04-19T23:50:00Z 2017-04-19 2017.0 4.0 19.0 23.0 50.0
7 4 2017-04-19T23:29:00Z 2017-04-19 2017.0 4.0 19.0 23.0 29.0
8
9 City State Shape Duration \
10 0 Palmyra NJ Other 5 minutes
11 1 Bridgeview IL Light 20 seconds
12 2 Newton AL Triangle 5 seconds
13 3 Newton AL Triangle 5-6 minutes
14 4 Denver CO Light 1 hour
15
16 Summary \
17 0 I observed an aircraft that seemed to look odd.
18 1 Bridgeview, IL, blue light. ((anonymous report))
19 2 Silent triangle UFO.
20 3 My friend and I stepped outside hoping to catc...
21 4 Moved slow but made quick turns staying and ci...
22
23 Event_URL
24 0 http://www.nuforc.org/webreports/133/S133726.html
25 1 http://www.nuforc.org/webreports/133/S133720.html
26 2 http://www.nuforc.org/webreports/133/S133724.html
27 3 http://www.nuforc.org/webreports/133/S133723.html
28 4 http://www.nuforc.org/webreports/133/S133721.html
http://liyangbit.com 389Chapter35. 项目实战：Python数据分析，UFO长啥样？ Python数据之道
• 由于1900年以前的数据较少，这里选择1900年以后的数据来进行分析，如下：
1 df_clean = df_clean[df_clean['Year']>=1900] # 获 取1900年 以 后 的 数 据 来 进 行 分 析
• 查看导入的每列数据的数据类型，通过运行结果，可以看到，“Event_Date”列并不是日期类型，因此要将之转
换。
• 可以采用pd.to_datetime()方法来操作
1 df_clean.dtypes
1 Event_Time object
2 Event_Date object
3 Year float64
4 Month float64
5 Day float64
6 Hour float64
7 Minute float64
8 City object
9 State object
10 Shape object
11 Duration object
12 Summary object
13 Event_URL object
14 dtype: object
• 用pd.to_datetime()方法来将str格式的日期转换成日期类型
1 pd.to_datetime(df_clean['Event_Date']) # 1061-12-31年 不 能 显 示
2 # OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 1061-12-31 00:00:00
3 df_clean.dtypes
1 Event_Time object
2 Event_Date object
3 Year float64
4 Month float64
5 Day float64
6 Hour float64
7 Minute float64
8 City object
9 State object
10 Shape object
11 Duration object
12 Summary object
13 Event_URL object
14 dtype: object
http://liyangbit.com 390Chapter35. 项目实战：Python数据分析，UFO长啥样？ Python数据之道
35.2 UFO长啥样？
• 按UFO出现的形状类型来分析,统计不同类型的UFO出现的次数
1 s_shape = df_clean.groupby('Shape')['Event_Date'].count()
2 print(type(s_shape))
3 s_shape.sort_values(inplace=True)
4 s_shape
1 <class 'pandas.core.series.Series'>
2
3 Shape
4 Changed 1
5 Hexagon 1
6 Pyramid 1
7 Flare 1
8 Round 2
9 Crescent 2
10 Delta 7
11 Cross 287
12 Cone 383
13 Egg 842
14 Teardrop 866
15 Chevron 1187
16 Diamond 1405
17 Cylinder 1495
18 Rectangle 1620
19 Flash 1717
20 Cigar 2313
21 Changing 2378
22 Formation 3070
23 Oval 4332
24 Disk 5841
25 Sphere 6482
26 Other 6658
27 Unknown 6887
28 Fireball 7785
29 Triangle 9358
30 Circle 9818
31 Light 20254
32 Name: Event_Date, dtype: int64
剔除特殊情况*剔除出现次数少于10次的类型*剔除“Unknown”及“Other”类型
1 s_shape_normal = s_shape[s_shape.values>10]
2 s_shape_normal
1 Shape
2 Cross 287
3 Cone 383
http://liyangbit.com 391Chapter35. 项目实战：Python数据分析，UFO长啥样？ Python数据之道
4 Egg 842
5 Teardrop 866
6 Chevron 1187
7 Diamond 1405
8 Cylinder 1495
9 Rectangle 1620
10 Flash 1717
11 Cigar 2313
12 Changing 2378
13 Formation 3070
14 Oval 4332
15 Disk 5841
16 Sphere 6482
17 Other 6658
18 Unknown 6887
19 Fireball 7785
20 Triangle 9358
21 Circle 9818
22 Light 20254
23 Name: Event_Date, dtype: int64
1 s_shape_normal = s_shape_normal[s_shape_normal.index.isin(['Unknown', 'Other'])
==False]
2 s_shape_normal
1 Shape
2 Cross 287
3 Cone 383
4 Egg 842
5 Teardrop 866
6 Chevron 1187
7 Diamond 1405
8 Cylinder 1495
9 Rectangle 1620
10 Flash 1717
11 Cigar 2313
12 Changing 2378
13 Formation 3070
14 Oval 4332
15 Disk 5841
16 Sphere 6482
17 Fireball 7785
18 Triangle 9358
19 Circle 9818
20 Light 20254
21 Name: Event_Date, dtype: int64
1 from matplotlib import font_manager as fm
2 from matplotlib import cm
3
http://liyangbit.com 392Chapter35. 项目实战：Python数据分析，UFO长啥样？ Python数据之道
4 labels = s_shape_normal.index
5 sizes = s_shape_normal.values
6
7 explode = (0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1) # ”explode” ， show the
selected slice
8
9 fig, axes = plt.subplots(figsize=(10,5),ncols=2) # 设 置 绘 图 区 域 大 小
10 ax1, ax2 = axes.ravel()
11
12 colors = cm.rainbow(np.arange(len(sizes))/len(sizes)) # colormaps: Paired,
autumn, rainbow, gray,spring,Darks
13 patches, texts, autotexts = ax1.pie(sizes, labels=labels, autopct='%1.0f%%',
explode=explode,
14 shadow=False, startangle=150, colors=colors, labeldistance=1.2,
pctdistance=1.05, radius=0.95)
15 # labeldistance: 控 制labels显 示 的 位 置
16 # pctdistance: 控 制 百 分 比 显 示 的 位 置
17 # radius: 控 制 切 片 突 出 的 距 离
18
19 ax1.axis('equal')
20
21 # 重 新 设 置 字 体 大 小
22 proptease = fm.FontProperties()
23 proptease.set_size('xx-small')
24 # font size include: ‘xx-small’,x-small’,'small’,'medium’,‘large’,‘x-
large’,‘xx-large’ or number, e.g. '12'
25 plt.setp(autotexts, fontproperties=proptease)
26 plt.setp(texts, fontproperties=proptease)
27
28 ax1.set_title('Shapes', loc='center')
29
30 # ax2 只 显 示 图 例 （legend）
31 ax2.axis('off')
32 ax2.legend(patches, labels, loc='center left', fontsize=9)
33
34 # plt.tight_layout()
35 # plt.savefig(”pie_shape_ufo.png”, bbox_inches='tight')
36 plt.savefig('ufo_shapes.jpg')
37 plt.show()
运行结果如下：
http://liyangbit.com 393Chapter35. 项目实战：Python数据分析，UFO长啥样？ Python数据之道
35.3 UFO在美国那些州（state）出现的次数比较多？
按“State”进行分组运算，统计ufo在各个州出现的次数
1 s_state = df_clean.groupby('State')['Event_Date'].count()
2 print(type(s_state))
3 s_state.head()
1 <class 'pandas.core.series.Series'>
2
3 State
4 AB 438
5 AK 472
6 AL 930
7 AR 791
8 AZ 3488
9 Name: Event_Date, dtype: int64
将分析得到的结果进行可视化显示，如下：
1 fig, ax1 = plt.subplots(figsize=(12,8))
2
3 width = 0.5
4 state = s_state.index
5 x_pos1 = np.arange(len(state))
6 y1 = s_state.values
7 ax1.bar(x_pos1, y1,color='#4F81BD',align='center', width=width, label='Amounts'
, linewidth=0)
8 ax1.set_title('Amount of reporting UFO events by State ')
9 ax1.set_xlim(-1, len(state))
10 ax1.set_xticks(x_pos1)
11 ax1.set_xticklabels(state, rotation = -90)
12 ax1.set_ylabel('Amount')
http://liyangbit.com 394Chapter35. 项目实战：Python数据分析，UFO长啥样？ Python数据之道
13
14 fig.savefig('ufo_state.jpg')
15 plt.show()
运行结果如下：
从上图可看出，ufo在加州（CA）出现的总次数明显比其他地方多，难道是ufo偏爱加州人民？
35.4 UFO在哪些年份出现的次数较多？
按“Year”进行分组运算，统计ufo在各个年份出现的次数
1 # df_clean['Year'].astype(int)
2 s_year = df_clean.groupby(df_clean['Year'].astype(int))['Event_Date'].count()
3 print(type(s_year))
4 s_year.head()
1 <class 'pandas.core.series.Series'>
2
3 Year
4 1905 1
5 1910 2
6 1920 1
7 1925 1
8 1929 1
9 Name: Event_Date, dtype: int64
将分析得到的结果进行可视化显示，如下：
http://liyangbit.com 395Chapter35. 项目实战：Python数据分析，UFO长啥样？ Python数据之道
1 fig, ax = plt.subplots(figsize=(12,20))
2 # fig, ax1 = plt.subplots(figsize=(12,8))
3 # fig, axes = plt.subplots(nrows=2, figsize=(12,8))
4 # fig, axes = plt.subplots(ncols=2, figsize=(18,4))
5
6 year = s_year.index
7 y_pos = np.arange(len(year))
8 x_value = s_year.values
9 ax.barh(y_pos, x_value,color='#4F81BD',align='center', label='Amounts',
linewidth=0)
10 ax.set_title('Amount of reporting UFO events by Year ')
11 ax.set_ylim(-0.5, len(year)-0.5)
12 ax.set_yticks(y_pos)
13 ax.set_yticklabels(year, rotation = 0, fontsize=6)
14 ax.set_xlabel('Amount')
15
16 plt.savefig('ufo_year.jpg')
17 plt.show()
运行结果如下：
http://liyangbit.com 396Chapter35. 项目实战：Python数据分析，UFO长啥样？ Python数据之道
从上图可看出，近年来UFO出现的报告次数最多
35.5 1997年以后的UFO事件分析
• 通过上述分析可看出，1997年以前，报告发现UFO的事件相对较少，下面将针对1997年以后的情况进行分析
1 df_97 = df_clean[(df_clean['Year']>=1997)]
http://liyangbit.com 397Chapter35. 项目实战：Python数据分析，UFO长啥样？ Python数据之道
2 df_97['Year'] = df_97['Year'].astype(int)
3 # df_97.astype({'Year':int})
4 print(df_97.shape)
5 print(df_97.head())
1 (86041, 13)
2 Event_Time Event_Date Year Month Day Hour Minute \
3 0 2017-04-20T14:15:00Z 2017-04-20 2017 4.0 20.0 14.0 15.0
4 1 2017-04-20T04:56:00Z 2017-04-20 2017 4.0 20.0 4.0 56.0
5 2 2017-04-19T23:55:00Z 2017-04-19 2017 4.0 19.0 23.0 55.0
6 3 2017-04-19T23:50:00Z 2017-04-19 2017 4.0 19.0 23.0 50.0
7 4 2017-04-19T23:29:00Z 2017-04-19 2017 4.0 19.0 23.0 29.0
8
9 City State Shape Duration \
10 0 Palmyra NJ Other 5 minutes
11 1 Bridgeview IL Light 20 seconds
12 2 Newton AL Triangle 5 seconds
13 3 Newton AL Triangle 5-6 minutes
14 4 Denver CO Light 1 hour
15
16 Summary \
17 0 I observed an aircraft that seemed to look odd.
18 1 Bridgeview, IL, blue light. ((anonymous report))
19 2 Silent triangle UFO.
20 3 My friend and I stepped outside hoping to catc...
21 4 Moved slow but made quick turns staying and ci...
22
23 Event_URL
24 0 http://www.nuforc.org/webreports/133/S133726.html
25 1 http://www.nuforc.org/webreports/133/S133720.html
26 2 http://www.nuforc.org/webreports/133/S133724.html
27 3 http://www.nuforc.org/webreports/133/S133723.html
28 4 http://www.nuforc.org/webreports/133/S133721.html
将数据按“Year”和“State”进行分组运算，如下：
1 df_amount_year = df_97.groupby(['Year', 'State'])['Event_Date'].size().
reset_index()
2 df_amount_year.columns = ['Year', 'State', 'Amount']
3 print(df_amount_year.head())
1 Year State Amount
2 0 1997 AB 6
3 1 1997 AK 5
4 2 1997 AL 8
5 3 1997 AR 10
6 4 1997 AZ 127
1 import seaborn as sns
2
http://liyangbit.com 398Chapter35. 项目实战：Python数据分析，UFO长啥样？ Python数据之道
3 df_pivot = df_amount_year.pivot_table(index='State', columns='Year', values='
Amount')
4
5 f, ax = plt.subplots(figsize = (10, 15))
6 cmap = sns.cubehelix_palette(start = 1, rot = 3, gamma=0.8, as_cmap = True)
7 sns.heatmap(df_pivot, cmap = cmap, linewidths = 0.05, ax = ax)
8
9 ax.set_title('Amounts per State and Year since Year 1997')
10 ax.set_xlabel('Year')
11 ax.set_ylabel('State')
12 ax.set_xticklabels(ax.get_xticklabels(), rotation=90)
13 f.savefig('ufo_per_year_state.jpg')
http://liyangbit.com 399Chapter35. 项目实战：Python数据分析，UFO长啥样？ Python数据之道
• 上图中，颜色越深的地方，表示UFO事件报告的次数越多。
原始数据来源于美国的UFO事件报告中心。
http://liyangbit.com 40036 项目实战：世界杯系列
36.1 第一篇：2018世界杯：用Python分析热门夺冠球队
2018年，火热的世界杯即将拉开序幕。在比赛开始之前，我们不妨用Python来对参赛队伍的实力情况进行分析，
并大胆的预测下本届世界杯的夺冠热门球队。
通过数据分析，可以发现很多有趣的结果，比如：
• 找出哪些队伍是首次进入世界杯的黑马队伍
• 找出2018年32强中之前已经进入过世界杯，但在世界杯上没有赢得过一场比赛的队伍
当然，我们本次的主要任务是要通过数据分析来预测2018年世界杯的夺冠热门队伍。
本次分析的数据来源于Kaggle，包含从1872年到今年的数据，包括世界杯比赛、世界杯预选赛、亚洲杯、欧洲
杯、国家之间的友谊赛等比赛，一共大约40000场比赛的情况。
本次的环境为
• window7系统
• python3.6
• JupyterNotebook
• pandasversion0.22.0
先来看看数据的情况：
1 import pandas as pd
2 import matplotlib.pyplot as plt
3
4 %matplotlib inline
5 plt.style.use('ggplot')
6
7 df = pd.read_csv('results.csv')
8 df.head()
该数据集包含的数据列的信息如下：
• 日期
• 主队名称
• 客队名称
• 主队进球数（不含点球）
• 客队进球数（不含点球）
• 比赛的类型
401Chapter36. 项目实战：世界杯系列 Python数据之道
• 比赛所在城市
• 比赛所在国家
• 是否中立
结果如下：
36.1.1 获取所有世界杯比赛的数据（不含预选赛）
1 df_FIFA_all = df[df['tournament'].str.contains('FIFA', regex=True)]
2 df_FIFA = df_FIFA_all[df_FIFA_all['tournament']=='FIFA World Cup']
3 df_FIFA.head()
结果如下：
数据做一个初步整理
1 df_FIFA.loc[:,'date'] = pd.to_datetime(df_FIFA.loc[:,'date'])
2 df_FIFA['year'] = df_FIFA['date'].dt.year
3 df_FIFA['diff_score'] = df_FIFA['home_score']-df_FIFA['away_score']
4 df_FIFA['win_team'] = ''
5 df_FIFA['diff_score'] = pd.to_numeric(df_FIFA['diff_score'])
创建一个新的列数据，包含获胜队伍的信息
1 # The first method to get the winners
2
3 df_FIFA.loc[df_FIFA['diff_score']> 0, 'win_team'] = df_FIFA.loc[df_FIFA['
diff_score']> 0, 'home_team']
4 df_FIFA.loc[df_FIFA['diff_score']< 0, 'win_team'] = df_FIFA.loc[df_FIFA['
diff_score']< 0, 'away_team']
5 df_FIFA.loc[df_FIFA['diff_score']== 0, 'win_team'] = 'Draw'
6
7 df_FIFA.head()
8
http://liyangbit.com 402Chapter36. 项目实战：世界杯系列 Python数据之道
9 # The second method to get the winners
10
11 def find_win_team(df):
12 winners = []
13 for i, row in df.iterrows():
14 if row['home_score'] > row['away_score']:
15 winners.append(row['home_team'])
16 elif row['home_score'] < row['away_score']:
17 winners.append(row['away_team'])
18 else:
19 winners.append('Draw')
20 return winners
21
22 df_FIFA['winner'] = find_win_team(df_FIFA)
23 df_FIFA.head()
结果如下：
36.1.2 获取世界杯所有比赛的前20强数据情况
36.1.2.1 获取世界杯所有比赛获胜场数最多的前20强数据
1 s = df_FIFA.groupby('win_team')['win_team'].count()
2 s.sort_values(ascending=False, inplace=True)
3 s.drop(labels=['Draw'], inplace=True)
用pandas可视化如下：
柱状图
1 s.head(20).plot(kind='bar', figsize=(10,6), title='Top 20 Winners of World Cup'
)
http://liyangbit.com 403Chapter36. 项目实战：世界杯系列 Python数据之道
水平柱状图
1 s.sort_values(ascending=True,inplace=True)
2 s.tail(20).plot(kind='barh', figsize=(10,6), title='Top 20 Winners of World Cup
')
饼图
1 s_percentage = s/s.sum()
2 s_percentage
http://liyangbit.com 404Chapter36. 项目实战：世界杯系列 Python数据之道
3 s_percentage.tail(20).plot(kind='pie', figsize=(10,10), autopct='%.1f%%',
4 startangle=173, title='Top 20 Winners of World Cup',
label='')
分析结论1：从赢球场数来看，巴西、德国、意大利、阿根廷四支球队实力最强。
通过上面的分析，我们还可以来查看部分国家的获胜情况
1 s.get('China', default = 'NA')
2 s.get('Japan', default = 'NA')
3 s.get('Korea DPR', default = 'NA')
4 s.get('Korea Republic', default = 'NA')
5 s.get('Egypt', default = 'NA')
运行结果分别是“NA”，4，1，5，“NA”。
从结果来看，中国队，在世界杯比赛上（不含预选赛）还没有赢过。当然，本次世界杯的黑马-埃及队，之前两度进
入世界杯上，但也没有赢过~~
上面分析的是赢球场数的情况，下面我们来看下进球总数情况。
36.1.2.2 各个国家队进球总数量情况
http://liyangbit.com 405Chapter36. 项目实战：世界杯系列 Python数据之道
1 df_score_home = df_FIFA[['home_team', 'home_score']]
2 column_update = ['team', 'score']
3 df_score_home.columns = column_update
4 df_score_away = df_FIFA[['away_team', 'away_score']]
5 df_score_away.columns = column_update
6 df_score = pd.concat([df_score_home,df_score_away], ignore_index=True)
7 s_score = df_score.groupby('team')['score'].sum()
8 s_score.sort_values(ascending=False, inplace=True)
9 s_score.sort_values(ascending=True, inplace=True)
10 s_score.tail(20).plot(kind='barh', figsize=(10,6), title='Top 20 in Total
Scores of World Cup')
分析结论2：从进球总数量来看，德国、巴西、阿根廷、意大利四支球实力最强。
上面分析的是自1872年以来的所有球队的数据情况，下面，我们重点来分析下2018年世界杯32强的数据情况。
36.1.3 2018年世界杯32强分析
2018年世界杯的分组情况如下：
第一组：俄罗斯、德国、巴西、葡萄牙、阿根廷、比利时、波兰、法国
第二组：西班牙、秘鲁、瑞士、英格兰、哥伦比亚、墨西哥、乌拉圭、克罗地亚
第三组：丹麦、冰岛、哥斯达黎加、瑞典、突尼斯、埃及、塞内加尔、伊朗
第四组：塞尔维亚、尼日利亚、澳大利亚、日本、摩洛哥、巴拿马、韩国、沙特阿拉伯
获取32强的所有数据
首先，判断是否有队伍首次打入世界杯。
1 team_list = ['Russia', 'Germany', 'Brazil', 'Portugal', 'Argentina', 'Belgium',
'Poland', 'France',
http://liyangbit.com 406Chapter36. 项目实战：世界杯系列 Python数据之道
2 'Spain', 'Peru', 'Switzerland', 'England', 'Colombia', 'Mexico', '
Uruguay', 'Croatia',
3 'Denmark', 'Iceland', 'Costa Rica', 'Sweden', 'Tunisia', 'Egypt', '
Senegal', 'Iran',
4 'Serbia', 'Nigeria', 'Australia', 'Japan', 'Morocco', 'Panama', '
Korea Republic', 'Saudi Arabia']
5 for item in team_list:
6 if item not in s_score.index:
7 print(item)
8
9 out:
10 Iceland
11 Panama
通过上述分析可知，冰岛队和巴拿马队是首次打入世界杯的。
由于冰岛队和巴拿马队是首次进入世界杯，所以这里的32强数据，事实上是没有这两支队伍的历史数据的。
1 df_top32 = df_FIFA[(df_FIFA['home_team'].isin(team_list))&(df_FIFA['away_team'
].isin(team_list))]
36.1.3.1 自1872年以来，32强数据情况
赢球场数情况
1 s_32 = df_top32.groupby('win_team')['win_team'].count()
2 s_32.sort_values(ascending=False, inplace=True)
3 s_32.drop(labels=['Draw'], inplace=True)
4 s_32.sort_values(ascending=True,inplace=True)
5 s_32.plot(kind='barh', figsize=(8,12), title='Top 32 of World Cup since year
1872')
http://liyangbit.com 407Chapter36. 项目实战：世界杯系列 Python数据之道
进球数据情况
1 df_score_home_32 = df_top32[['home_team', 'home_score']]
2 column_update = ['team', 'score']
3 df_score_home_32.columns = column_update
4 df_score_away_32 = df_top32[['away_team', 'away_score']]
5 df_score_away_32.columns = column_update
6 df_score_32 = pd.concat([df_score_home_32,df_score_away_32], ignore_index=True)
7 s_score_32 = df_score_32.groupby('team')['score'].sum()
8 s_score_32.sort_values(ascending=False, inplace=True)
9 s_score_32.sort_values(ascending=True, inplace=True)
10 s_score_32.plot(kind='barh', figsize=(8,12), title='Top 32 in Total Scores of
World Cup since year 1872')
http://liyangbit.com 408Chapter36. 项目实战：世界杯系列 Python数据之道
分析结论3：自1872年以来，32强之间的世界杯比赛，从赢球场数和进球数量来看，德国、巴西、阿根廷三支球
队实力最强。
自1872年到现在，已经有100多年，时间跨度较大，有些国家已发生重大变化，后续分别分析自1978年（近10
届）以及2002年（近4届）以来的比赛情况。
程序代码是类似的，这里只显示可视化的结果。
http://liyangbit.com 409Chapter36. 项目实战：世界杯系列 Python数据之道
36.1.3.2 自1978年以来，32强数据情况
赢球场数情况
进球数据情况
http://liyangbit.com 410Chapter36. 项目实战：世界杯系列 Python数据之道
分析结论4：自1978年以来，32强之间的世界杯比赛，从赢球场数来看，阿根廷、德国、巴西三支球队实力最强。
从进球数量来看，前3强也是这三支球队，但德国队的数据优势更明显。
36.1.3.3 自2002年以来，32强数据情况
赢球场数情况
http://liyangbit.com 411Chapter36. 项目实战：世界杯系列 Python数据之道
进球数据情况
http://liyangbit.com 412Chapter36. 项目实战：世界杯系列 Python数据之道
分析结论5：自2002年以来，32强之间的世界杯比赛，从赢球场数和进球数量来看，德国、阿根廷、巴西三支球
队实力最强。其中，德国队的数据优势更明显。
36.1.4 综合结论
2018年世界杯的32支队伍，根据以往的世界杯比赛数据来看，预测前三强为德国、阿根廷和巴西，其中德国队应
该是夺冠的最大热门。
特别说明：以上数据分析，纯属个人学习用，预测结果与实际情况可能偏差很大，不能用于其他用途。
如需获取本文源代码，请在公众号『Python数据之道』后台回复“code”
http://liyangbit.com 413Chapter36. 项目实战：世界杯系列 Python数据之道
36.2 第二篇：德国是2018世界杯夺冠最大热门？
2018年，世界杯小组赛已经在如火如荼的进行中。在上篇文章的基础上，我们继续分析世界杯32强的实力情况，
以期能够更进一步分析本次世界杯的夺冠热门球队。
三十年河东三十年河西，对于世界杯而言，这个时间可能4年就足够。
前几场爆冷，使得天台上已经拥挤不堪，跳水的股市更是让天台一度混乱。
在文章开始之前，提醒大家：
赌球有风险，看球须尽兴
本文的重点是通过分析32强之间的比赛，透过历史数据来预测夺冠热门球队。
本次分析的数据来源于Kaggle，包含从1872年到今年的数据，包括世界杯比赛、世界杯预选赛、亚洲杯、欧洲
杯、国家之间的友谊赛等比赛，一共大约40000场比赛的情况。
本次的环境为
• window7系统
• python3.6
• JupyterNotebook
• pandasversion0.22.0
先来看看数据的情况：
1 import pandas as pd
2 import matplotlib.pyplot as plt
3
4 %matplotlib inline
5 plt.style.use('ggplot')
6
7 # 解 决matplotlib显 示 中 文 问 题
8 plt.rcParams['font.sans-serif'] = ['SimHei'] # 指 定 默 认 字 体
9 plt.rcParams['axes.unicode_minus'] = False # 解 决 保 存 图 像 是 负 号'-'显 示 为 方 块 的
问 题
10
11 df = pd.read_csv('results.csv')
12 df.head()
该数据集包含的数据列的信息如下：
• 日期
• 主队名称
• 客队名称
• 主队进球数（不含点球）
• 客队进球数（不含点球）
• 比赛的类型
• 比赛所在城市
• 比赛所在国家
http://liyangbit.com 414Chapter36. 项目实战：世界杯系列 Python数据之道
• 是否中立
结果如下：
36.2.1 获取所有世界杯比赛的数据（含预选赛）
创建一个新的列数据，包含获胜队伍的信息，以及获取所有世界杯比赛的数据，包含预选赛。
1 mask = df['home_score'] - df['away_score']
2 df.loc[mask > 0, 'win_team'] = df.loc[mask > 0, 'home_team']
3 df.loc[mask < 0, 'win_team'] = df.loc[mask < 0, 'away_team']
4 df.loc[mask == 0, 'win_team'] = 'Draw'
5
6 df_FIFA_all = df[df['tournament'].str.contains('FIFA', regex=True)]
结果如下：
36.2.2 世界杯战绩分析（含预选赛）
从前文来看，在世界杯历史上，实力最强的5支球队是德国、阿根廷、巴西、法国、西班牙。
接下来，我们将比赛的范围扩大至包含世界杯预选赛，通过5支球队之间的比赛情况来进行分析。
1 team_top5 = ['Germany', 'Argentina', 'Brazil', 'France', 'Spain']
2 df_FIFA_top5 = df_FIFA_all[(df_FIFA_all['home_team'].isin(team_top5))&
3 (df_FIFA_all['away_team'].isin(
team_top5))]
4 df_FIFA_top5.shape
5
6
7 out:
8 (43, 10)
在世界杯历史上，5支球队在共有43场比赛相遇。
http://liyangbit.com 415Chapter36. 项目实战：世界杯系列 Python数据之道
通过这43场比赛分析后，5支球队的胜负场数排名如下：
1 s_FIFA_top5 = df_FIFA_top5.groupby('win_team')['win_team'].count()
2 s_FIFA_top5.drop('Draw', inplace=True)
3 s_FIFA_top5.sort_values(ascending=False, inplace=True)
4 s_FIFA_top5.plot(kind='bar', figsize=(10,6), title='Top Five in World Cup')
结果如下：
下面，着重来分析下这5支球队，在世界杯上，两两对阵时的胜负情况。
首先自定义两个函数，分别获得两支球队获胜场数情况以及自定义绘图函数
1 # 自 定 义 函 数， 返 回 两 支 球 队 获 胜 场 数 情 况
2 def team_vs(df,team_A,team_B):
3 df_team_A_B = df[(df['home_team'].isin([team_A,team_B]))&
4 (df['away_team'].isin([team_A,team_B]))
]
5 s_win_team = df_team_A_B.groupby('win_team')['win_team'].count()
6 return s_win_team
7
8 # 如 需 获 取 本 文 源 代 码， 请 关 注 公 众 号 “Python数 据 之 道”，
9 # 在 公 众 号 后 台 回 复 “code” ， 谢 谢 大 家 支 持。
10
11 # 自 定 义 函 数， 两 支 球 队 在 世 界 杯 的 对 阵 胜 负 情 况 制 图
12 def team_vs_plot(df,team_A,team_B,ax):
13 s_win_FIFA = team_vs(df,team_A,team_B)
14 title = team_A + ' vs ' +team_B
15 s_win_FIFA.plot(kind='bar', ax =ax)
16 ax.set_xlabel('')
http://liyangbit.com 416Chapter36. 项目实战：世界杯系列 Python数据之道
17 ax.set_title(title,fontdict={'fontsize':10})
18 ax.set_xticklabels(s_win_FIFA.index, rotation=20)
基于上述函数，分析结果如下：
36.2.2.1 世界杯战绩：巴西vs其他4支球队
1 f, axes = plt.subplots(figsize=(10,10), ncols=2, nrows=2)
2 ax1, ax2,ax3,ax4 = axes.ravel()
3 team_vs_plot(df_FIFA_all,'Brazil','Germany',ax=ax1)
4 team_vs_plot(df_FIFA_all,'Brazil','Argentina',ax=ax2)
5 team_vs_plot(df_FIFA_all,'Brazil','France',ax=ax3)
6 team_vs_plot(df_FIFA_all,'Brazil','Spain',ax=ax4)
7
8 # 如 需 获 取 本 文 源 代 码， 请 关 注 公 众 号 “Python数 据 之 道”，
9 # 在 公 众 号 后 台 回 复 “code” ， 谢 谢 大 家 支 持。
10
11 # set main title of the figure
12 plt.suptitle('Brazil vs other Top 4 teams in World Cup', fontsize=14,
fontweight='bold', x=0.5, y=0.94)
13 plt.show()
结果如下：
http://liyangbit.com 417Chapter36. 项目实战：世界杯系列 Python数据之道
统计现象1：在世界杯上的战绩，统计获胜场数如下（不含平局）：巴西1：1德国，巴西6：3阿根廷，巴西1：2
法国，巴西3：1西班牙巴西队，输赢不好判断……
36.2.2.2 世界杯战绩：德国vs其他3支球队
代码跟2.1部分是类似的，结果如下：
http://liyangbit.com 418Chapter36. 项目实战：世界杯系列 Python数据之道
统计现象2：
在世界杯上的战绩，统计获胜场数如下（不含平局）：德国4：1阿根廷，德国2：1法国，德国2：1西班牙，德国
在这5支持球队里，获胜的优势相对比较明显。
36.2.2.3 世界杯战绩：阿根廷vs其他2支球队
代码跟2.1部分是类似的，结果如下：
http://liyangbit.com 419Chapter36. 项目实战：世界杯系列 Python数据之道
统计现象3：在世界杯上的战绩，统计获胜场数如下（不含平局）：阿根廷2：0法国，阿根廷1：0西班牙但阿根
廷不敌巴西和德国
36.2.2.4 世界杯战绩：西班牙vs法国
综合小结论：从历届世界杯上的表现情况来看，分析5强之间两两对阵后，发现德国队的表现是最好的。其次巴西
和阿根廷的表现也不错。
考虑到，历届世界杯的数据，时间跨度很大，很多球队其实已经发生了很大变化。
球队真实的情况，可能选择近几年的比赛，以及包含不同级别的比赛，可能分析效果要更好些。
http://liyangbit.com 420Chapter36. 项目实战：世界杯系列 Python数据之道
下面，重点来分析2014年以来包含所有比赛的情况。
36.2.3 2014年以来，所有比赛的战绩对比
首先，时间选择2014年之后（含2014年），距离现在的时间比较近，相对来说，球队人员的组成变化小一些。
当然，这里的时间选择，对于结果是有影响的。大家可以探讨下这个因素带来的影响。
36.2.3.1 2014年以来所有球队所有比赛胜负情况概览
1 df['date'] = pd.to_datetime(df['date'])
2 df['year'] = df['date'].dt.year
3 df_since_2014 = df[df['year']>=2014]
4 df_since_2014.shape
2014年以来，共有3600多场比赛。
针对3600多场比赛分析后，胜负场数情况如下：
1 s_all = df_since_2014.groupby('win_team')['win_team'].count()
2 s_all.drop('Draw', inplace=True)
3 s_all.sort_values(ascending=True, inplace=True)
4 s_all.tail(50).plot(kind='barh', figsize=(8,16), tick_label='',title='Top 50 in
all tournament since 2014')
http://liyangbit.com 421Chapter36. 项目实战：世界杯系列 Python数据之道
从上图来看，2014年以来，墨西哥，法国，德国、葡萄牙、巴西、比利时、韩国和西班牙表现相对较好。
结果是不是跟想象中的有些差异？
6月17日的小组赛，德国不敌墨西哥，看来也不是全无理由的。
但是，本次我们主要还是要考虑32强之间的对阵，这样更能反映现实情况。
36.2.3.2 2014年以来32强相互之间在所有比赛中的概览情况
http://liyangbit.com 422Chapter36. 项目实战：世界杯系列 Python数据之道
1 team_list = ['Russia', 'Germany', 'Brazil', 'Portugal', 'Argentina', 'Belgium',
'Poland', 'France',
2 'Spain', 'Peru', 'Switzerland', 'England', 'Colombia', 'Mexico', '
Uruguay', 'Croatia',
3 'Denmark', 'Iceland', 'Costa Rica', 'Sweden', 'Tunisia', 'Egypt', '
Senegal', 'Iran',
4 'Serbia', 'Nigeria', 'Australia', 'Japan', 'Morocco', 'Panama', '
Korea Republic', 'Saudi Arabia']
5
6 df_top32 = df_since_2014[(df_since_2014['home_team'].isin(team_list))&(
df_since_2014['away_team'].isin(team_list))]
7
8 s_top32 = df_top32.groupby('win_team')['win_team'].count()
9 s_top32.drop('Draw', inplace=True)
10 s_top32.sort_values(ascending=True, inplace=True)
11 s_top32.plot(kind='barh', figsize=(8,12), tick_label='',title='Top 32 in all
tournament since 2014')
12 # plt.ylabel('')
http://liyangbit.com 423Chapter36. 项目实战：世界杯系列 Python数据之道
从上图来看，自2014年以来，巴西、法国、葡萄牙、阿根廷、墨西哥、比利时、德国、西班牙、英国为前9强。
下面我们来分析top9之间的胜负情况。
1 team_top9 = [ 'Brazil', 'France', 'Portugal',
2 'Argentina','Mexico','Belgium',
3 'Germany','Spain','England']
4 df_top9 = df_since_2014[(df_since_2014['home_team'].isin(team_top9))&
5 (df_since_2014['away_team'].isin(team_top9))]
6 df_top9.shape
2014年以来，top9之间一共踢了44场比赛（包括友谊赛）。
http://liyangbit.com 424Chapter36. 项目实战：世界杯系列 Python数据之道
总体来说，比赛的场数不是太多，基于这些数据来分析，可能对结果会有较大的影响。
九强排名如下：
1 s_top9 = df_top9.groupby('win_team')['win_team'].count()
2 s_top9.drop('Draw', inplace=True)
3 s_top9.sort_values(ascending=False, inplace=True)
4 s_top9.plot(kind='bar', figsize=(10,6), title='Top 9 in all tournament since
2014')
来查看下都统计了哪些类型的比赛
1 s_tournament = df_top9.groupby('tournament')['tournament'].count()
2 s_tournament_percentage = s_tournament/s_tournament.sum()
3 # s_tournament_percentage.sort_values(ascending=False, inplace=True)
4 s_tournament_percentage.tail(20).plot(kind='pie', figsize=(10,10), autopct='%.1
f%%',
5 startangle=150, title='percentage of tournament',
label='',
6 labeldistance=1.25, pctdistance=1.08)
http://liyangbit.com 425Chapter36. 项目实战：世界杯系列 Python数据之道
从上面来看，友谊赛占的比例较大。
考虑到友谊赛在有些情况下可能不能比较准确的反映出球队的真实水平，且友谊赛站的场数比例较大，我们剔除友
谊赛再来看看结果情况。
36.2.3.3 2014年以来32强剔除友谊赛后的胜负情况概览
1 df_top9_no_friendly = df_top9[df_top9['tournament']!= 'Friendly']
2 df_top9_no_friendly.groupby('tournament')['tournament'].count()
3
4 out:
5 tournament
6 Confederations Cup 3
7 FIFA World Cup 6
8 FIFA World Cup qualification 2
9 UEFA Euro 2
10 Name: tournament, dtype: int64
剔除友谊赛后，比赛类型分布如下：
http://liyangbit.com 426Chapter36. 项目实战：世界杯系列 Python数据之道
剔除友谊赛后，Top9的情况如下：
在概览中可以看出，是否剔除友谊赛（Friendly），对排名还是有影响的。
http://liyangbit.com 427Chapter36. 项目实战：世界杯系列 Python数据之道
另外，剔除友谊赛后，总的比赛场数更少了（只有13场），9强之间有些队伍没有比赛，或者没有赢过，这个数据
用来分析的作用更有限。
当然，在分析中是否要剔除友谊赛，应该是值得商榷的。
36.2.3.4 九强两两对阵的胜负情况概览
这里，我们后续分析采用包含友谊赛的数据，来分别分析9强之间两两对阵的情况，看看哪支球队的胜率更高些。
首先自定义几个函数，方便进行分析。
自定义获取球队某年至今获胜比例函数
1 # 自 定 义 获 取 球 队 某 年 至 今 获 胜 比 例 函 数
2 def probability(df,year,team_A,team_B):
3 prob = []
4 df_year = df[df['year']>= year]
5 s = team_vs(df_year,team_A,team_B)
6 s_team_A = 0 if s.get(team_A) is None else s.get(team_A)
7 s_A_win = s_team_A/s.sum()
8 s_team_B = 0 if s.get(team_B) is None else s.get(team_B)
9 s_B_win = s_team_B/s.sum()
10 s_draw = 1 - s_A_win - s_B_win
11 prob.append(year)
12 prob.append(s_A_win)
13 prob.append(s_B_win)
14 prob.append(s_draw)
15 return prob
自定义获取两支球队历史获胜情况对比函数
1 # 自 定 义 获 取 两 支 球 队 历 史 获 胜 情 况 对 比 函 数
2 def his_team_data(df,year_start,year_end,team_A,team_B):
3 row_team = []
4 # df_team = pd.DataFrame(columns=('year', 'team_A_win', 'team_B_win', 'draw
'))
5 for yr in list(range(year_start,year_end+1)):
6 team_A_vs_team_B = probability(df,yr,team_A,team_B)
7 row_team.append(team_A_vs_team_B)
8 team_A_win = team_A + '_win_percentage'
9 team_B_win = team_B + '_win_percentage'
10 df_team = pd.DataFrame(row_team, columns=('year', team_A_win, team_B_win, '
draw_percentage'))
11 return df_team
自定义两支球队历史获胜情况制图函数
1 # 自 定 义 两 支 球 队 历 史 获 胜 情 况 制 图 函 数
2 def team_plot(df,year_start,year_end,team_A,team_B,ax):
3 team_A_vs_team_B = team_A + '_vs_' + team_B
4 team_A_vs_team_B = his_team_data(df,year_start,year_end,team_A,team_B)
http://liyangbit.com 428Chapter36. 项目实战：世界杯系列 Python数据之道
5 title = team_A + ' vs ' + team_B
6 columns = [team_A+'_win_percentage',team_B+'_win_percentage','
draw_percentage']
7 team_A_vs_team_B.set_index('year')[columns].plot(kind='line',figsize=(10,6)
, title=title,ax=ax)
这些函数有什么用呢，首先我们来分析下巴西vs德国的情况，如下：
1 team_A = 'Brazil'
2 team_B = 'Germany'
3
4 f, axes = plt.subplots(figsize=(6,12), ncols=1, nrows=2)
5 ax1, ax2 = axes.ravel()
6
7 team_plot(df,1930,2016,team_A,team_B,ax1)
8 ax1.set_xlabel('')
9 team_plot(df,2000,2016,team_A,team_B,ax2)
10 ax2.set_title('')
11
12 plt.show()
上述图中，x轴代表的含义是从某年至今（数据集含有部分2018年的比赛数据），两支球队的胜负情况。例如2012
对应的是德国跟巴西从2012年至今，两支球队的胜负情况。
所以，时间越早，两支球队的比赛数量越多，数据曲线的波动可能要小些。但由于球队的成员组成在不断的变化，
会导致越早的数据，其分析价值越弱。因此，选择合适的年份进行分析就显得很重要。
有童鞋说，如果我要同时分析德国对阵另外8支球队呢？
这里，用上面的函数，也是很迅速的，代码如下：
http://liyangbit.com 429Chapter36. 项目实战：世界杯系列 Python数据之道
1 team_A = 'Germany'
2
3 for team in ['France','Portugal', 'Argentina','Mexico','Belgium','Brazil','
Spain','England']:
4 team_B = team
5
6 f, axes = plt.subplots(figsize=(6,12), ncols=1, nrows=2)
7 ax1, ax2 = axes.ravel()
8
9 team_plot(df,1930,2016,team_A,team_B,ax1)
10 ax1.set_xlabel('')
11 team_plot(df,2000,2016,team_A,team_B,ax2)
12 ax2.set_title('')
13
14 plt.show()
运行上述代码后，将会绘制8张图，下面只放上其中几张图。
同理，如果你喜欢巴西队或者别的球队，也可以用上述代码进行分析。
http://liyangbit.com 430Chapter36. 项目实战：世界杯系列 Python数据之道
用上述函数可以快速的分析两支球队的历史胜负情况，当然，有些球队之间，相遇很少，或者近些年没有遭遇过，
那分析结果可能就不好用了。
当然，数据分析的只是历史情况，足球是圆的，场上瞬息万变。比如，阿根廷现在岌岌可危，梅西内心慌得一逼……
36.2.4 预测
本届世界杯真的是爆冷太多。
意大利，荷兰，连小组赛都没进；阿根廷，可以说现在已凉了半截；德国队，若不是最后的绝杀，也差不过可以送
首凉凉了，不过现在看已回血大半。
最后，来放上我的神预测。
黑马年年有，今年特别多，预测不准，坐等piapia打脸。
http://liyangbit.com 431Chapter36. 项目实战：世界杯系列 Python数据之道
当然，其实我内心深处希望是下面这样的。
怎么样，为强大的内心点赞吧~~
http://liyangbit.com 432Chapter36. 项目实战：世界杯系列 Python数据之道
特别说明：以上数据分析，纯属个人学习用，预测结果与实际情况可能偏差很大，不能用于其他用途。
如需获取本文源代码，请在公众号『Python数据之道』后台回复“code”
http://liyangbit.com 43337 项目实战：福布斯系列
37.1 数据分析思路
福布斯每年都会发布福布斯全球上市企业2000强排行榜（ForbesGlobal2000），这个排行榜每年发布的时候，
国内外总有新闻会热闹的讨论一番，但很少见到比较全面的分析。
因此才有了这样一个想法，搜集近些年每年发布的排行榜，做一个进一步的分析。
在准备做这个小小的项目前，先理了一下整个思路，大概可以分为下面这几个步骤：
1. 数据采集
2. 原始数据完整性检查
3. 数据清洗、整理
4. 从不同角度对数据进行分析
5. 数据可视化
6. 总结
整个分析过程会涉及多篇文章，主要使用Python来进行分析。
• 数据采集:主要涉及的python库包括requests，BeautifulSoup，csv，以及一些其他常用工具。
• 数据完整性检查: 包括不同数据来源的对比，以及其他一些常识性的知识。需要对比数据量的多少是否完整，以
及有些数据是否缺失。
当然，在拿到数据的初期，其实只能做一个初步的判断，有些内容是在整个分析过程中发现的。
• 数据清洗与整理: 主要用到Pandas、Numpy以及其他常用库和函数。由于数据比较杂乱，数据清洗与整理涉
及的内容比较多，可以说是整个福布斯系列的重点之一。
同时，这个也印证了通常我们所说的数据清洗与整理可能占整个分析的50~80%。
• 数据分析与可视化:经常是伴随在一起的。主要根据不同分析目的进行分析与可视化。用到的工具包括Pandas、
Numpy、Matplotlib、Seaborn以及其他一些相关库。
希望能通过福布斯系列的实战来对数据分析的知识点与工具作一个简单的示例整理与分享。
37.2 数据采集
37.2.1 数据采集概述
开始一个数据分析项目，首先需要做的就是get到原始数据，获得原始数据的方法有多种途径。比如：
434Chapter37. 项目实战：福布斯系列 Python数据之道
1. 获取数据集（dataset）文件
2. 使用爬虫采集数据
3. 直接获得excel、csv及其他数据文件
4. 其他途径…
本次福布斯系列数据分析项目实战，数据采集方面，主要数据来源于使用爬虫进行数据采集，同时也辅助其他数据
进行对比。
本节主要是介绍使用爬虫进行数据采集的思路和步骤。
本次采集的福布斯全球上市企业2000强排行榜数据，涉及年份从2007年到2017年，跨越10多年。
本次采集的目标网站，是多个网页，但多个网页的分布结构都有所不同，虽然思路和步骤都差不多，但需要分开来
编写，分别采集。
37.2.2 数据采集步骤
数据采集大体分为几步：
1. 目标主网页内容的Download
2. 主网页上数据的采集
3. 主网页上其他分发页面网站链接的采集
4. 各分发网页数据的download与采集
5. 将采集的数据保存
涉及到的python库包括，requests、BeautifulSoup以及csv。下面以采集某年的数据为案例，来描述下数据
采集的步骤。
1 import requests
2 from bs4 import BeautifulSoup
3 import csv
37.2.2.1 数据Download模块
主要是基于requests，代码如下：
1 def download(url):
2 headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64)
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari
/537.36'}
3 response = requests.get(url,headers=headers)
4 # print(response.status_code)
5 return response.text
这个模块会在主网页数据下载，以及各个分页面数据下载时使用，是一个比较通用的模块。
http://liyangbit.com 435Chapter37. 项目实战：福布斯系列 Python数据之道
37.2.2.2 主网页上数据的采集
主网页的页面结构，主要分为两个部分，一类是包含其他页面数据的网页链接，一类是主网页上的公司数据列表，
以表格形式在网页上显示。
用BeautifulSoup可以把这些数据解析出来。代码模块如下：
• 解析主网页上的公司数据列表信息
1 def get_content_first_page(html, year):
2 '''
3 获 取 排 名 在1-100的 公 司 列 表， 且 包 含 表 头
4 '''
5 soup = BeautifulSoup(html, 'lxml')
6 body = soup.body
7 body_content = body.find('div', {'id': 'bodyContent'})
8 tables = body_content.find_all('table', {'class': 'XXXXtable'})
9
10 # tables一 共 有3个， 最 后 一 个 才 是 我 们 想 要 的
11 trs = tables[-1].find_all('tr')
12
13 # 获 取 表 头 名 称
14 # trs[1], 这 里 跟 其 他 年 份 不 一 样
15 row_title = [item.text.strip() for item in trs[1].find_all('th')]
16 row_title.insert(0, '年 份')
17
18 rank_list = []
19 rank_list.append(row_title)
20 for i, tr in enumerate(trs):
21 if i == 0 or i == 1:
22 continue
23 tds = tr.find_all('td')
24
25 # 获 取 公 司 排 名 及 列 表
26 row = [ item.text.strip() for item in tds]
27 row.insert(0, year)
28 rank_list.append(row)
29 return rank_list
• 解析主网页上其他页面的网页链接
1 def get_page_urls(html):
2 '''
3 获 取 排 名 在101-2000的 公 司 的 网 页 链 接
4 '''
5 soup = BeautifulSoup(html, 'lxml')
6 body = soup.body
7 body_content = body.find('div', {'id': 'bodyContent'})
8 label_div = body_content.find('div', {'align':'center'})
9 label_a = label_div.find('p').find('b').find_all('a')
10
11 page_urls = ['basic_url' + item.get('href') for item in label_a]
http://liyangbit.com 436Chapter37. 项目实战：福布斯系列 Python数据之道
12 return page_urls
37.2.2.3 各个分发页面上的数据采集
步骤也是网页页面下载和表格类数据爬取。代码内容跟主网页页面类似，只是细节上有些差异，这里就不作赘述了。
37.2.2.4 数据存储
采集的数据，最后保存到csv文件中。模块代码如下：
1 def save_data_to_csv_file(data, file_name):
2 '''
3 保 存 数 据 到csv文 件 中
4 '''
5 with open(file_name, 'a', errors='ignore', newline='') as f:
6 f_csv = csv.writer(f)
7 f_csv.writerows(data)
37.2.2.5 数据采集主函数
1 def get_forbes_global_year_2007(year=2007):
2 url = 'url'
3 html = download(url)
4 # print(html)
5
6 data_first_page = get_content_first_page(html, year)
7 # print(data_first_page)
8 save_data_to_csv_file(data_first_page, 'forbes_'+str(year)+'.csv')
9
10 page_urls = get_page_urls(html)
11 # print(page_urls)
12
13 for url in page_urls:
14 html = download(url)
15 data_other_page = get_content_other_page(html, year)
16 # print(data_other_page)
17 print('saving data ...', url)
18 save_data_to_csv_file(data_other_page, 'forbes_'+str(year)+'.csv')
19
20
21 if __name__ == '__main__':
22
23 # get data from Forbes Global 2000 in Year 2009
24 get_forbes_global_year_2007()
http://liyangbit.com 437Chapter37. 项目实战：福布斯系列 Python数据之道
37.2.3 小结
本节只介绍了数据采集的思路与各个模块，并没有提供目标网页的链接，一方面由于原始网页的数据信息比较杂乱，
采集的时候需要写多个采集程序，另外一方面，由于我们的重点在于后续的数据分析部分，希望不要着重于数据爬取。
37.3 数据完整性检查
福布斯系列之数据完整性检查-Python数据分析项目实战
在上文中，介绍了使用python爬取福布斯全球上市企业2000强排行榜（ForbesGlobal2000）数据的思路
和步骤。
在获取数据后，首先需要对获得的数据的完整性情况进行检查，主要查看数据是否有缺失、是否有重复、是否有数
据错位或其他异常情况。
数据完整性检查也包括不同数据来源的对比，以及其他一些常识性的知识。当然，在拿到数据的初期，其实只能做
一个初步的判断，有些内容是在整个分析过程中发现的。
37.3.1 每年企业数量检查
福布斯全球上市企业2000强排行榜，企业数量应该是2000家，或者稍微多几家（因为排名可能相同，会增加少
量企业）。
所以首先需要检查这个排行榜上每年的企业数量，如果明显多余2000家或者少于2000家企业，则说明获得的数
据可能异常。
在使用Python分析时，可以用Pandas来进行检查，使用DataFrame.shape就可以查看企业数量。
下面以2007年的数据为例来介绍。代码如下：
1 df_2007 = pd.read_csv('./data/data_forbes_2007.csv', encoding='gbk', thousands=
',')
2 print('the shape of DataFrame: ', df_2007.shape)
3
4 out:
5 the shape of DataFrame: (2000, 9)
从运行结果可以看出，2007年的企业数量是2000家，是符合要求的。用同样的方法，逐个检查其他年份，发现
2013年和2015年的企业数量有异常。
通过查看，发现2013年的企业数量只有1991家，少于2000家。
1 df_2013 = pd.read_csv('./data/data_forbes_2013.csv', encoding='gbk',header=None
)
2 print('the shape of DataFrame: ', df_2013.shape)
3
4 out:
5 the shape of DataFrame: (1991, 9)
http://liyangbit.com 438Chapter37. 项目实战：福布斯系列 Python数据之道
同时，2015年的企业数量为2020家，明显多余2000家，可能有重复的内容。
1 df_2015 = pd.read_csv('./data/data_forbes_2015.csv', encoding='gbk',header=None
)
2 print('the shape of DataFrame: ', df_2015.shape)
3
4 out:
5 the shape of DataFrame: (2020, 9)
上述的这些问题，都需要在后续的数据清洗、分析中进一步处理。
37.3.2 数据错位、数据缺失等情况检查
通过查看csv数据文件以及目标网页等文件，发现有部分数据错位或缺失等情况，需要分别处理。
37.3.2.1 数据错位
通过检查发现，2012年和2014年的数据有错位的现象，通过手动在csv文件中进行调整相关数据，以便在后续
分析中继续使用这些数据。
2012年的情况
原始数据如下：
调整后的csv文件如下：
http://liyangbit.com 439Chapter37. 项目实战：福布斯系列 Python数据之道
2014年的情况
原始数据以及调整后的csv文件如下：
http://liyangbit.com 440Chapter37. 项目实战：福布斯系列 Python数据之道
37.3.2.2 数据缺失
通过检查发现，2016年的数据有缺失的现象，通过查找其他信息，发现其他网站有完整的信息。
对比网站的数据，进行了手动修改。用来对比的网站信息如下：
http://www.askci.com/news/finance/20160530/13364822511_7.shtml
通过手动在csv文件中进行调整相关数据，以便在后续分析中继续使用这些数据。
37.3.3 小结
本次数据完整性检查主要包括企业数量是否完整，以及相关数据是否缺失、错位等情况的检查。
当然，有可能还有其他问题暂时没有发现，在后续数据清洗以及分析过程中，若发现有相关问题，可以进一步处理。
http://liyangbit.com 441Chapter37. 项目实战：福布斯系列 Python数据之道
37.4 补充数据收集
福布斯系列之补充数据收集-Python数据分析项目实战
在之前分析中，提到2013年的企业数量只有1991家，少于2000家。数据可能存在缺失与不完整。
于是，继续来寻找其他可用的数据源。
37.4.1 使用爬虫获取数据
通过网站搜索，发现EconomyWatch网站有相关数据，且数据结构比较规范，质量较好，于是进行数据爬取。
网站地址如下：
• EconomyWatch：ForbesGlobal2000:China’sLargestCompanies
先打开网站，来查看数据情况，在这个页面上，我们关心的内容主要有两个部分。
1. 某个国家的数据列表，即数据表格。
2. 获得所有国家的信息列表的网页链接，方便后续从这些网页中爬取相关国家的数据。
这两部分在网站上对应的情况如下图所示：
http://liyangbit.com 442Chapter37. 项目实战：福布斯系列 Python数据之道
本次数据爬取大体分为几步：
1. 起始页面内容的爬取
2. 起始页面上其他国家的网站链接的采集
3. 其他国家的网页数据的download与企业列表数据信息采集
4. 将采集的数据保存
涉及到的python库主要包括requests、BeautifulSoup以及csv。
下面来描述下数据采集的步骤。
先引入相关python库：
1 # _*_ coding: utf-8 _*_
2
3 '''
4 # Code based on Python 3.x
5 # Author: ”LEMON”
6 # 微 信 公 众 号ID：PyDataLab
7 '''
8
9 import requests
10 from bs4 import BeautifulSoup
11 import csv
模块：页面Download
网站页面download页面是一个通用的模块，在所有国家的数据获取时都要用到。代码如下：
1 def download(url):
2 headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64)
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari
/537.36'}
3 response = requests.get(url,headers=headers)
http://liyangbit.com 443Chapter37. 项目实战：福布斯系列 Python数据之道
4 # print(response.status_code)
5 return response.text
模块：企业表格信息
在分析完网页结构之后，首先我们来获取起始页面上的信息，分别包括国家的企业信息数据和其他国家的网页链接
信息。
可以分为两个模块来实现这些信息的获取。
国家的企业列表表格上的信息获取的模块代码如下：
1 def get_content_first_page(html, country='China'):
2 '''
3 获 取China的 公 司 列 表， 且 包 含 表 头
4 '''
5 soup = BeautifulSoup(html, 'lxml')
6 body = soup.body
7 label_div = body.find('div', {'class': 'content post'})
8 tables = label_div.find_all('table')
9
10 # tables一 共 有2个， 第 一 个 才 是 我 们 想 要 的 数 据
11 trs = tables[0].find_all('tr')
12
13 # 获 取 标 题 行 信 息
14 row_title = [td.text for td in trs[0].find_all('td')]
15 row_title.insert(2, 'Country')
16
17 data_list = []
18 data_list.append(row_title)
19 for i, tr in enumerate(trs):
20 if i == 0:
21 continue
22 tds = tr.find_all('td')
23
24 # 获 取 公 司 排 名 及 列 表
25 row = [ item.text.strip() for item in tds]
26 row.insert(2, country)
27 data_list.append(row)
28 return data_list
模块：其他国家的网页链接
从起始页面上获取其他国家的网页链接信息的模块如下：
1 def get_country_info(html):
2 '''
3 获 取 除China外 其 他 国 家 的 网 页 链 接 及 国 家 名 称
4 '''
5 soup = BeautifulSoup(html, 'lxml')
6 body = soup.body
7 label_div = body.find('div', {'class': 'content post'})
http://liyangbit.com 444Chapter37. 项目实战：福布斯系列 Python数据之道
8 tables = label_div.find_all('table')
9
10 label_a = tables[1].find_all('a')
11 country_names = [item.text for item in label_a]
12 page_urls = [ item.get('href') for item in label_a]
13 country_info = list(zip(country_names, page_urls))
14 return country_info
模块：其他国家的企业表格信息
对于其他国家而言，只需要在其对应的页面上获取企业列表信息即可，代码内容跟起始页面上的企业信息获取代码
基本是一样的。
有一点差异，就是不需要再获取表格的标题信息了。代码就不作赘述了。
模块：信息保存
由于信息量不大，且结构相对比较简单，保存到csv文件即可。
1 def save_data_to_csv_file(data, file_name):
2 '''
3 保 存 数 据 到csv文 件 中
4 '''
5 with open(file_name, 'a', errors='ignore', newline='') as f:
6 f_csv = csv.writer(f)
7 f_csv.writerows(data)
编写网页爬取主函数文件
在各个模块编写好后，就可以在主函数里进行调用，从而爬取数据。如下：
1 def get_forbes_global_year_2013():
2 url = 'http://www.economywatch.com/companies/forbes-list/china.html'
3 html = download(url)
4 # print(html)
5
6 data_first_page = get_content_first_page(html)
7 # print(data_first_page)
8 print('saving data ...', 'China')
9 save_data_to_csv_file(data_first_page, 'forbes_economywatch_2013.csv')
10
11 country_info = get_country_info(html)
12 # print(country_info)
13
14 for item in country_info:
15 country_name = item[0]
16 country_url = item[1]
17
18 if country_name == 'China':
19 continue
20
21 html = download(country_url)
http://liyangbit.com 445Chapter37. 项目实战：福布斯系列 Python数据之道
22 data_other_country = get_content_other_country(html, country_name)
23 # print(data_other_country)
24 print('saving data ...', country_name)
25 save_data_to_csv_file(data_other_country, 'forbes_economywatch_2013.csv
')
26
27
28 if __name__ == '__main__':
29
30 # get data from Forbes Global 2000 in Year 2013
31 get_forbes_global_year_2013()
将数据爬取下来后，查看数据是否完整，通过pandas查看，发现EconomyWatch上，2013年的企业数量只
有1984家，数据也不完整。
需要继续查找其他可靠的数据。
37.4.2 其他数据源收集
后来，在网站找到一个excel文件，发现其企业数量是2000家，初步认为数据是完整的。
在后续分析过程中，将会使用这个excel文件的企业数据信息。
所以说，数据收集的途径很多，关键是数据的质量要好，数据要完整，尽量少的出现异常情况。
至此，福布斯系列文章已经发布了多篇文章，接下来要进入数据清洗的深水区，欢迎大家持续关注~~
37.5 数据清洗-2007年数据
福布斯系列之数据清洗（1）-Python数据分析项目实战
37.5.1 前言
本文作为数据清洗的第一篇，将详细的描述福布斯全球上市企业2000强排行榜数据中2007年数据的处理过程。
通常，我们都希望自己拿到的数据是比较规范，看起来令人清爽的数据。然而，实际上我们获得的信息或数据，会
因为各种各样的原因，存在数据缺失、不准确、不规范等异常情况。需要我们在进行数据分析之前进行预处理。
福布斯全球上市企业2000强排行榜数据，从2007年到2017年，各个年份的数据都存在一些不规范的地方。从
本文开始至接下来的几篇文章，将通过福布斯项目来进行数据清洗的项目实战，希望能给大家带来些收获。
本项目运行环境：
• windows7
• jupyternotebook
http://liyangbit.com 446Chapter37. 项目实战：福布斯系列 Python数据之道
37.5.2 数据清洗的目的
将不规范的数据进行处理，包括：
1. 替换NaN值
2. 将字符串型数字转换为数字类型
3. 将数字后面多余的字母等文字剔除
4. 将公司和国家进行拆分
5. 将国家名称进行统一
6. 按列名将DataFrame重新排序
最终达到的效果如下：
数据清洗前
数据类型
1 年 份 int64
2 排 名(Rank) int64
3 公 司 名 称(Company) object
4 所 在 国 家 或 地 区(Country) object
5 所 在 行 业(Industry) object
6 销 售 收 入(Sales) object
7 利 润(Profits) object
8 总 资 产(Assets) object
9 市 值(Market Vaue) float64
10 dtype: object
数据清洗后
数据类型
1 Year int64
2 Rank int64
3 Company_cn_en object
4 Company_en object
5 Company_cn object
6 Country_cn_en object
7 Country_cn object
8 Country_en object
9 Industry_cn object
10 Industry_en object
11 Sales float64
12 Profits float64
13 Assets float64
14 Market_value float64
15 dtype: object
http://liyangbit.com 447Chapter37. 项目实战：福布斯系列 Python数据之道
数据清洗后，sales、Profits、Assets及Market_value列的数据均为数字类型，方便后续分析时计算使用。
37.5.3 数据清洗的详细过程
1 import pandas as pd
2 import numpy as np
2007年的数据，原始数据的单位为十亿美元。
37.5.3.1 先查看数据的类型（dtype）和结构
1 df_2007 = pd.read_csv('./data/data_forbes_2007.csv', encoding='gbk', thousands=
',')
2 print('the shape of DataFrame: ', df_2007.shape)
3 print(df_2007.dtypes)
4 df_2007.head(3)
5
6 out:
7 the shape of DataFrame: (2000, 9)
8 年 份 int64
9 排 名(Rank) int64
10 公 司 名 称(Company) object
11 所 在 国 家 或 地 区(Country) object
12 所 在 行 业(Industry) object
13 销 售 收 入(Sales) object
14 利 润(Profits) object
15 总 资 产(Assets) object
16 市 值(Market Vaue) float64
17 dtype: object
更新columns的命名
1 column_update = ['Year', 'Rank', 'Company_cn_en', 'Country_cn_en',
2 'Industry_cn', 'Sales', 'Profits', 'Assets', 'Market_value']
3 df_2007.columns = column_update
http://liyangbit.com 448Chapter37. 项目实战：福布斯系列 Python数据之道
37.5.3.2 将字符串转换为数字类型
通过前面的分析可看出，只有“Market_value”是数字类型，找出’Sales’,’Profits’及’Assets’中非数字的内容
1 df_2007[df_2007['Sales'].str.contains('.*[A-Za-z]', regex=True)]
用replace()方法替换“Sales”列中含有字母的内容
1 df_2007['Sales'] = df_2007['Sales'].replace('([A-Za-z])', '', regex=True)
查看替换后的结果
1 df_2007.loc[[117,616,880], :]
查看“Assets”列中非数字的内容
1 df_2007[df_2007['Assets'].str.contains('.*[A-Za-z]', regex=True)]
替换非数字的内容，以及替换千分位间隔符号
1 # 将 数 字 后 面 的 字 母 进 行 替 换
2 df_2007['Assets'] = df_2007['Assets'].replace('([A-Za-z])', '', regex=True)
3
4 # 千 分 位 数 字 的 逗 号 被 识 别 为string了， 需 要 替 换
5 df_2007['Assets'] = df_2007['Assets'].replace(',', '', regex=True)
6 df_2007.loc[616, :]
7
8 out:
9 Year 2007
10 Rank 617
11 Company_cn_en Inpex Holdings
12 Country_cn_en 日 本(JA)
13 Industry_cn 炼 油
14 Sales 6.49
15 Profits 1.02 E
16 Assets 10.77
17 Market_value 19.65
http://liyangbit.com 449Chapter37. 项目实战：福布斯系列 Python数据之道
18 Name: 616, dtype: object
发现“Profits”中有NaN值，需要先进行替换
1 df_2007[pd.isnull(df_2007['Profits'])]
2
3 # 将NaN值 替 换 为0
4 df_2007['Profits'].fillna(0, inplace=True)
5 df_2007.loc[[958,1440,1544,1912], :]
将“Profits”列中非数字的内容进行替换，并查看替换后的结果
1 df_2007['Profits'] = df_2007['Profits'].replace('([A-Za-z])', '', regex=True)
2 df_2007.loc[[117,616,880], :]
将string类型的数字转换为数据类型，这里使用pd.to_numeric()方法
1 df_2007['Sales'] = pd.to_numeric(df_2007['Sales'])
2 df_2007['Profits'] = pd.to_numeric(df_2007['Profits'])
3 df_2007['Assets'] = pd.to_numeric(df_2007['Assets'])
4 df_2007.dtypes
5
6 out:
7 Year int64
8 Rank int64
9 Company_cn_en object
10 Country_cn_en object
11 Industry_cn object
http://liyangbit.com 450Chapter37. 项目实战：福布斯系列 Python数据之道
12 Sales float64
13 Profits float64
14 Assets float64
15 Market_value float64
16 dtype: object
37.5.3.3 拆分“Company_cn_en”列
新生成两列，分别为公司英文名称和中文名称
1 df_2007['Company_en'],df_2007['Company_cn'] = df_2007['Company_cn_en'].str.
split('/', 1).str
2 print(df_2007['Company_en'][:5])
3 print(df_2007['Company_cn'] [-5:])
4 df_2007.tail(3)
5
6 out:
7 0 Citigroup
8 1 Bank of America
9 2 HSBC Holdings
10 3 General Electric
11 4 JPMorgan Chase
12 Name: Company_en, dtype: object
13 1995 NaN
14 1996 NaN
15 1997 NaN
16 1998 NaN
17 1999 NaN
18 Name: Company_cn, dtype: object
37.5.3.4 拆分“Country_cn_en”列
新生成两列，分别为国家中文名称和英文名称
1 df_2007['Country_cn'],df_2007['Country_en'] = df_2007['Country_cn_en'].str.
split('(', 1).str
2 print(df_2007['Country_cn'][:5])
3 print(df_2007['Country_en'][-5:])
4
5 out:
6 0 美 国
7 1 美 国
8 2 英 国
9 3 美 国
http://liyangbit.com 451Chapter37. 项目实战：福布斯系列 Python数据之道
10 4 美 国
11 Name: Country_cn, dtype: object
12 1995 US)
13 1996 US)
14 1997 US)
15 1998 SI)
16 1999 GE)
17 Name: Country_en, dtype: object
由于国家的英文名称中，最后有半个括号，需要去除，用Series.str.slice()方法。参数表示选取从开始到倒数第
二个，即不要括号“)”
1 df_2007['Country_en'] = df_2007['Country_en'].str.slice(0,-1)
2 df_2007.head(3)
考虑的中国的企业有区分为中国大陆，中国香港，中国台湾。对应的国家英文名称也需要修改下。
• 中国大陆：CN；
• 中国香港：CN-HK；
• 中国台湾：CN-TA。
1 # 查 找 含 “中 国” 的 数 据
2 df_2007[df_2007['Country_cn'].str.contains('中 国',regex=True)]
3
4 # 替 换 并 查 看 结 果
5 df_2007['Country_en'] = df_2007['Country_en'].replace(['HK.*','TA'],['CN-HK', '
CN-TA'],regex=True)
6 df_2007[df_2007['Country_en'].str.contains('CN',regex=True)]
考虑到其他年份，公司所在行业有用英文名称展示的，这里添加一列英文的行业名称，但内容是空白。
1 df_2007['Industry_en'] = ''
2 df_2007.tail(3)
http://liyangbit.com 452Chapter37. 项目实战：福布斯系列 Python数据之道
37.5.3.5 将列名进行重新排序
1 columns_sort = ['Year', 'Rank', 'Company_cn_en','Company_en',
2 'Company_cn', 'Country_cn_en', 'Country_cn',
3 'Country_en', 'Industry_cn', 'Industry_en',
4 'Sales', 'Profits', 'Assets', 'Market_value']
5
6 # 按 指 定list重 新 将columns进 行 排 序
7 df_2007 = df_2007.reindex(columns=columns_sort)
8 print(df_2007.shape)
9 print(df_2007.dtypes)
10 df_2007.head(3)
11
12 out:
13 (2000, 14)
14 Year int64
15 Rank int64
16 Company_cn_en object
17 Company_en object
18 Company_cn object
19 Country_cn_en object
20 Country_cn object
21 Country_en object
22 Industry_cn object
23 Industry_en object
24 Sales float64
25 Profits float64
26 Assets float64
27 Market_value float64
28 dtype: object
37.5.4 小结
至此，将2007年的原始数据进行了初步处理，当然，在后续的分析过程中，可能还会有少量的处理。经过数据清
洗后，获得的数据信息相对来说比较规范，数据的质量较好，可以用来后续分析使用或者进一步清洗。
数据清洗是数据分析、数据挖掘中一项基本且非常重要的技能，通过在整个数据分析中所用时间的比例会比较高。
当然，本次数据清洗的过程，还有可以优化的地方，各位同学可以自行研究下。
37.6 数据清洗-2008-2010年
福布斯系列之数据清洗（2）-Python数据分析项目实战
http://liyangbit.com 453Chapter37. 项目实战：福布斯系列 Python数据之道
37.6.1 前言
本文作为数据清洗的第二篇，内容包含福布斯全球上市企业2000强排行榜数据中2008-2010年数据的初步处
理过程。
福布斯全球上市企业2000强排行榜数据，从2007年到2017年，各个年份的数据都存在一些不规范的地方。
本文以2008年为例，描述2008年至2010年的数据清洗过程。
本项目运行环境：
• windows7
• jupyternotebook
37.6.2 数据清洗的目的
将不规范的数据进行处理，包括：
1. 替换NaN值
2. 将字符串型数字转换为数字类型
3. 将数字后面多余的字母等文字剔除
4. 将公司和国家进行拆分
5. 按列名将DataFrame重新排序
最终达到的效果如下：
数据清洗前
数据类型
1 the shape of DataFrame: (2000, 10)
2 年 份 int64
3 Rank int64
4 公 司 名 称 （英 文） object
5 公 司 名 称 （中 文） object
6 Country/area（国 家 或 地 区） object
7 Industry（行 业） object
8 Sales （销 售 额）($bil十 亿 美 元) object
9 Profits （利 润）($bil) object
10 Assets 资 产($bil) object
11 Market Value 市 值($bil) float64
12 dtype: object
数据清洗后
数据类型
http://liyangbit.com 454Chapter37. 项目实战：福布斯系列 Python数据之道
1 Year int64
2 Rank int64
3 Company_cn_en object
4 Company_en object
5 Company_cn object
6 Country_cn_en object
7 Country_cn object
8 Country_en object
9 Industry_cn object
10 Industry_en object
11 Sales float64
12 Profits float64
13 Assets float64
14 Market_value float64
数据清洗后，sales、Profits、Assets及Market_value列的数据均为数字类型，方便后续分析时计算使用。
37.6.3 2008年数据清洗的详细过程
导入相关python库
1 import pandas as pd
2 import numpy as np
从csv文件中读取数据
1 df_2008 = pd.read_csv('./data/data_forbes_2008.csv', encoding='gbk', thousands=
',')
2 print('the shape of DataFrame: ', df_2008.shape)
3 print(df_2008.dtypes)
4 df_2008.head()
更新列名
1 df_2008.columns = ['Year', 'Rank', 'Company_en', 'Company_cn','Country_en', '
Industry_en', 'Sales', 'Profits', 'Assets', 'Market_value']
2 df_2008.head()
添加空白列，使之与其他年份的格式保持一致
1 df_2008['Company_cn_en'], df_2008['Country_cn_en'], df_2008['Country_cn'],
df_2008['Industry_cn'] = ['','','','']
2 df_2008.head()
将字符串转换为数字类型
http://liyangbit.com 455Chapter37. 项目实战：福布斯系列 Python数据之道
在前文2007年的数据清洗过程中，单独针对’Sales’,’Profits’及’Assets’进行处理。
本文中，对处理过程进行了一些优化，通过for循环，一次性处理不同的字段，大大减少了代码量，以及程序的繁
冗程度，使整个过程变得更简洁。
处理过程如下：
1 col_digit = ['Sales', 'Profits', 'Assets', 'Market_value']
2
3 for col in col_digit:
4 # 将 数 字 后 面 的 字 母 进 行 替 换
5 df_2008[col] = df_2008[col].replace('([A-Za-z])', '', regex=True)
6
7 # 千 分 位 数 字 的 逗 号 被 识 别 为string了， 需 要 替 换
8 df_2008[col] = df_2008[col].replace(',', '', regex=True)
9
10 #将 数 字 型 字 符 串 转 换 为 可 进 行 计 算 的 数 据 类 型
11 df_2008[col] = pd.to_numeric(df_2008[col])
按指定list重新将columns进行排序
1 # 按 指 定list重 新 将columns进 行 排 序
2 columns_sort = ['Year', 'Rank', 'Company_cn_en','Company_en',
3 'Company_cn', 'Country_cn_en', 'Country_cn',
4 'Country_en', 'Industry_cn', 'Industry_en',
5 'Sales', 'Profits', 'Assets', 'Market_value']
6
7 df_2008 = df_2008.reindex(columns=columns_sort)
8 print(df_2008.shape)
9 print(df_2008.dtypes)
10 df_2008.head()
37.6.4 后续
2009年和2010年的数据清洗过程跟2008年基本一致，处理过程就不在本文中描述了（提供源代码供参考）。
细心的同学可能会发现，本次没有提到针对国家数据的规整，因为国家数据的规整将统一到最后进行处理，敬请后
续关注。
37.7 数据清洗-2011-2015年
福布斯系列之数据清洗（3）-Python数据分析项目实战
37.7.1 前言
本文作为数据清洗的第三篇，内容包含福布斯全球上市企业2000强排行榜数据中2011-2015年数据的初步处
理过程。
http://liyangbit.com 456Chapter37. 项目实战：福布斯系列 Python数据之道
福布斯全球上市企业2000强排行榜数据，从2007年到2017年，各个年份的数据都存在一些不规范的地方。
本文以2013年为例，描述2011年至2015年的数据清洗过程。
本项目运行环境：
• windows7
• jupyternotebook
• Python3.5
37.7.2 数据清洗的目的
将不规范的数据进行处理，包括：
1. 检查数据完整性
2. 去除重复内容
3. 替换NaN值
4. 将字符串型数字转换为数字类型
5. 将数字后面多余的字母等文字剔除
6. 将公司和国家进行拆分
7. 按列名将DataFrame重新排序
最终达到的效果如下：
数据清洗后
数据类型
1 Year int64
2 Rank int64
3 Company_cn_en object
4 Company_en object
5 Company_cn object
6 Country_cn_en object
7 Country_cn object
8 Country_en object
9 Industry_cn object
10 Industry_en object
11 Sales float64
12 Profits float64
13 Assets float64
14 Market_value float64
15 dtype: object
http://liyangbit.com 457Chapter37. 项目实战：福布斯系列 Python数据之道
37.7.3 2013年数据清洗的详细过程
导入相关python库
1 import pandas as pd
2 import numpy as np
2013年的数据，由于初始爬取的数据不完整，后续又查找了另外2个数据源，所以共有3个数据源，下面分别介
绍。
37.7.3.1 数据源1
从csv文件中读取数据
1 df_2013 = pd.read_csv('./data/data_forbes_2013.csv', encoding='gbk',header=None
)
2 print('the shape of DataFrame: ', df_2013.shape)
3 print(df_2013.dtypes)
4 df_2013.head()
5
6 out:
7 the shape of DataFrame: (1991, 9)
数据源1中，2013年的数据只有1991条记录，数据可能有缺失，待进一步核实。
37.7.3.2 数据源2
2013年的数据在网上继续寻找，发现EconomyWatch网站有相关数据，于是进行数据爬取。前文已描述了从
该网站获取数据的过程。
从csv文件中读取数据
1 df_2013_economy = pd.read_csv('./data/data_forbes_2013_economywatch.csv',
encoding='gbk')
2 print('the shape of DataFrame: ', df_2013_economy.shape)
3 print(df_2013_economy.dtypes)
4 df_2013_economy.head()
5
6 out:
7 the shape of DataFrame: (1984, 7)
发现数据只有1984条记录，也缺少相关记录，需要继续寻找其他记录。
37.7.3.3 数据源3
后来，找到一个excel文件，发现其数据记录是完整的，于是用该文件。
首先，从csv文件中读取数据
http://liyangbit.com 458Chapter37. 项目实战：福布斯系列 Python数据之道
1 df_2013_all = pd.read_excel('./data/data_forbes_2013_all.xlsx')
2 print('the shape of DataFrame: ', df_2013_all.shape)
3 print(df_2013_all.dtypes)
4 df_2013_all.head()
5
6 out:
7 the shape of DataFrame: (2000, 7)
8 排 名 int64
9 公 司 名 object
10 国 家 （地 区） object
11 销 售 额 （亿 美 元） int64
12 利 润 （亿 美 元） int64
13 资 产 （亿 美 元） int64
14 市 值 （亿 美 元） int64
15 dtype: object
这里在前面两篇数据清洗文章的基础上，将数据清洗的过程整合到一起了。处理过程如下：
1 # 更 新 列 名
2 df_2013_all.columns = ['Rank', 'Company_cn_en',
3 'Country_cn', 'Sales', 'Profits', 'Assets', 'Market_value']
4
5 # 拆 分”Company_cn_en”列， 新 生 成 两 列， 分 别 为 公 司 英 文 名 称 和 中 文 名 称
6 df_2013_all['Company_cn'],df_2013_all['Company_en'] = df_2013_all['
Company_cn_en'].str.split('/', 1).str
7 # print(df_2013_all['Company_cn'][:5])
8 # print(df_2013_all['Company_en'] [-5:])
9
10 # 将 数 据 单 位 转 换 成 十 亿 美 元
11 df_2013_all[['Sales','Profits','Assets','Market_value']] =df_2013_all[['Sales',
'Profits','Assets','Market_value']].apply(lambda x: x/10)
12
13 # 添 加 年 份2013
14 df_2013_all['Year'] = 2013
15
16 # 添 加 空 白 列
17 df_2013_all['Country_en'],df_2013_all['Country_cn_en'], df_2013_all['
Industry_cn'], df_2013_all['Industry_en'] = ['','','','']
18
19 df_2013_all['Rank'] = pd.to_numeric(df_2013_all['Rank'])
20
21 # 按 指 定list重 新 将columns进 行 排 序
22 df_2013_all = df_2013_all.reindex(columns=columns_sort)
23 print(df_2013_all.shape)
24 print(df_2013_all.dtypes)
25 df_2013_all.head()
请注意，2013年的数据，sales、Profits、Assets及Market_value列的本身就是为数字类型，不需要进一步
处理。
http://liyangbit.com 459Chapter37. 项目实战：福布斯系列 Python数据之道
37.7.4 2015年的个别情况
在前文中提到，2015年的企业数量有重复的，因此，在数据处理过程中，需要剔除重复的内容。
去重的代码如下：
1 # 数 据 有2020行， 有 重 复 行， 需 要 去 除 重 复 行
2 # inplace=True， 使 去 重 生 效
3 df_2015.drop_duplicates('Company_cn_en', inplace=True)
查看是否还有重复的内容：
1 # 查 看'Company_cn_en'是 否 还 有 重 复 行
2 df_2015[df_2015['Company_cn_en'].duplicated()]
通过查看结果可知，已无重复内容。
2015年的其他处理过程与2013年类似，就不做进一步描述了。
37.7.5 小结
2011年至2015年的数据，相互之间的相似度较高，数据清洗与处理过程和2013年的基本类似，具体过程就不再
详细描述了（提供源代码供参考）。
2011年-2015年的数据，Sales、Profits、Assets及Market_value列的本身就是数字类型，可以在后续分
析时计算使用，所以不需要像2008-2010年的数据那样处理。
细心的同学可能会发现，本次依然没有提到针对国家数据的规整，因为国家数据的规整将统一到最后进行处理，敬
请后续关注。
37.8 数据清洗-2016年
Pandas数据处理实战：福布斯全球上市企业排行榜数据整理
本文通过实例操作来介绍用pandas进行数据整理。
先说下我的运行环境，如下：
• windows7，64位
• python3.5
• pandas0.19.2版本
在拿到原始数据后，我们先来看看数据的情况，并思考下我们需要什么样的数据结果。
下面是原始数据：
http://liyangbit.com 460Chapter37. 项目实战：福布斯系列 Python数据之道
在本文中，我们需要以下的初步结果，以供以后继续使用。
可以看到，原始数据中，跟企业相关的数据中（“Sales”，“Profits”，“Assets”，“Market_value”），目前
都是不是可以用来计算的数字类型。
原始内容中包含货币符号”$“，“-”，纯字母组成的字符串以及其他一些我们认为异常的信息。更重要的是，这些数
据的单位并不一致。分别有以“B”（Billion，十亿）和“M”（Million，百万）表示的。在后续计算之前需要进行单位统一。
37.8.1 处理方法Method-1
首先想到的处理思路就是将数据信息分别按十亿（“B”）和百万(“M”)进行拆分，分别进行处理，最后在合并到一
起。过程如下所示。
• 加载数据，并添加列的名称
http://liyangbit.com 461Chapter37. 项目实战：福布斯系列 Python数据之道
1 import pandas as pd
2
3 df_2016 = pd.read_csv('data_2016.csv', encoding='gbk',header=None)
4
5 # 更 新 列 名
6 df_2016.columns = ['Year', 'Rank', 'Company_cn','Company_en',
7 'Country_en', 'Sales', 'Profits', 'Assets', 'Market_value']
8
9 print('the shape of DataFrame: ', df_2016.shape)
10 print(df_2016.dtypes)
11 df_2016.head(3)
• 获取单位为十亿（“B”）的数据
1 # 数 据 单 位 为 B的 数 据 （Billion， 十 亿）
2 df_2016_b = df_2016[df_2016['Sales'].str.endswith('B')]
3 print(df_2016_b.shape)
4 df_2016_b
• 获取单位为百万(“M”)的数据
1 # 数 据 单 位 为 M的 数 据 （Million， 百 万）
2 df_2016_m = df_2016[df_2016['Sales'].str.endswith('M')]
3 print(df_2016_m.shape)
4 df_2016_m
这种方法理解起来比较简单，但操作起来会比较繁琐，尤其是如果有很多列数据需要处理的话，会花费很多时间。
进一步的处理，我这里就不描述了。当然，各位可以试试这个方法。
下面介绍稍微简单一点的方法。
37.8.2 处理方法Method-2
37.8.2.1 加载数据
第一步还是加载数据，跟Method-1是一样的。
下面来处理’Sales’列
37.8.2.2 替换相关的异常字符
首先是替换相关的异常字符，包括美元的货币符号’$“，纯字母的字符串’undefined”，以及’B’。这里，我们想统
一把数据的单位整理成十亿，所以’B’可以直接进行替换。而’M’需要更多的处理步骤。
37.8.2.3 处理’M’相关的数据
处理含有百万“M”为单位的数据，即以“M”结尾的数据，思路如下：
http://liyangbit.com 462Chapter37. 项目实战：福布斯系列 Python数据之道
（1）设定查找条件mask；
（2）替换字符串“M”为空值
（3）用pd.to_numeric()转换为数字
（4）除以1000，转换为十亿美元，与其他行的数据一致
上面两个步骤相关的代码如下：
1 # 替 换 美 元 符 号
2 df_2016['Sales'] = df_2016['Sales'].str.replace('$','')
3
4 # # 查 看 异 常 值， 均 为 字 母 （“undefined”）
5 # df_2016[df_2016['Sales'].str.isalpha()]
6
7 # 替 换 异 常 值 “undefined” 为 空 白
8 # df_2016['Sales'] = df_2016['Sales'].str.replace('undefined','')
9 df_2016['Sales'] = df_2016['Sales'].str.replace('^[A-Za-z]+$','')
10
11 # 替 换 符 号 十 亿 美 元 “B” 为 空 白， 数 字 本 身 代 表 的 就 是 十 亿 美 元 为 单 位
12 df_2016['Sales'] = df_2016['Sales'].str.replace('B','')
13
14
15 # 处 理 含 有 百 万 “M” 为 单 位 的 数 据， 即 以 “M” 结 尾 的 数 据
16 # 思 路：
17 # （1） 设 定 查 找 条 件mask；
18 # （2） 替 换 字 符 串 “M” 为 空 值
19 # （3） 用pd.to_numeric()转 换 为 数 字
20 # （4） 除 以1000， 转 换 为 十 亿 美 元， 与 其 他 行 的 数 据 一 致
21 mask = df_2016['Sales'].str.endswith('M')
22 df_2016.loc[mask, 'Sales'] = pd.to_numeric(df_2016.loc[mask, 'Sales'].str.
replace('M', ''))/1000
23
24 df_2016['Sales'] = pd.to_numeric(df_2016['Sales'])
25 print('the shape of DataFrame: ', df_2016.shape)
26 print(df_2016.dtypes)
27 df_2016.head(3)
用同样类似的方法处理其他列
可以看到，这个方法比第一种方法还是要方便很多。当然，这个方法针对DataFrame的每列数据都要进行相关的
操作，如果列数多了，也还是比较繁琐的。
有没有更方便一点的方法呢。答案是有的。
37.8.3 处理方法Method-3
在Method-2的基础上，将处理方法写成更通用的数据处理函数，根据数据的结构，拓展更多的适用性，则可以
比较方便的处理相关数据。
http://liyangbit.com 463Chapter37. 项目实战：福布斯系列 Python数据之道
37.8.3.1 加载数据
第一步还是加载数据，跟Method-1是一样的。
37.8.3.2 编写数据处理的自定义函数
参考Method-2的处理过程，编写数据处理的自定义函数’pro_col’，并在Method-2的基础上拓展其他替换功
能，使之适用于这四列数据（“Sales”，“Profits”，“Assets”，“Market_value”）。
函数编写的代码如下：
1 def pro_col(df, col):
2 # 替 换 相 关 字 符 串， 如 有 更 多 的 替 换 情 形， 可 以 自 行 添 加
3 df[col] = df[col].str.replace('$','')
4 df[col] = df[col].str.replace('^[A-Za-z]+$','')
5 df[col] = df[col].str.replace('B','')
6
7 # 注 意 这 里 是'-$'， 即 以'-'结 尾， 而 不 是'-'， 因 为 有 负 数
8 df[col] = df[col].str.replace('-$','')
9 df[col] = df[col].str.replace(',','')
10
11 # 处 理 含 有 百 万 “M” 为 单 位 的 数 据， 即 以 “M” 结 尾 的 数 据
12 # 思 路：
13 # （1） 设 定 查 找 条 件mask；
14 # （2） 替 换 字 符 串 “M” 为 空 值
15 # （3） 用pd.to_numeric()转 换 为 数 字
16 # （4） 除 以1000， 转 换 为 十 亿 美 元， 与 其 他 行 的 数 据 一 致
17 mask = df[col].str.endswith('M')
18 df.loc[mask, col] = pd.to_numeric(df.loc[mask, col].str.replace('M',''))
/1000
19
20 # 将 字 符 型 的 数 字 转 换 为 数 字 类 型
21 df[col] = pd.to_numeric(df[col])
22 return df
37.8.3.3 将自定义函数进行应用
针对DataFrame的每列，应用该自定义函数，进行数据处理，得到需要的结果。
1 pro_col(df_2016, 'Sales')
2 pro_col(df_2016, 'Profits')
3 pro_col(df_2016, 'Assets')
4 pro_col(df_2016, 'Market_value')
5
6 print('the shape of DataFrame: ', df_2016.shape)
7 print(df_2016.dtypes)
8 df_2016.head()
当然，如果DataFrame的列数特别多，可以用for循环，这样代码更简洁。代码如下：
http://liyangbit.com 464Chapter37. 项目实战：福布斯系列 Python数据之道
1 cols = ['Sales', 'Profits', 'Assets', 'Market_value']
2 for col in cols:
3 pro_col(df_2016, col)
4
5 print('the shape of DataFrame: ', df_2016.shape)
6 print(df_2016.dtypes)
7 df_2016.head()
最终处理后，获得的数据结果如下：
37.9 数据清洗-2017年
福布斯系列之数据清洗（5）-Python数据分析项目实战
37.9.1 前言
本文作为数据清洗的第五篇，内容包含：
（1）2016-2017年数据的初步处理过程。
（2）将2007-2017年数据合并，并统一国家名称
本项目运行环境：
• windows7
• jupyternotebook
• Python3.5
http://liyangbit.com 465Chapter37. 项目实战：福布斯系列 Python数据之道
37.9.2 2016-2017年数据处理
2016年数据的清洗，前文已做详细描述。
2017年的数据信息，跟2016年基本一致。具体过程就不描述了。代码如下：
1 df_2017 = pd.read_csv('./data/data_forbes_2017.csv', encoding='gbk')
2
3 # 更 新 列 名
4 df_2017.columns = ['Year', 'Rank', 'Company_cn','Company_en',
5 'Country_en', 'Sales', 'Profits', 'Assets', 'Market_value']
6 print('the shape of DataFrame: ', df_2017.shape)
7 df_2017.head()
8
9 cols = ['Sales', 'Profits', 'Assets', 'Market_value']
10 for col in cols:
11 pro_col(df_2017, col)
12 print('the shape of DataFrame: ', df_2017.shape)
13 print(df_2017.dtypes)
14 df_2017.head()
15
16 # 添 加 空 白 列
17 df_2017['Company_cn_en'],df_2017['Country_cn_en'], df_2017['Country_cn'],
df_2017['Industry_cn'], df_2017['Industry_en'] = ['','','','','']
18
19 # 按 指 定list重 新 将columns进 行 排 序
20 columns_sort = ['Year', 'Rank', 'Company_cn_en','Company_en',
21 'Company_cn', 'Country_cn_en', 'Country_cn',
22 'Country_en', 'Industry_cn', 'Industry_en',
23 'Sales', 'Profits', 'Assets', 'Market_value']
24 df_2017 = df_2017.reindex(columns=columns_sort)
25 print(df_2017.shape)
26 df_2017.head()
其中，函数pro_col()来自2016年处理过程的函数。
37.9.3 2007-2017年数据的合并
用pandas的concat()方法，将2007年至2017年的数据进行合并，统一到一个Dataframe中。
当然，也可以保存到csv文件中，以便后续分析时使用。
1 df_concat = pd.concat([df_2007, df_2008, df_2009,
2 df_2010, df_2011, df_2012,
3 df_2013_all, df_2014,
4 df_2015, df_2016, df_2017], ignore_index=True)
5 # 保 存 数 据
6 # df_concat.to_csv('data_forbes_concat.csv')
7 df_concat.head()
http://liyangbit.com 466Chapter37. 项目实战：福布斯系列 Python数据之道
37.9.4 国家名称的统一
福布斯全球上市企业排行榜数据，我们获得的数据相对比较杂乱。
我们希望将国家名称进行规范，这里以国家英文名称的列“Country_en”作为清洗的目标。
国家英文名称中，不规范的情况包括：
1. 有些样本中国家的英文名称为空白
2. 有些是中文名称
3. 有些一个国家对应多个英文名称
4. 有些一个缩写对应多个不同的国家
5. 一家企业可能在多个国家有登记
37.9.4.1 国家英文名称为空值的处理
若“Country_en”列有空值，则用“Conutry_cn”列的值替换
1 df_concat.loc[df_concat['Country_en']=='', 'Country_en'] = df_concat.loc[
df_concat['Country_en']=='', 'Country_cn']
37.9.4.2 将国家名称统一为英文
将国家名称统一成英文名称，这里主要用的是replace()方法
1 list_origin = [']','AR', 'AS','AU', 'US', 'BE', 'BR', 'BS', 'BU', 'CA', 'CH', '
CI',
2 'CN-HK', 'CN-TA', 'CO', 'CZ', 'DE', 'EG', 'FI', 'FR', 'GE',
3 'GR', 'Hong Kong', 'Hong Kong/China', 'HU', 'IC', 'ID', 'IN', 'IR
',
4 'IS','IT', 'JA', 'JO', 'KO','LI', 'LU', 'MX', 'NL', 'NO', 'NZ',
5 'PA', 'PE', 'PH', 'PK', 'PL', 'PO', 'RU', 'SI', 'SP', 'SU',
6 'SW', 'SZ', 'Taiwan', 'TH', 'TU', 'UK', 'VE', '阿 根 廷', '阿 联 酋',
7 '阿 曼', '埃 及', '爱 尔 兰', '奥 地 利', '澳 大 利 亚', '巴 基 斯 坦', '巴 林
',
8 '巴 拿 马', '巴 西', '百 慕 大', '比 利 时', '波 多 黎 各', '波 兰', '丹 麦',
9 '德 国', '多 哥', '俄 罗 斯', '法 国', '菲 律 宾', '芬 兰', '哥 伦 比 亚',
10 '哈 萨 克 斯 坦', '海 峡 群 岛', '韩 国', '荷 兰', '加 拿 大', '捷 克 共 和 国',
11 '卡 塔 尔', '开 曼 群 岛', '科 威 特', '克 罗 地 亚', '黎 巴 嫩', '利 比 里 亚',
12 '列 支 敦 士 登', '列 支 敦 斯 登', '卢 森 堡', '马 来 西 亚', '毛 里 求 斯',
13 '美 国', '秘 鲁', '摩 洛 哥', '墨 西 哥', '南 非', '尼 日 利 亚', '挪 威',
14 '葡 萄 牙', '日 本', '瑞 典', '瑞 士', '塞 浦 路 斯', '沙 特$',
15 '沙 特 阿 拉 伯', '斯 洛 伐 克', '泰 国', '土 耳 其', '委 内 瑞 拉', '西 班 牙',
16 '希 腊', '新 加 坡', '新 西 兰', '匈 牙 利', '以 色 列', '意 大 利',
17 '印 度$', '印 度 尼 西 亚', '印 尼', '英 国', '约 旦', '越 南', '智 利',
18 '中 国 大 陆', '中 国 台 湾', '中 国 香 港', 'CN$', '中 国$', 'Netherlands'
]
19
http://liyangbit.com 467Chapter37. 项目实战：福布斯系列 Python数据之道
20 list_replace = ['','Argentina', 'Austria','Australia', 'United States', '
Belgium',
21 'Brazil', 'Bahamas', 'Bermuda', 'Canada', 'Chile', 'Cayman
Islands',
22 'China-HongKong', 'China-Taiwan', 'Colombia', 'Czech Republic',
23 'Denmark', 'Egypt', 'Finland', 'France', 'Germany', 'Greece', '
China-HongKong',
24 'China-HongKong', 'Hungary', 'Iceland', 'Indonesia', 'India', '
Ireland', 'Israel',
25 'Italy', 'Japan', 'Jordan', 'South Korea','Liberia', 'Luxembourg
', 'Mexico',
26 'Netherland', 'Norway', 'New Zealand', 'Panama', 'Peru', '
Philippines',
27 'Pakistan', 'Poland', 'Portugal', 'Russia', 'Singapore', 'Spain'
,
28 'Saudi Arabia', 'Sweden', 'Switzerland', 'China-Taiwan',
29 'Thailand', 'Turkey', 'United Kingdom', 'Venezuela', 'Argentina'
,
30 'United Arab Emirates', 'Oman', 'Egypt', 'Ireland', 'Austria', '
Australia',
31 'Pakistan', 'Bahrain', 'Panama', 'Brazil', 'Bermuda', 'Belgium',
32 'Puerto Rico', 'Poland', 'Denmark', 'Germany', 'Togo', 'Russia',
33 'France', 'Philippines', 'Finland', 'Colombia', 'Kazakhstan',
34 'Channel Islands', 'South Korea', 'Netherland', 'Canada',
35 'Czech Republic', 'Qatar', 'Cayman Islands', 'Kuwait', 'Croatia'
, 'Lebanon',
36 'Liberia', 'Liechtenstein', 'Liechtenstein', 'Luxembourg', '
Malaysia',
37 'Mauritius', 'United States', 'Peru', 'Morocco', 'Mexico', '
South Africa',
38 'Nigeria', 'Norway', 'Portugal', 'Japan', 'Sweden', 'Switzerland
', 'Cyprus',
39 'Saudi Arabia', 'Saudi Arabia', 'Slovakia', 'Thailand', 'Turkey'
, 'Venezuela',
40 'Spain', 'Greece', 'Singapore', 'New Zealand', 'Hungary', '
Israel', 'Italy',
41 'India', 'Indonesia', 'Indonesia', 'United Kingdom', 'Jordan', '
Vietnam',
42 'Chile', 'China', 'China-Taiwan', 'China-HongKong', 'China', '
China',
43 'Netherland']
44
45 df_concat['Country_en'] = df_concat['Country_en'].replace(list_origin,
list_replace, regex=True)
两个list数据的录入，没有捷径，自己录入的。
37.9.4.3 一家公司属于多个国家的情况
进一步替换，包括一家公司属于多个国家的情况。
http://liyangbit.com 468Chapter37. 项目实战：福布斯系列 Python数据之道
1 # 进 一 步 替 换， 包 括 一 家 公 司 属 于 多 个 国 家 的 情 况
2 # 注 意 这 里 没 有 regex=True
3 df_concat['Country_en'] = df_concat['Country_en'].replace(['China-China-Taiwan'
,r'China-HongKong/China', r'Australia)/United Kingdom(United Kingdom',
4 r'Netherland)/United
Kingdom(United
Kingdom', r'Panama
)/United Kingdom(
United Kingdom',
5 r'SA)/United Kingdom(
United Kingdom', r
'United Kingdom)/
Australia(SA', r'
United Kingdom)/
Netherland(
Netherland',
6 r'United Kingdom)/
South Africa(SA' ,
'Netherland/
United Kingdom', '
Panama/United
Kingdom',
7 'Australia/United
Kingdom'],
8 ['China-Taiwan', '
China-HongKong', '
United Kingdom/
Australia', '
United Kingdom/
Netherland',
9 'United Kingdom/
Panama', 'United
Kingdom/Australia'
, 'United Kingdom/
Australia',
10 'United Kingdom/
Netherland', '
United Kingdom/
South Africa', '
United Kingdom/
Netherland',
11 'United Kingdom/
Panama', 'United
Kingdom/Australia'
])
37.9.4.4 同一个英文缩写代表不同国家的情况
通过观察数据，发现有一类特殊情况，同样的英文缩写，代表的国家可能不同，需要进一步处理。
http://liyangbit.com 469Chapter37. 项目实战：福布斯系列 Python数据之道
如SA可以代表Australia或SouthAfrica，MA可以代表Malaysia或Morocco
处理这类特殊情况的思路与步骤为：
（a）将此类英文缩写用空白值替换
（b）将空白值替换为当前样本的中文国家名称
（c）将中文国家名称替换为各自的英文国家名称
1 # 注 意 需 要 以 ”$”结 尾
2 df_concat['Country_en'] = df_concat['Country_en'].replace(['MA$','SA$'],['', ''
], regex=True)
3
4 df_concat.loc[df_concat['Country_en']=='', 'Country_en'] = df_concat.loc[
df_concat['Country_en']=='', 'Country_cn']
5
6 df_concat['Country_en'] = df_concat['Country_en'].replace(['澳 大 利 亚','马 来 西 亚
', '摩 洛 哥', '南 非'],
7 ['Australia', '
Malaysia', '
Morocco', 'South
Africa'])
最后，保存数据
1 df_concat.to_csv('data_forbes_concat.csv')
37.9.5 总结
2016年至2017年的数据清洗，前文已作详细描述，本文主要是描述国家名称的统一处理。
至此，福布斯全球上市企业排行榜数据（2007年至2017年）的清洗告一段落。
结合前文2007年-2015年的数据清洗，将2007-2017年的数据清洗的代码合并到一起，感兴趣的同学可以回复
关键字获取相关代码。
关注微信公众号公众号“Python数据之道”，后台回复“code”，获取本文的源代码及原始数据文件。
http://liyangbit.com 470Chapter37. 项目实战：福布斯系列 Python数据之道
http://liyangbit.com 47138 Python 学习资料
经常有读者问我，想让我推荐一些关于Python学习的书籍，鉴于每个人的知识背景以及跟我交流的深浅程度不一
样，可能每次推荐的书籍都会有些差异。
最近有些想法，打算陆陆续续介绍一些书籍，希望能给大家带来一些益处。
需要注意的是，我本身不会刻意对书籍的内容进行点评，尽量做到相对客观的对书籍进行介绍。
至于你是否需要去学习文章中介绍的书籍，这个还是需要结合自己的实际情况来做筛选。毕竟每个人的时间和精力
都有限，市面上优秀的书籍数不胜数，我们挑选适合自己的就好。
38.1 《笨办法学Python3》
笨办法学Python，其实一点都不笨
本次来介绍的书籍是《笨办法学Python3》，这本书的作者是泽德A.肖（ZedA.Shaw）。
472Chapter38. Python学习资料 Python数据之道
Figure38.1:img-笨办法学Python3-封面
本书作者是《“笨办法”学Python》《“笨办法”学Ruby》《“笨办法”学C语言》等几本备受欢迎的图书的作者，他
的书在全世界有数百万读者。他还是诸多开源项目的创建者，拥有近20年的编程和写作经验。
《笨办法学Python3》基于Python3.6，是一本从零基础入门开始介绍Python3的书籍。本书介绍了一个非常
有用的学习窍门，就是把编程书籍习题化，全书共52道习题。
习题的安排遵循了循序渐进的原则，每道题2~5页不等，内容相对通俗易懂，对于初学者而言，可以减少不少枯燥
之处。
下面将就书中的部分内容进行描述，希望大家对本书有所了解。
38.1.1 习题1：第一个程序
作者建议先编写一个完整的Python程序，即.py文件，然后在终端运行代码。
例如，文件ex1.py的代码如下
http://liyangbit.com 473Chapter38. Python学习资料 Python数据之道
1 print('Hello World!')
2 print('Hello again')
3 print('I like typing this.')
4 print('This is fun.')
5 print('Yay! Printing.')
6 print(”I'd much rather you 'not'.”)
7 print('I ”said” do not touch this.')
在终端运行下面代码
1 python ex1.py
我运行的效果如下：
Figure38.2:img-演示
38.1.2 习题4：变量和命名
在这个习题中，提到了一些我们经常遇到的小问题。
比如：
=（单等号）和==（双等号）有什么不同？
这个问题的答案，请自行查找（本书也进行了阐述）。
比如：
http://liyangbit.com 474Chapter38. Python学习资料 Python数据之道
书写x=100和x = 100有什么差异？
上面两种写法，本质上都是可以的，但操作符两边加上空格，会让代码可阅读性更好。
38.1.3 习题6：字符串和文本
在Python3中，字符串有几种常见的输入表达方法，本书着重介绍了一种，称为“f-string”，其语法格式如下：
1 f”some stuff here {avariable}”
在ipython中实际运行下面代码：
1 num = 10
2
3 y = f”there are {num} apples.”
4
5 print(y)
效果如下：
Figure38.3:img演示
同时，关于字符串中.format()语法格式的使用，书中亦有描述。
http://liyangbit.com 475Chapter38. Python学习资料 Python数据之道
38.1.4 总结
总体而言，本书52道习题，内容逐步深入，在最后几道习题，作者通过使用flask来建立一个微小型的网页，使
本书内容延伸到web编程的范畴。
在本书的结尾部分，作者也给出了一些建议：
你已经完成了这本书并且打算继续编程。也许这会成为你的职业，也许你只是作为业余爱好玩玩而已。无论如何，
你都需要一些建议以确保你在正确的道路上继续前行，并且让这项新的爱好最大程序为你带来享受。
…………
编程语言这东西并不重要，重要的是你用这些编程语言做的事情。
更多关于本书的详情，可以点击下面的链接进行了解：
可以说，本书以一种较为新颖的方式，帮助广大读者完成了Python学习从0到0.5的过程，如果需要继续深入
了解Python，可以配合其他书籍进一步学习。
最后，「Python数据之道」将本书的内容概览进行了整理，以便更加清晰的了解本书的内容结构，在公众号
「Python数据之道」后台回复“pdf”可以获取本书高清版pdf版的思维导图。
http://liyangbit.com 476Chapter38. Python学习资料 Python数据之道
Figure38.4:img-思维导图
http://liyangbit.com 477